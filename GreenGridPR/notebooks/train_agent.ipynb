{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81222bf5-c8bd-4341-8569-71e7b8e0cd86",
   "metadata": {},
   "source": [
    "# Overview\n",
    "## State Space\n",
    "- The state space consists of an 80,000-cell grid, representing different geographical locations in Puerto Rico.\n",
    "- Each cell has attributes like solar PV output, wind power density, elevation, slope, cyclone risk score, building density, road density, and distance to transmission lines.\n",
    "- Approximately 70% of cells are unavailable for development due to environmental or other constraints.\n",
    "\n",
    "## Action Space\n",
    "- Two types of actions are available: building a solar array or a wind turbine.\n",
    "- Actions can be taken on any available cell.\n",
    "\n",
    "## Rewards and Costs\n",
    "The reward function should incorporate:\n",
    "- Energy production potential (solar and wind).\n",
    "- Costs or penalties associated with building on certain terrains (e.g., high elevation or steep slopes).\n",
    "- Penalties for building in high cyclone risk areas.\n",
    "- Incentives for maintaining a balance between solar and wind energy.\n",
    "- Incentives for early deployment and distributed grid development.\n",
    "- Penalties for high building or road density areas.\n",
    "- Distance to transmission lines.\n",
    "\n",
    "## RL Model\n",
    "- Model Choice: Given the size of the state space, a model-based RL algorithm (like Deep Q-Networks or Actor-Critic methods) is suitable.\n",
    "- Representation: The state representation should include the current status of each grid cell (whether it has a solar array, a wind turbine, or is vacant) along with its attributes.\n",
    "- Sequence of Actions: The RL agent will sequentially choose actions (where to build next) based on the current state of the grid.\n",
    "- Terminal State: The agent is done when the environment reaches a certain level of energy capacity or after a fixed number of steps.\n",
    "\n",
    "# Implementation Steps\n",
    "## Environment Setup: \n",
    "- Implement the environment to reflect the grid and its dynamics, including applying the binary mask for unavailable cells.\n",
    "- The step(action) method should update the grid state based on the chosen action and calculate the immediate reward or cost.\n",
    "\n",
    "## Agent Development:\n",
    "- Use PyTorch for implementing the neural network models for the agent.\n",
    "The agent needs to learn a policy that maximizes long-term rewards, considering the complex reward structure and large state space.\n",
    "\n",
    "## Training and Evaluation:\n",
    "- Set up a training loop where the agent interacts with the environment, receives feedback, and improves its policy.\n",
    "- Periodically evaluate the agent's performance, possibly using separate evaluation episodes or metrics like total energy capacity achieved or adherence to environmental constraints.\n",
    "\n",
    "## Hyperparameter Tuning:\n",
    "- Adjust learning rates, exploration rates, discount factors, and network architecture as needed to improve performance.\n",
    " \n",
    "## Scalability:\n",
    "Due to the large state space, may need to:\n",
    "- use function approximation for value functions\n",
    "- prioritizing important experiences in the replay buffer\n",
    "- parallelize computation process\n",
    "\n",
    "## Visualization and Analysis:\n",
    "- Develop tools to visualize the evolving grid layout and analyze the trade-offs made by the RL agent between different objectives (like energy maximization vs. environmental constraints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a0776a-f41d-41ba-9261-9079553eae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "\n",
    "import random\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c724dafa-ff4d-4fae-babe-283dcb25bb04",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac168609-d297-4963-b880-bcfdc86c2f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenewableEnergyEnvironment:\n",
    "    def __init__(self, grid_gdf, demand):\n",
    "        self.step_count = 0\n",
    "        self.demand_satisfied_ratio = 0\n",
    "\n",
    "        # Daily cost calculated in data_preprocessing notebook\n",
    "        # Multiplying by 4 because grid cell size was changed from 500m to 1 km\n",
    "        self.wind_daily_cost = 4 * 821.68\n",
    "        self.solar_daily_cost = 4 * 3007.26\n",
    "        \n",
    "        self.grid_columns = ['distance_to_transmission_line', \n",
    "                             'cyclone_risk',\n",
    "                             'occupied']\n",
    "        self.grid_columns += [f'wind_power_kW_hour_available_{i}' for i in range(1,25)]\n",
    "        self.grid_columns += [f'solar_power_kW_hour_available_{i}' for i in range(1,25)]\n",
    "        self.grid_columns += [f'demand_{i}' for i in range(1, 25)]\n",
    "        \n",
    "        # Initialize the environment\n",
    "        self.starting_environment = grid_gdf\n",
    "        self.state_gdf = grid_gdf.copy()\n",
    "        self.state_tensor = self.gdf_to_tensor(self.state_gdf)\n",
    "        self.mapping, self.action_space_size = self.create_action_to_gdf_mapping()\n",
    "        \n",
    "        self.total_energy_output = 0\n",
    "        self.stored_energy = 0\n",
    "\n",
    "        self.initial_demand = demand\n",
    "        #print('Demand intialized in environment:', self.current_demand)\n",
    "        self.unmet_demand = self.initial_demand.sum() * 1000\n",
    "        self.total_demand = self.unmet_demand\n",
    "\n",
    "        # LEGACY - the following parameters are no longer in use\n",
    "        self.decay_rate = 0.1 #TODO determine good decay rate\n",
    "        self.max_distance = 1000 #TODO get max distance between two cells\n",
    "        self.weights = {\n",
    "        'transmission_build_cost': -1.0,\n",
    "        'early_choice_reward': 1.0,\n",
    "        'distributed_grid_reward': 1.0,\n",
    "        }\n",
    "        \n",
    "    def reset(self):\n",
    "        # Reset the environment to the initial state\n",
    "        self.state_gdf = self.starting_environment.copy()\n",
    "        self.state_tensor = self.gdf_to_tensor(self.starting_environment)\n",
    "        self.total_energy_output = 0\n",
    "        self.step_count = 0\n",
    "        return self.state_tensor\n",
    "\n",
    "    def gdf_to_tensor(self, gdf):\n",
    "        # Calculate grid dimensions\n",
    "        x_start = 100000\n",
    "        x_end = 300000\n",
    "        y_start = 200000\n",
    "        y_end = 300000\n",
    "        square_size = 1000\n",
    "        \n",
    "        grid_width = int((x_end - x_start) / square_size)\n",
    "        grid_height = int((y_end - y_start) / square_size)\n",
    "\n",
    "\n",
    "        transposed_data = self.state_gdf[self.grid_columns].values.T\n",
    "        grid_data = np.array([feature.reshape(100, 200) for feature in transposed_data])\n",
    "        tensor = torch.tensor(grid_data, dtype=torch.float32)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def create_action_to_gdf_mapping(self):\n",
    "        unmasked_gdf = self.state_gdf[self.state_gdf['masked'] == 0]\n",
    "        \n",
    "        mapping = {}\n",
    "        action_idx = 0  # Initialize action index\n",
    "    \n",
    "        for _, row in unmasked_gdf.iterrows():\n",
    "            # Check for valid solar action\n",
    "            if row['slope'] <= 8.749:  # Slope check for solar\n",
    "                mapping[action_idx] = (row.name, 'solar')\n",
    "                action_idx += 1\n",
    "    \n",
    "            # Check for valid wind action\n",
    "            if row['slope'] <= 26.795:  # Slope check for wind\n",
    "                mapping[action_idx] = (row.name, 'wind')\n",
    "                action_idx += 1\n",
    "        \n",
    "        action_space_size = action_idx  # Total number of valid actions\n",
    "        return mapping, action_space_size\n",
    "\n",
    "    def output_stats(self, writer, cell_index, action_type, reward, invalid=False):\n",
    "        if writer:\n",
    "            writer.writerow([None,None,None,cell_index, action_type, int(reward), int(self.total_energy_output.sum()), int(self.unmet_demand)])\n",
    "        if invalid:\n",
    "            print(f'Cell: {cell_index:<6d} | Step: INV  | Action: {action_type:<5} | Reward: {reward:8.5f} | Energy Output: {int(self.total_energy_output.sum()):<10} | Unmet Demand: {int(self.unmet_demand):<10}')\n",
    "        else:\n",
    "            print(f'Cell: {cell_index:<6d} | Step: {self.step_count:<4d} | Action: {action_type:<5} | Reward: {reward:8.5f} | Energy Output: {int(self.total_energy_output.sum()):<10} | Unmet Demand: {int(self.unmet_demand):<10}')\n",
    "\n",
    "    def step(self, action, writer=None, verbose=True):\n",
    "        # Apply the action to the environment and return the result\n",
    "        # action: Tuple (cell_index, action_type) where action_type could be 'solar' or 'wind'\n",
    "\n",
    "        # Map action from agent output to action in terms of state_gdf\n",
    "        cell_index, action_type = self.mapping[action]\n",
    "\n",
    "        # Check if the action is valid\n",
    "        if not self.is_valid_action(cell_index, action_type):\n",
    "            reward = -2 # Penalty for invalid action\n",
    "            done = self.is_terminal_state() # Check if in terminal state\n",
    "            self.total_energy_output = self.calculate_energy_output() # Calculate total energy output\n",
    "            if verbose:\n",
    "                self.output_stats(writer, cell_index, action_type, reward, invalid=True) # Print results and write results to file\n",
    "            return self.state_tensor, reward, done, {}\n",
    "\n",
    "        # Calculate the total reward before applying the action\n",
    "        total_reward_before_action = self.calculate_reward()\n",
    "    \n",
    "        # Apply the action\n",
    "        self.apply_action(cell_index, action_type)\n",
    "    \n",
    "        # Calculate the total reward after applying the action\n",
    "        total_reward_after_action = self.calculate_reward()\n",
    "    \n",
    "        # The reward for the action is the difference in total reward\n",
    "        reward = total_reward_after_action - total_reward_before_action\n",
    "        reward = reward / 100000\n",
    "\n",
    "        # Update total energy output or other state attributes as needed\n",
    "        self.total_energy_output = self.calculate_energy_output()\n",
    "        \n",
    "        # Check if the state is terminal\n",
    "        done = self.is_terminal_state()\n",
    "\n",
    "        if verbose:\n",
    "            # Print results and write results to file\n",
    "            self.output_stats(writer, cell_index, action_type, reward)\n",
    "        \n",
    "        # Update the state tensor with the new state gdf\n",
    "        self.state_tensor = self.gdf_to_tensor(self.state_gdf)\n",
    "\n",
    "        self.step_count += 1\n",
    "        \n",
    "        return self.state_tensor, reward, done, {}\n",
    "\n",
    "    def is_valid_action(self, cell_index, action_type):\n",
    "        # Implement logic to check if an action is valid\n",
    "        cell = self.state_gdf.iloc[cell_index]\n",
    "        if cell['masked']: # This should never occur\n",
    "            print('Error: Attempting to build on a masked cell')\n",
    "            return False\n",
    "        elif cell['occupied']:\n",
    "            return False\n",
    "        elif action_type == 'solar' and cell['slope'] > 8.749: # This should never occur\n",
    "            print('Error: Attempting to build solar on slope of more than 5%')\n",
    "            return False\n",
    "        elif action_type == 'wind' and cell['slope'] > 26.795: # This should never occur\n",
    "            print('Error: Attempting to build wind on slope of more than 15%')\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def apply_action(self, cell_index, action_type):\n",
    "        # Implement the changes to the environment based on the action\n",
    "        # Example: Mark the cell as occupied and record the type of installation\n",
    "        self.state_gdf.at[cell_index, 'occupied'] = 1\n",
    "        self.state_gdf.at[cell_index, 'installation_type'] = action_type\n",
    "        for i in range(1,25):\n",
    "          self.state_gdf.at[cell_index, f'wind_power_kW_hour_available_{i}'] = 0\n",
    "          self.state_gdf.at[cell_index, f'solar_power_kW_hour_available_{i}'] = 0\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        # Solar installation cost\n",
    "        solar_cost = self.calculate_solar_cost()\n",
    "\n",
    "        # Wind turbine installation cost\n",
    "        wind_cost = self.calculate_wind_cost()\n",
    "        \n",
    "        # Solar power Reward\n",
    "        power_output_reward = self.calculate_power_output_reward()\n",
    "\n",
    "        # Transmission build cost\n",
    "        transmission_build_cost = self.calculate_transmission_build_cost()\n",
    "        \n",
    "        # Cyclone risk cost\n",
    "        cyclone_risk_cost = self.calculate_cyclone_risk_cost()\n",
    "    \n",
    "        # Distribution reward\n",
    "        # distributed_grid_reward = self.calculate_distributed_grid_reward()\n",
    "    \n",
    "        # Early choice reward\n",
    "        # early_choice_reward = self.time_dependent_reward_factor()\n",
    "\n",
    "        total_reward = power_output_reward + solar_cost + wind_cost + cyclone_risk_cost + transmission_build_cost\n",
    "        \n",
    "        return total_reward\n",
    "\n",
    "    def calculate_energy_output(self):\n",
    "        # Filter for solar and wind installations\n",
    "        solar_gdf = self.state_gdf[self.state_gdf['installation_type'] == 'solar']\n",
    "        wind_gdf = self.state_gdf[self.state_gdf['installation_type'] == 'wind']\n",
    "    \n",
    "        # Prepare column names for solar and wind power\n",
    "        solar_power_columns = [f'solar_power_kW_hour_{i}' for i in range(1, 25)]\n",
    "        wind_power_columns = [f'wind_power_kW_hour_{i}' for i in range(1, 25)]\n",
    "    \n",
    "        # Vectorized sum of power output for solar and wind for each hour\n",
    "        total_solar_power = solar_gdf[solar_power_columns].sum().to_numpy() * 1000\n",
    "        total_wind_power = wind_gdf[wind_power_columns].sum().to_numpy() * 1000\n",
    "    \n",
    "        total_power = total_solar_power + total_wind_power\n",
    "        \n",
    "        return total_power\n",
    "\n",
    "    def calculate_power_output_reward(self):\n",
    "        \"\"\"Uses supply and demand curve to determine the amount of demand satisfied by\n",
    "        the solar and wind installations\"\"\"\n",
    "        cost_kWh = .22  # TODO get more rigorous number \n",
    "\n",
    "        #print('Getting demand to calculate power output reward:', self.initial_demand)\n",
    "\n",
    "        # Calculate the reward using vectorized minimum\n",
    "        total_power = self.calculate_energy_output()\n",
    "        demand_satisfied = np.minimum(self.initial_demand * 1000, total_power)\n",
    "        #print('Demand satisfied in calc pow out rew:', demand_satisfied)\n",
    "        self.demand_satisfied_ratio = demand_satisfied / (self.initial_demand * 1000)\n",
    "\n",
    "        self.update_demand()\n",
    "        \n",
    "        power_reward = demand_satisfied.sum() * cost_kWh\n",
    "        # self.stored_energy = np.maximum(total_power - demand, 0).sum()\n",
    "        # storage_cost = self.stored_energy * cost_storage_kWh\n",
    "\n",
    "        reward = power_reward # - storage_cost\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def update_demand(self):\n",
    "        unsatisfied_demand_ratio = 1 - self.demand_satisfied_ratio\n",
    "        for i in range(1, 25):\n",
    "          self.state_gdf[f'demand_{i}'] *= unsatisfied_demand_ratio[i-1]\n",
    "    \n",
    "    def calculate_solar_cost(self):\n",
    "        \"\"\"Cost of installing solar array on cell. Based on elevation and slope\"\"\"\n",
    "        return -self.solar_daily_cost * len(self.state_gdf[self.state_gdf['installation_type'] == 'solar'])\n",
    "\n",
    "    def calculate_wind_cost(self):\n",
    "        \"\"\"Cost of installing wind turbine on cell. Based on elevation and slope\"\"\"\n",
    "        return -self.wind_daily_cost * len(self.state_gdf[self.state_gdf['installation_type'] == 'wind'])\n",
    "\n",
    "    def transmission_line_cost_per_km(self, distance):\n",
    "        # $2.29 million per mile divided by 1.60934 to get into km, \n",
    "        # then divided by 25 * 365.25 for daily costs with 25 year decommission time\n",
    "        distance /= 1000 # Convert to km\n",
    "        KM_PER_MILE = 1.60934\n",
    "        COST_PER_KM = 2.29 * 1000000 / (KM_PER_MILE * 25 * 365.25) \n",
    "        SHORT_DISTANCE_THRESHOLD = 3 * KM_PER_MILE # Threshold for short distance \n",
    "        MEDIUM_DISTANCE_THRESHOLD = 10 * KM_PER_MILE # Threshold for medium distance \n",
    "        \n",
    "        if distance < SHORT_DISTANCE_THRESHOLD:\n",
    "            cost_modifier = 1.5  # 50% increase for less than 3 miles\n",
    "        elif distance < MEDIUM_DISTANCE_THRESHOLD:\n",
    "            cost_modifier = 1.2  # 20% increase for 3-10 miles\n",
    "        else:\n",
    "            cost_modifier = 1  # No modification for more than 10 miles\n",
    "        return -distance * COST_PER_KM * cost_modifier\n",
    "    \n",
    "    def calculate_transmission_build_cost(self):\n",
    "        occupied_cells = self.state_gdf[self.state_gdf['occupied'] == 1]\n",
    "        \n",
    "        # Check if there is only one occupied cell\n",
    "        if len(occupied_cells) == 0:\n",
    "            return 0\n",
    "        elif len(occupied_cells) == 1:\n",
    "            # For a single occupied cell, use the distance to transmission line for cost calculation\n",
    "            occupied_cell = occupied_cells.iloc[0]\n",
    "            distance_km = occupied_cell['distance_to_transmission_line'] * 1000\n",
    "            build_cost = self.transmission_line_cost_per_km(distance_km)\n",
    "        else:\n",
    "            # Get coordinates of occupied cells\n",
    "            coords = np.array(list(zip(occupied_cells.geometry.centroid.x, occupied_cells.geometry.centroid.y)))\n",
    "    \n",
    "            # Calculate pairwise distances between occupied cells\n",
    "            distances = cdist(coords, coords)\n",
    "    \n",
    "            # Replace zeros in distance matrix with np.inf to avoid zero distance to itself\n",
    "            np.fill_diagonal(distances, np.inf)\n",
    "    \n",
    "            # Find the nearest installation for each installation\n",
    "            nearest_installation_distances = np.min(distances, axis=1)\n",
    "    \n",
    "            # Determine the relevant distance for cost calculation\n",
    "            relevant_distances = np.minimum(nearest_installation_distances, occupied_cells['distance_to_transmission_line'].to_numpy()) * 1000\n",
    "    \n",
    "            # Calculate build cost\n",
    "            build_costs = [self.transmission_line_cost_per_km(distance) for distance in relevant_distances]\n",
    "            build_cost = sum(build_costs)\n",
    "            \n",
    "        return build_cost\n",
    "\n",
    "    def calculate_cyclone_risk_cost(self):\n",
    "        if len(self.state_gdf[self.state_gdf.installation_type == 'wind']) == 0:\n",
    "            return 0\n",
    "        # Wind operational expenses = $40/kW/yr\n",
    "        # Wind turbine capacity is 3 MW\n",
    "        # To get daily cost, multiply by 3000 (3 MW = 3000 kW) and divide by 365.25 days in a year\n",
    "        wind_opex = 4 * 40 * 3000 / 365.25\n",
    "        # Wind capital expenses = $1501/kW\n",
    "        # To get daily cost, multiply by 3000 (3 MW = 3000 kW) and divide by (25 years of lifetime * 365.25 days in a year)\n",
    "        wind_capex = 4 * 1501 * 3000 / (25 * 365.25)\n",
    "        cyclone_risk_cost = -(self.state_gdf[self.state_gdf.installation_type == 'wind']['cyclone_risk'] * (wind_capex - wind_opex)).sum()\n",
    "        return cyclone_risk_cost\n",
    "    \n",
    "    def calculate_distributed_grid_reward(self):\n",
    "        # Extract the coordinates of the occupied cells (where installations are located)\n",
    "        occupied_cells = self.state_gdf[self.state_gdf['occupied'] == 1]\n",
    "        if len(occupied_cells) < 2:\n",
    "            # If there are less than two installations, we cannot calculate distances\n",
    "            return 1\n",
    "    \n",
    "        coords = np.array(list(zip(occupied_cells.geometry.x, occupied_cells.geometry.y)))\n",
    "    \n",
    "        # Calculate pairwise distances between all occupied cells\n",
    "        distances = pdist(coords)\n",
    "    \n",
    "        # Calculate the average distance. The larger this is, the more distributed the installations are.\n",
    "        avg_distance = np.mean(distances)\n",
    "    \n",
    "        # Normalize the reward such that it ranges between 0 and 1\n",
    "        normalized_reward = avg_distance / self.max_distance\n",
    "    \n",
    "        return normalized_reward\n",
    "    \n",
    "    def time_dependent_reward_factor(self):\n",
    "        # A function to calculate the time-dependent reward factor\n",
    "        # It decreases with each year from the base year\n",
    "        action_number = self.state_gdf.occupied.sum()\n",
    "        return 1 / (1 + self.decay_rate * action_number)\n",
    "        \n",
    "    def is_terminal_state(self):\n",
    "        # The episode ends when there is no unmet demand\n",
    "        self.unmet_demand = np.maximum(self.initial_demand * 1000 - self.total_energy_output, 0).sum()\n",
    "        #return self.unmet_demand <= 0 or self.total_energy_output.sum() >= self.total_demand \n",
    "        return self.unmet_demand <= 0\n",
    "\n",
    "    def render(self):\n",
    "        # Optional: Implement a method to visualize the current state of the environment\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cb54eb-f64b-4bd0-b00f-1dbe3d5c219a",
   "metadata": {},
   "source": [
    "# Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7a787f8-9e25-4b21-859b-34979915fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected\n",
    "\n",
    "    # def __init__(self, input_shape, num_actions):\n",
    "    #     super(DQN, self).__init__()\n",
    "\n",
    "    #     #self.num_features = input_shape[0]\n",
    "\n",
    "    #     # Fully connected layers\n",
    "    #     self.fc1 = nn.Linear(input_shape[0], 512)\n",
    "    #     self.fc2 = nn.Linear(512, 512)\n",
    "    #     self.fc3 = nn.Linear(512, 512)\n",
    "    #     self.fc4 = nn.Linear(512, num_actions)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     # Flatten the input tensor\n",
    "    #     x = x.view(x.size(0), -1)\n",
    "\n",
    "    #     # Fully connected layers with ReLU activation\n",
    "    #     x = F.relu(self.fc1(x))\n",
    "    #     x = F.relu(self.fc2(x))\n",
    "    #     x = F.relu(self.fc3(x))\n",
    "\n",
    "    #     # Output layer\n",
    "    #     return self.fc4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0a461ea-ffe6-40d2-8e8f-93913535d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla CNN\n",
    "\n",
    "# def __init__(self, input_shape, num_actions):\n",
    "    #     super(DQN, self).__init__()\n",
    "    #     self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=3, stride=2)\n",
    "    #     self.bn1 = nn.BatchNorm2d(32)\n",
    "    #     self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=2)\n",
    "    #     self.bn2 = nn.BatchNorm2d(32)\n",
    "    #     self.conv3 = nn.Conv2d(32, 16, kernel_size=3, stride=2)\n",
    "    #     self.bn3 = nn.BatchNorm2d(16)\n",
    "\n",
    "    #     self.input_shape = input_shape  # Store input_shape for feature size calculation\n",
    "    #     print(f'Input shape: {input_shape[0]}, Feature size: {self._feature_size()}, n Actions: {num_actions}')\n",
    "    #     self.fc1 = nn.Linear(self._feature_size(), 4096)\n",
    "    #     self.fc2 = nn.Linear(4096, num_actions)\n",
    "\n",
    "    # def _feature_size(self):\n",
    "    #     with torch.no_grad():\n",
    "    #         return self.bn3(self.conv3(self.bn2(self.conv2(self.bn1(self.conv1(torch.zeros(1, *self.input_shape))))))).view(1, -1).size(1)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = F.relu(self.bn1(self.conv1(x)))\n",
    "    #     x = F.relu(self.bn2(self.conv2(x)))\n",
    "    #     x = F.relu(self.bn3(self.conv3(x)))\n",
    "    #     x = x.view(x.size(0), -1)\n",
    "    #     x = F.relu(self.fc1(x))\n",
    "    #     return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f382086d-cee9-46f6-8f48-4d6a81c8a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    " \n",
    "        self.num_nontemporal_channels = 2\n",
    "        #self.num_temporal_channels = input_shape[0] - self.num_nontemporal_channels  # Subtracting the non-temporal channels\n",
    "        self.num_temporal_channels = 48\n",
    "        self.time_dimension = 24\n",
    "        self.grid_height = input_shape[1]\n",
    "        self.grid_width = input_shape[2]\n",
    "\n",
    "        # 3D Convolutional layers for temporal features\n",
    "        self.conv3d1 = nn.Conv3d(self.num_temporal_channels // 24, 2, kernel_size=(3, 3, 3), stride=(1, 2, 2))\n",
    "        self.bn3d1 = nn.BatchNorm3d(2)\n",
    "        self.conv3d2 = nn.Conv3d(2, 2, kernel_size=(3, 3, 3), stride=(1, 2, 2))\n",
    "        self.bn3d2 = nn.BatchNorm3d(2)\n",
    "        # Add more layers if necessary\n",
    "\n",
    "        # 2D Convolutional layers for non-temporal features\n",
    "        self.conv2d1 = nn.Conv2d(self.num_nontemporal_channels, self.num_nontemporal_channels, kernel_size=3, stride=2)\n",
    "        self.bn2d1 = nn.BatchNorm2d(self.num_nontemporal_channels)\n",
    "        self.conv2d2 = nn.Conv2d(self.num_nontemporal_channels, self.num_nontemporal_channels, kernel_size=3, stride=2)\n",
    "        self.bn2d2 = nn.BatchNorm2d(self.num_nontemporal_channels)\n",
    "        # Add more layers if necessary\n",
    "\n",
    "        # Fully connected layer for processing 'occupied' cells feature\n",
    "        # self.fc_occupied = nn.Linear(self.grid_height * self.grid_width, 1024)\n",
    "        # self.fc_demand = nn.Linear(24, 1024)\n",
    "\n",
    "        # Calculate the size of the combined feature vector\n",
    "        # combined_feature_size = self.calculate_combined_feature_size(input_shape)\n",
    "\n",
    "        # Fully connected layers for decision making\n",
    "        self.fc1 = nn.Linear(49416, 1024)\n",
    "        self.fc2 = nn.Linear(21024, num_actions)\n",
    "\n",
    "    def calculate_combined_feature_size(self, input_shape):\n",
    "        # Dummy inputs for calculating feature sizes\n",
    "        dummy_temporal = torch.zeros(1, 72 // 24, self.time_dimension, self.grid_height, self.grid_width)\n",
    "        dummy_nontemporal = torch.zeros(1, self.num_nontemporal_channels, self.grid_height, self.grid_width)\n",
    "        dummy_occupied = torch.zeros(1, 1, self.grid_height, self.grid_width)\n",
    "\n",
    "        # Forward pass through convolutional layers\n",
    "        x_temporal = self.bn3d2(self.conv3d2(self.bn3d1(self.conv3d1(dummy_temporal))))\n",
    "        x_nontemporal = self.bn2d2(self.conv2d2(self.bn2d1(self.conv2d1(dummy_nontemporal))))\n",
    "        x_occupied = dummy_occupied.view(1, -1)\n",
    "\n",
    "        # Calculate the total number of features\n",
    "        total_features = x_temporal.numel() + x_nontemporal.numel() + x_occupied.numel()\n",
    "        return total_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Split input into temporal and non-temporal components\n",
    "        x_temporal = x[:, 3:51, :, :].view(x.size(0), self.num_temporal_channels // 24, self.time_dimension, self.grid_height, self.grid_width)\n",
    "        x_nontemporal = x[:, :2, :, :]  # Distance to transmission line & cyclone risk\n",
    "        x_occupied = x[:, 2:3, :, :].view(x.size(0), -1)  # Occupied cells\n",
    "        x_demand = x[:, 51:, 0, 0].view(x.size(0), -1) / 1000 # Demand cells\n",
    "\n",
    "        # Process temporal features\n",
    "        x_temporal = F.relu(self.bn3d1(self.conv3d1(x_temporal)))\n",
    "        x_temporal = F.relu(self.bn3d2(self.conv3d2(x_temporal)))\n",
    "        x_temporal = x_temporal.view(x_temporal.size(0), -1)\n",
    "\n",
    "        # Process non-temporal features\n",
    "        x_nontemporal = F.relu(self.bn2d1(self.conv2d1(x_nontemporal)))\n",
    "        x_nontemporal = F.relu(self.bn2d2(self.conv2d2(x_nontemporal)))\n",
    "        x_nontemporal = x_nontemporal.view(x_nontemporal.size(0), -1)\n",
    "\n",
    "        # Process 'demand' feature\n",
    "        # x_demand = F.relu(self.fc_demand(self.x_demand))\n",
    "        # Process 'occupied' cells feature\n",
    "        #x_occupied = F.relu(self.fc_occupied(x_occupied))\n",
    "\n",
    "        # batch_size = x.size(0)\n",
    "        # x_demand = self.demand.unsqueeze(0).repeat(batch_size, 1)\n",
    "        #print('Demand in forward:', self.demand, x_demand)\n",
    "\n",
    "        # Combine features\n",
    "        x_combined = torch.cat((x_temporal, x_nontemporal, x_demand), dim=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x_combined = F.relu(self.fc1(x_combined))\n",
    "        x_combined = torch.cat((x_combined, x_occupied), dim=1)\n",
    "        return self.fc2(x_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ab3b4f-b218-4ce4-bf5d-482d2eae1923",
   "metadata": {},
   "source": [
    "# DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3b2715-0e1f-4729-9844-fd3aee881f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_space, action_space_size, device):\n",
    "        #print('Demand intialized in DQNAgent:', demand)\n",
    "        self.device = device\n",
    "        self.action_space_size = action_space_size\n",
    "        self.model = DQN(state_space.shape, action_space_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
    "        self.gamma = 0.995\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            # Choose the best action (exploitation)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.model(state)\n",
    "                action = q_values.max(1)[1].item()  # Select the action with the highest Q-value\n",
    "        else:\n",
    "            # Choose a random action (exploration)\n",
    "            action = random.randrange(self.action_space_size)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, batch):\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "        # Compute Q values\n",
    "        current_q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.model(next_states).max(1)[0]\n",
    "        expected_q_values = rewards + self.gamma * (1 - dones) * next_q_values.detach()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = torch.nn.functional.mse_loss(current_q_values, expected_q_values)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def learn_from_immediate_feedback(self, state, action, penalty, next_state):\n",
    "        # Convert to tensors if they are not already\n",
    "        state = state.clone().detach().unsqueeze(0).to(self.device)\n",
    "        action = torch.tensor([action], dtype=torch.long).to(self.device)\n",
    "        reward = torch.tensor([penalty], dtype=torch.float32).to(self.device)\n",
    "        next_state = next_state.clone().detach().unsqueeze(0).to(self.device)\n",
    "        done = torch.tensor([False], dtype=torch.float32).to(self.device)\n",
    "    \n",
    "        # Compute current Q values (only for the specific action taken)\n",
    "        current_q_values = self.model(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "        # For immediate feedback, we don't look at next state's Q values\n",
    "        expected_q_values = reward  # The expected Q value is just the immediate penalty\n",
    "    \n",
    "        # Compute loss\n",
    "        loss = torch.nn.functional.mse_loss(current_q_values, expected_q_values)\n",
    "    \n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78facf33-0887-46bb-ba62-89ca956e553d",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64ee56fc-3afc-4389-b78f-4bbe022f18ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "\n",
    "        # Stack the tuples of tensors to create a single tensor for each component\n",
    "        states = torch.stack(states).to(device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float).to(device)\n",
    "        next_states = torch.stack(next_states).to(device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float).to(device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccdad5c-e260-423f-9989-0c144f28de9f",
   "metadata": {},
   "source": [
    "# Save the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d58104a-c536-4cf7-817a-4bd064984466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_agent(agent):\n",
    "    # Ensure the models directory exists\n",
    "    dir_path = '../models/'\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    # Save the agent\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    filename = f'agent_model_{current_time}.pth'\n",
    "    torch.save(agent.model.state_dict(), f'../models/{filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254993b-b1b0-4822-82c9-ca9ba3294557",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4036b194-8954-45df-b0aa-aacef4da5c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, environment, episodes, epsilon_start, epsilon_end, epsilon_decay, replay_buffer, batch_size, device):\n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    filename = f'step_log_{current_time}.csv'\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    dir_path = '../data/logs/'\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    # Complete file path\n",
    "    file_path = os.path.join(dir_path, filename)\n",
    "\n",
    "    # Open a file for writing\n",
    "    with open(file_path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the header\n",
    "        writer.writerow(['Episode', 'Total Reward', 'Epsilon', 'Cell Index', 'Action Type', 'Reward', 'Energy Output', 'Unmet Demand'])\n",
    "    \n",
    "        for episode in range(episodes):\n",
    "            state = environment.reset().to(device)\n",
    "            done = False\n",
    "            total_reward = 0  # To keep track of total reward per episode\n",
    "    \n",
    "            while not done:\n",
    "                action = agent.select_action(state.unsqueeze(0), epsilon)\n",
    "                    \n",
    "                next_state, reward, done, _ = environment.step(action, writer=writer)\n",
    "\n",
    "                cell_index, action_type = environment.mapping[action]\n",
    "                if not environment.is_valid_action(cell_index, action_type):\n",
    "                    # Provide immediate negative feedback for invalid action\n",
    "                    immediate_penalty = -2  # Define an appropriate penalty for your scenario\n",
    "                    agent.learn_from_immediate_feedback(state, action, immediate_penalty, next_state)\n",
    "    \n",
    "                # Store experience in replay buffer\n",
    "                replay_buffer.store(state, action, reward, next_state, torch.tensor(bool(done)))\n",
    "    \n",
    "                # Check if buffer is ready for sampling\n",
    "                if len(replay_buffer) > batch_size:\n",
    "                    # Sample a batch from replay buffer\n",
    "                    batch = replay_buffer.sample(batch_size)\n",
    "                    # Learn from the sampled experiences\n",
    "                    agent.learn(batch)\n",
    "    \n",
    "                # Update state\n",
    "                state = next_state.to(device)\n",
    "                total_reward += reward\n",
    "            \n",
    "            # Write episode data to the file\n",
    "            writer.writerow([episode + 1, total_reward, epsilon, None, None, None, None, None])\n",
    "\n",
    "            # Log training progress\n",
    "            print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "            # Decay epsilon\n",
    "            epsilon = max(epsilon_end, epsilon_decay * epsilon)  # Ensure epsilon doesn't go below the minimum\n",
    "\n",
    "    save_agent(agent)\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d0b32-09a3-4aef-bcaf-51f0bc870bcf",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39715d08-6208-4791-9064-e5de7a05abed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m state_space \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mstate_tensor\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     26\u001b[0m action_space_size \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39maction_space_size\n\u001b[0;32m---> 28\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mDQNAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_space_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[1;32m     32\u001b[0m epsilon_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mDQNAgent.__init__\u001b[0;34m(self, state_space, action_space_size, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space_size \u001b[38;5;241m=\u001b[39m action_space_size\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mDQN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_space_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.95\u001b[39m\n",
      "Cell \u001b[0;32mIn[5], line 35\u001b[0m, in \u001b[0;36mDQN.__init__\u001b[0;34m(self, input_shape, num_actions)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Add more layers if necessary\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Fully connected layer for processing 'occupied' cells feature\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Fully connected layers for decision making\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m49416\u001b[39m, \u001b[38;5;241m1024\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2 \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m21024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_actions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/gtomscs/dl/GreenGridPR/slim_environment/lib/python3.11/site-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/gtomscs/dl/GreenGridPR/slim_environment/lib/python3.11/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/Projects/gtomscs/dl/GreenGridPR/slim_environment/lib/python3.11/site-packages/torch/nn/init.py:419\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    417\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state_gdf = gpd.read_parquet('../data/processed/state_1km_grid_cells_masked_power.parquet')\n",
    "demand_df = pd.read_csv('../data/generation_and_demand/demand_profile.csv')\n",
    "demand = demand_df['demand_MW'].to_numpy()\n",
    "\n",
    "state_gdf['distance_to_transmission_line'] /= 1000\n",
    "\n",
    "new_data = {}\n",
    "for i in range(1, 25):\n",
    "    state_gdf[f'wind_power_kW_hour_{i}'] /= 1000\n",
    "    state_gdf[f'solar_power_kW_hour_{i}'] /= 1000\n",
    "    state_gdf[f'demand_{i}'] = demand[i-1]\n",
    "    new_data[f'wind_power_kW_hour_available_{i}'] = state_gdf[f'wind_power_kW_hour_{i}']\n",
    "    new_data[f'solar_power_kW_hour_available_{i}'] = state_gdf[f'solar_power_kW_hour_{i}']\n",
    "    \n",
    "# Create a new DataFrame from the dictionary\n",
    "new_df = pd.DataFrame(new_data)\n",
    "\n",
    "# Join the new DataFrame with the original DataFrame\n",
    "state_gdf = state_gdf.join(new_df)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "environment = RenewableEnergyEnvironment(state_gdf, demand)\n",
    "state_space = environment.state_tensor.to(device)\n",
    "action_space_size = environment.action_space_size\n",
    "\n",
    "agent = DQNAgent(state_space, action_space_size, device)\n",
    "\n",
    "episodes = 256\n",
    "\n",
    "epsilon_start = 1\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.965\n",
    "\n",
    "replay_buffer = ReplayBuffer(1024)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train(agent, environment, episodes, epsilon_start, epsilon_end, epsilon_decay, replay_buffer, batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c839bdb7-d4e5-4efb-b460-c1a1aef49c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(agent, environment, num_episodes, device):\n",
    "    # Ensure the directory for logs exists\n",
    "    dir_path = '../data/inference_logs/'\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    # Prepare the log file\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    filename = f'inference_log_{current_time}.csv'\n",
    "    file_path = os.path.join(dir_path, filename)\n",
    "\n",
    "    with open(file_path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Episode', 'Step', 'Action Type', 'Reward', 'Energy Output', 'Unmet Demand'])\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state = environment.reset().to(device)\n",
    "            done = False\n",
    "            step = 0\n",
    "\n",
    "            while not done:\n",
    "                # Select action based on the current state\n",
    "                action = agent.select_action(state.unsqueeze(0), 0)  # Epsilon set to 0 for greedy action selection\n",
    "                next_state, reward, done, _ = environment.step(action)\n",
    "                \n",
    "                # Log the results\n",
    "                cell_index, action_type = environment.mapping[action]\n",
    "                writer.writerow([episode + 1, step + 1, action_type, reward, environment.total_energy_output.sum(), environment.unmet_demand])\n",
    "\n",
    "                # Move to the next state\n",
    "                state = next_state.to(device)\n",
    "                step += 1\n",
    "            print(f\"Inference Episode {episode + 1} complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f218fc6-3cab-484a-b4c2-ed8364c3ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_baseline_model(environment, num_episodes, device):\n",
    "    # Ensure the directory for logs exists\n",
    "    dir_path = '../data/efficient_baseline_logs/'\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    # Prepare the log file\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    filename = f'efficient_baseline_log_{current_time}.csv'\n",
    "    file_path = os.path.join(dir_path, filename)\n",
    "\n",
    "    with open(file_path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Episode', 'Step', 'Action Type', 'Reward', 'Energy Output', 'Unmet Demand'])\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state = environment.reset().to(device)\n",
    "            total_power_output = np.zeros(24)  # Initialize total power output for 24 hours\n",
    "            all_actions = list(range(environment.action_space_size))\n",
    "            random.shuffle(all_actions)  # Shuffle the list of actions\n",
    "\n",
    "            step = 0\n",
    "            for action in all_actions:\n",
    "                cell_index, action_type = environment.mapping[action]\n",
    "\n",
    "                # Check if the action is valid\n",
    "                if not environment.is_valid_action(cell_index, action_type):\n",
    "                    continue  # Skip invalid actions\n",
    "\n",
    "                next_state, reward, done, _ = environment.step(action, verbose=False)\n",
    "\n",
    "                # Update total power output\n",
    "                total_power_output = environment.calculate_energy_output()\n",
    "                \n",
    "                # Check if total power output meets or exceeds demand\n",
    "                if np.all(total_power_output >= environment.initial_demand.sum() * 1000):\n",
    "                    done = True\n",
    "\n",
    "                # Log the results\n",
    "                writer.writerow([episode + 1, step + 1, action_type, reward, total_power_output.sum(), environment.unmet_demand])\n",
    "\n",
    "                if done:\n",
    "                    break  # Stop if terminal state is reached\n",
    "\n",
    "                step += 1\n",
    "\n",
    "            print(f\"Efficient Baseline Episode {episode + 1} complete.\")\n",
    "\n",
    "            # Calculate and log the unmet hourly demand\n",
    "            unmet_hourly_demand = environment.initial_demand * 1000 - total_power_output\n",
    "            unmet_hourly_demand[unmet_hourly_demand < 0] = 0\n",
    "            writer.writerow([episode + 1, 'Final', 'NA', 'NA', total_power_output.sum(), unmet_hourly_demand.sum()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4366bc3c-1eb8-41cc-b83f-86dadfdbd728",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficient Baseline Episode 1 complete.\n",
      "Efficient Baseline Episode 2 complete.\n",
      "Efficient Baseline Episode 3 complete.\n",
      "Efficient Baseline Episode 4 complete.\n",
      "Efficient Baseline Episode 5 complete.\n",
      "Efficient Baseline Episode 6 complete.\n",
      "Efficient Baseline Episode 7 complete.\n",
      "Efficient Baseline Episode 8 complete.\n",
      "Efficient Baseline Episode 9 complete.\n",
      "Efficient Baseline Episode 10 complete.\n",
      "Efficient Baseline Episode 11 complete.\n",
      "Efficient Baseline Episode 12 complete.\n",
      "Efficient Baseline Episode 13 complete.\n",
      "Efficient Baseline Episode 14 complete.\n",
      "Efficient Baseline Episode 15 complete.\n",
      "Efficient Baseline Episode 16 complete.\n",
      "Efficient Baseline Episode 17 complete.\n",
      "Efficient Baseline Episode 18 complete.\n",
      "Efficient Baseline Episode 19 complete.\n",
      "Efficient Baseline Episode 20 complete.\n",
      "Efficient Baseline Episode 21 complete.\n",
      "Efficient Baseline Episode 22 complete.\n",
      "Efficient Baseline Episode 23 complete.\n",
      "Efficient Baseline Episode 24 complete.\n",
      "Efficient Baseline Episode 25 complete.\n",
      "Efficient Baseline Episode 26 complete.\n",
      "Efficient Baseline Episode 27 complete.\n",
      "Efficient Baseline Episode 28 complete.\n",
      "Efficient Baseline Episode 29 complete.\n",
      "Efficient Baseline Episode 30 complete.\n",
      "Efficient Baseline Episode 31 complete.\n",
      "Efficient Baseline Episode 32 complete.\n",
      "Efficient Baseline Episode 33 complete.\n",
      "Efficient Baseline Episode 34 complete.\n",
      "Efficient Baseline Episode 35 complete.\n",
      "Efficient Baseline Episode 36 complete.\n",
      "Efficient Baseline Episode 37 complete.\n",
      "Efficient Baseline Episode 38 complete.\n",
      "Efficient Baseline Episode 39 complete.\n",
      "Efficient Baseline Episode 40 complete.\n",
      "Efficient Baseline Episode 41 complete.\n",
      "Efficient Baseline Episode 42 complete.\n",
      "Efficient Baseline Episode 43 complete.\n",
      "Efficient Baseline Episode 44 complete.\n",
      "Efficient Baseline Episode 45 complete.\n",
      "Efficient Baseline Episode 46 complete.\n",
      "Efficient Baseline Episode 47 complete.\n",
      "Efficient Baseline Episode 48 complete.\n",
      "Efficient Baseline Episode 49 complete.\n",
      "Efficient Baseline Episode 50 complete.\n",
      "Efficient Baseline Episode 51 complete.\n",
      "Efficient Baseline Episode 52 complete.\n",
      "Efficient Baseline Episode 53 complete.\n",
      "Efficient Baseline Episode 54 complete.\n",
      "Efficient Baseline Episode 55 complete.\n",
      "Efficient Baseline Episode 56 complete.\n",
      "Efficient Baseline Episode 57 complete.\n",
      "Efficient Baseline Episode 58 complete.\n",
      "Efficient Baseline Episode 59 complete.\n",
      "Efficient Baseline Episode 60 complete.\n",
      "Efficient Baseline Episode 61 complete.\n",
      "Efficient Baseline Episode 62 complete.\n",
      "Efficient Baseline Episode 63 complete.\n",
      "Efficient Baseline Episode 64 complete.\n",
      "Efficient Baseline Episode 65 complete.\n",
      "Efficient Baseline Episode 66 complete.\n",
      "Efficient Baseline Episode 67 complete.\n",
      "Efficient Baseline Episode 68 complete.\n",
      "Efficient Baseline Episode 69 complete.\n",
      "Efficient Baseline Episode 70 complete.\n",
      "Efficient Baseline Episode 71 complete.\n",
      "Efficient Baseline Episode 72 complete.\n",
      "Efficient Baseline Episode 73 complete.\n",
      "Efficient Baseline Episode 74 complete.\n",
      "Efficient Baseline Episode 75 complete.\n",
      "Efficient Baseline Episode 76 complete.\n",
      "Efficient Baseline Episode 77 complete.\n",
      "Efficient Baseline Episode 78 complete.\n",
      "Efficient Baseline Episode 79 complete.\n",
      "Efficient Baseline Episode 80 complete.\n",
      "Efficient Baseline Episode 81 complete.\n",
      "Efficient Baseline Episode 82 complete.\n",
      "Efficient Baseline Episode 83 complete.\n",
      "Efficient Baseline Episode 84 complete.\n",
      "Efficient Baseline Episode 85 complete.\n",
      "Efficient Baseline Episode 86 complete.\n",
      "Efficient Baseline Episode 87 complete.\n",
      "Efficient Baseline Episode 88 complete.\n",
      "Efficient Baseline Episode 89 complete.\n",
      "Efficient Baseline Episode 90 complete.\n",
      "Efficient Baseline Episode 91 complete.\n",
      "Efficient Baseline Episode 92 complete.\n",
      "Efficient Baseline Episode 93 complete.\n",
      "Efficient Baseline Episode 94 complete.\n",
      "Efficient Baseline Episode 95 complete.\n",
      "Efficient Baseline Episode 96 complete.\n",
      "Efficient Baseline Episode 97 complete.\n",
      "Efficient Baseline Episode 98 complete.\n",
      "Efficient Baseline Episode 99 complete.\n",
      "Efficient Baseline Episode 100 complete.\n"
     ]
    }
   ],
   "source": [
    "#baseline_model(environment, num_episodes=100, device=device)\n",
    "efficient_baseline_model(environment, num_episodes=100, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c565a4c2-4f90-4c69-906f-2be366811db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def parse_logs_and_plot(log_directory):\n",
    "    # Find the latest log file\n",
    "    list_of_files = glob.glob(f'{log_directory}/efficient_baseline_log_*.csv')\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "\n",
    "    # Read the log file into a DataFrame\n",
    "    df = pd.read_csv(latest_file)\n",
    "\n",
    "    # Filter rows that mark the end of each episode\n",
    "    final_unmet_demand = df[df['Step'] == 'Final']['Unmet Demand']\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.boxplot(final_unmet_demand)\n",
    "    plt.title('Distribution of Final Unmet Energy Demand per Episode')\n",
    "    plt.ylabel('Unmet Demand (kW)')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return final_unmet_demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb03f4bb-58a5-4464-b4ae-b54ff3e8201d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a09f39e-0d52-454a-a60b-6dfa931b703c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_unmet_demand' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfinal_unmet_demand\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_unmet_demand' is not defined"
     ]
    }
   ],
   "source": [
    "final_unmet_demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf7a5836-1832-4fd4-9f5c-6aa9e522292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20c01eff-1f9c-4256-ac83-5ff76bdff5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_directory = '../data/efficient_baseline_logs/'\n",
    "demand = parse_logs_and_plot(log_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04a3334b-826a-42dc-9cb7-e88d931359ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25867402.354994126"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demand.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f4337f-f614-43a3-a3a4-1b67767757c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
