{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81222bf5-c8bd-4341-8569-71e7b8e0cd86",
   "metadata": {},
   "source": [
    "# Overview\n",
    "## State Space\n",
    "- The state space consists of an 80,000-cell grid, representing different geographical locations in Puerto Rico.\n",
    "- Each cell has attributes like solar PV output, wind power density, elevation, slope, cyclone risk score, building density, road density, and distance to transmission lines.\n",
    "- Approximately 70% of cells are unavailable for development due to environmental or other constraints.\n",
    "\n",
    "## Action Space\n",
    "- Two types of actions are available: building a solar array or a wind turbine.\n",
    "- Actions can be taken on any available cell.\n",
    "\n",
    "## Rewards and Costs\n",
    "The reward function should incorporate:\n",
    "- Energy production potential (solar and wind).\n",
    "- Costs or penalties associated with building on certain terrains (e.g., high elevation or steep slopes).\n",
    "- Penalties for building in high cyclone risk areas.\n",
    "- Incentives for maintaining a balance between solar and wind energy.\n",
    "- Incentives for early deployment and distributed grid development.\n",
    "- Penalties for high building or road density areas.\n",
    "- Distance to transmission lines.\n",
    "\n",
    "## RL Model\n",
    "- Model Choice: Given the size of the state space, a model-based RL algorithm (like Deep Q-Networks or Actor-Critic methods) is suitable.\n",
    "- Representation: The state representation should include the current status of each grid cell (whether it has a solar array, a wind turbine, or is vacant) along with its attributes.\n",
    "- Sequence of Actions: The RL agent will sequentially choose actions (where to build next) based on the current state of the grid.\n",
    "- Terminal State: The agent is done when the environment reaches a certain level of energy capacity or after a fixed number of steps.\n",
    "\n",
    "# Implementation Steps\n",
    "## Environment Setup: \n",
    "- Implement the environment to reflect the grid and its dynamics, including applying the binary mask for unavailable cells.\n",
    "- The step(action) method should update the grid state based on the chosen action and calculate the immediate reward or cost.\n",
    "\n",
    "## Agent Development:\n",
    "- Use PyTorch for implementing the neural network models for the agent.\n",
    "The agent needs to learn a policy that maximizes long-term rewards, considering the complex reward structure and large state space.\n",
    "\n",
    "## Training and Evaluation:\n",
    "- Set up a training loop where the agent interacts with the environment, receives feedback, and improves its policy.\n",
    "- Periodically evaluate the agent's performance, possibly using separate evaluation episodes or metrics like total energy capacity achieved or adherence to environmental constraints.\n",
    "\n",
    "## Hyperparameter Tuning:\n",
    "- Adjust learning rates, exploration rates, discount factors, and network architecture as needed to improve performance.\n",
    " \n",
    "## Scalability:\n",
    "Due to the large state space, may need to:\n",
    "- use function approximation for value functions\n",
    "- prioritizing important experiences in the replay buffer\n",
    "- parallelize computation process\n",
    "\n",
    "## Visualization and Analysis:\n",
    "- Develop tools to visualize the evolving grid layout and analyze the trade-offs made by the RL agent between different objectives (like energy maximization vs. environmental constraints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a0776a-f41d-41ba-9261-9079553eae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "\n",
    "import random\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c724dafa-ff4d-4fae-babe-283dcb25bb04",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac168609-d297-4963-b880-bcfdc86c2f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenewableEnergyEnvironment:\n",
    "    def __init__(self, grid_gdf):\n",
    "        self.step_count = 0\n",
    "        self.demand_satisfied_ratio = 0\n",
    "\n",
    "        # Daily cost calculated in data_preprocessing notebook\n",
    "        # Multiplying by 4 because grid cell size was changed from 500m to 1 km\n",
    "        self.wind_daily_cost = 4 * 821.68\n",
    "        self.solar_daily_cost = 4 * 3007.26\n",
    "        \n",
    "        self.grid_columns = ['distance_to_transmission_line', \n",
    "                             'cyclone_risk',\n",
    "                             'occupied']\n",
    "        self.grid_columns += [f'wind_power_kW_hour_{i}' for i in range(1,25)]\n",
    "        self.grid_columns += [f'solar_power_kW_hour_{i}' for i in range(1,25)]\n",
    "        \n",
    "        # Initialize the environment\n",
    "        self.starting_environment = grid_gdf\n",
    "        self.state_gdf = grid_gdf.copy()\n",
    "        self.state_tensor = self.gdf_to_tensor(self.state_gdf)\n",
    "        self.mapping, self.action_space_size = self.create_action_to_gdf_mapping()\n",
    "        \n",
    "        self.total_energy_output = 0\n",
    "        self.stored_energy = 0\n",
    "\n",
    "        demand_df = pd.read_csv('../data/generation_and_demand/demand_profile.csv')\n",
    "        self.initial_demand = demand_df['demand_MW'].to_numpy() * 1000\n",
    "        self.current_demand = demand_df['demand_MW'].to_numpy() * 1000\n",
    "        #print('Demand intialized in environment:', self.current_demand)\n",
    "        self.unmet_demand = self.initial_demand.sum()\n",
    "        self.total_demand = self.unmet_demand\n",
    "\n",
    "        # LEGACY - the following parameters are no longer in use\n",
    "        self.decay_rate = 0.1 #TODO determine good decay rate\n",
    "        self.max_distance = 1000 #TODO get max distance between two cells\n",
    "        self.weights = {\n",
    "        'transmission_build_cost': -1.0,\n",
    "        'early_choice_reward': 1.0,\n",
    "        'distributed_grid_reward': 1.0,\n",
    "        }\n",
    "        \n",
    "    def reset(self):\n",
    "        # Reset the environment to the initial state\n",
    "        self.state_gdf = self.starting_environment.copy()\n",
    "        self.state_tensor = self.gdf_to_tensor(self.starting_environment)\n",
    "        self.total_energy_output = 0\n",
    "        self.step_count = 0\n",
    "        return self.state_tensor\n",
    "\n",
    "    def gdf_to_tensor(self, gdf):\n",
    "        # Calculate grid dimensions\n",
    "        x_start = 100000\n",
    "        x_end = 300000\n",
    "        y_start = 200000\n",
    "        y_end = 300000\n",
    "        square_size = 1000\n",
    "        \n",
    "        grid_width = int((x_end - x_start) / square_size)\n",
    "        grid_height = int((y_end - y_start) / square_size)\n",
    "\n",
    "        transposed_data = self.state_gdf[self.grid_columns].values.T\n",
    "        grid_data = np.array([feature.reshape(100, 200) for feature in transposed_data])\n",
    "        tensor = torch.tensor(grid_data, dtype=torch.float32)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def create_action_to_gdf_mapping(self):\n",
    "        unmasked_gdf = self.state_gdf[self.state_gdf['masked'] == 0]\n",
    "        \n",
    "        mapping = {}\n",
    "        action_idx = 0  # Initialize action index\n",
    "    \n",
    "        for _, row in unmasked_gdf.iterrows():\n",
    "            # Check for valid solar action\n",
    "            if row['slope'] <= 8.749:  # Slope check for solar\n",
    "                mapping[action_idx] = (row.name, 'solar')\n",
    "                action_idx += 1\n",
    "    \n",
    "            # Check for valid wind action\n",
    "            if row['slope'] <= 26.795:  # Slope check for wind\n",
    "                mapping[action_idx] = (row.name, 'wind')\n",
    "                action_idx += 1\n",
    "        \n",
    "        action_space_size = action_idx  # Total number of valid actions\n",
    "        return mapping, action_space_size\n",
    "\n",
    "    def output_stats(self, writer, cell_index, action_type, reward, invalid=False):\n",
    "        if writer:\n",
    "            writer.writerow([None,None,None,cell_index, action_type, int(reward), int(self.total_energy_output.sum()), int(self.unmet_demand)])\n",
    "        if invalid:\n",
    "            print(f'Cell: {cell_index:<6d} | Step: INV  | Action: {action_type:<5} | Reward: -9999999 | Energy Output: {int(self.total_energy_output.sum()):<10} | Unmet Demand: {int(self.unmet_demand):<10}')\n",
    "        else:\n",
    "            print(f'Cell: {cell_index:<6d} | Step: {self.step_count:<4d} | Action: {action_type:<5} | Reward: {reward:8.5f} | Energy Output: {int(self.total_energy_output.sum()):<10} | Unmet Demand: {int(self.unmet_demand):<10}')\n",
    "\n",
    "    def step(self, action, writer=None):\n",
    "        # Apply the action to the environment and return the result\n",
    "        # action: Tuple (cell_index, action_type) where action_type could be 'solar' or 'wind'\n",
    "\n",
    "        # Map action from agent output to action in terms of state_gdf\n",
    "        cell_index, action_type = self.mapping[action]\n",
    "\n",
    "        # Check if the action is valid\n",
    "        if not self.is_valid_action(cell_index, action_type):\n",
    "            reward = -9999999 # Penalty for invalid action\n",
    "            done = self.is_terminal_state() # Check if in terminal state\n",
    "            self.total_energy_output = self.calculate_energy_output() # Calculate total energy output\n",
    "            self.output_stats(writer, cell_index, action_type, reward, invalid=True) # Print results and write results to file\n",
    "            return self.state_tensor, reward, done, {}\n",
    "\n",
    "        # Calculate the total reward before applying the action\n",
    "        total_reward_before_action = self.calculate_reward()\n",
    "    \n",
    "        # Apply the action\n",
    "        self.apply_action(cell_index, action_type)\n",
    "    \n",
    "        # Calculate the total reward after applying the action\n",
    "        total_reward_after_action = self.calculate_reward()\n",
    "    \n",
    "        # The reward for the action is the difference in total reward\n",
    "        reward = total_reward_after_action - total_reward_before_action\n",
    "        reward = reward / 100000\n",
    "\n",
    "        # Update total energy output or other state attributes as needed\n",
    "        self.total_energy_output = self.calculate_energy_output()\n",
    "        \n",
    "        # Check if the state is terminal\n",
    "        done = self.is_terminal_state()\n",
    "\n",
    "        # Print results and write results to file\n",
    "        self.output_stats(writer, cell_index, action_type, reward)\n",
    "        \n",
    "        # Update the state tensor with the new state gdf\n",
    "        self.state_tensor = self.gdf_to_tensor(self.state_gdf)\n",
    "\n",
    "        self.step_count += 1\n",
    "        \n",
    "        return self.state_tensor, reward, done, {}\n",
    "\n",
    "    def is_valid_action(self, cell_index, action_type):\n",
    "        # Implement logic to check if an action is valid\n",
    "        cell = self.state_gdf.iloc[cell_index]\n",
    "        if cell['masked']: # This should never occur\n",
    "            print('Error: Attempting to build on a masked cell')\n",
    "            return False\n",
    "        elif cell['occupied']:\n",
    "            return False\n",
    "        elif action_type == 'solar' and cell['slope'] > 8.749: # This should never occur\n",
    "            print('Error: Attempting to build solar on slope of more than 5%')\n",
    "            return False\n",
    "        elif action_type == 'wind' and cell['slope'] > 26.795: # This should never occur\n",
    "            print('Error: Attempting to build wind on slope of more than 15%')\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def apply_action(self, cell_index, action_type):\n",
    "        # Implement the changes to the environment based on the action\n",
    "        # Example: Mark the cell as occupied and record the type of installation\n",
    "        self.state_gdf.at[cell_index, 'occupied'] = 1\n",
    "        self.state_gdf.at[cell_index, 'installation_type'] = action_type\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        # Solar installation cost\n",
    "        solar_cost = self.calculate_solar_cost()\n",
    "\n",
    "        # Wind turbine installation cost\n",
    "        wind_cost = self.calculate_wind_cost()\n",
    "        \n",
    "        # Solar power Reward\n",
    "        power_output_reward = self.calculate_power_output_reward()\n",
    "\n",
    "        # Transmission build cost\n",
    "        transmission_build_cost = self.calculate_transmission_build_cost()\n",
    "        \n",
    "        # Cyclone risk cost\n",
    "        cyclone_risk_cost = self.calculate_cyclone_risk_cost()\n",
    "    \n",
    "        # Distribution reward\n",
    "        # distributed_grid_reward = self.calculate_distributed_grid_reward()\n",
    "    \n",
    "        # Early choice reward\n",
    "        # early_choice_reward = self.time_dependent_reward_factor()\n",
    "\n",
    "        total_reward = power_output_reward + solar_cost + wind_cost + cyclone_risk_cost + transmission_build_cost\n",
    "        \n",
    "        return total_reward\n",
    "\n",
    "    def calculate_energy_output(self):\n",
    "        # Filter for solar and wind installations\n",
    "        solar_gdf = self.state_gdf[self.state_gdf['installation_type'] == 'solar']\n",
    "        wind_gdf = self.state_gdf[self.state_gdf['installation_type'] == 'wind']\n",
    "    \n",
    "        # Prepare column names for solar and wind power\n",
    "        solar_power_columns = [f'solar_power_kW_hour_{i}' for i in range(1, 25)]\n",
    "        wind_power_columns = [f'wind_power_kW_hour_{i}' for i in range(1, 25)]\n",
    "    \n",
    "        # Vectorized sum of power output for solar and wind for each hour\n",
    "        total_solar_power = solar_gdf[solar_power_columns].sum().to_numpy() * 1000\n",
    "        total_wind_power = wind_gdf[wind_power_columns].sum().to_numpy() * 1000\n",
    "    \n",
    "        total_power = total_solar_power + total_wind_power\n",
    "        \n",
    "        return total_power\n",
    "\n",
    "    def calculate_power_output_reward(self):\n",
    "        \"\"\"Uses supply and demand curve to determine the amount of demand satisfied by\n",
    "        the solar and wind installations\"\"\"\n",
    "        cost_kWh = .22  # TODO get more rigorous number \n",
    "        # Storage cost = $400/kWh over the 15 year lifetime of a 4 hour battery\n",
    "        # cost_storage_kWh = 400 / (365.25 * 15)\n",
    "        demand = self.initial_demand\n",
    "        #print('Getting demand to calculate power output reward:', self.initial_demand)\n",
    "\n",
    "        # Calculate the reward using vectorized minimum\n",
    "        total_power = self.calculate_energy_output()\n",
    "        demand_satisfied = np.minimum(demand, total_power)\n",
    "        #print('Demand satisfied in calc pow out rew:', demand_satisfied)\n",
    "        self.demand_satisfied_ratio = demand_satisfied / demand\n",
    "\n",
    "        self.current_demand = self.update_demand(self.current_demand)\n",
    "        \n",
    "        power_reward = demand_satisfied.sum() * cost_kWh\n",
    "        # self.stored_energy = np.maximum(total_power - demand, 0).sum()\n",
    "        # storage_cost = self.stored_energy * cost_storage_kWh\n",
    "\n",
    "        reward = power_reward # - storage_cost\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def update_demand(self, demand):\n",
    "        #print('Demand in update demand:', demand)\n",
    "        unsatisfied_demand_ratio = 1 - self.demand_satisfied_ratio\n",
    "        for i in range(0, 24):\n",
    "            demand[i] *= unsatisfied_demand_ratio[i-1]\n",
    "            #self.state_gdf[f'demand_{i}'] *= unsatisfied_demand_ratio[i-1]\n",
    "        #print('Updated demand in update_demand:', self.current_demand)\n",
    "        return demand \n",
    "    \n",
    "    def calculate_solar_cost(self):\n",
    "        \"\"\"Cost of installing solar array on cell. Based on elevation and slope\"\"\"\n",
    "        return -self.solar_daily_cost * len(self.state_gdf[self.state_gdf['installation_type'] == 'solar'])\n",
    "\n",
    "    def calculate_wind_cost(self):\n",
    "        \"\"\"Cost of installing wind turbine on cell. Based on elevation and slope\"\"\"\n",
    "        return -self.wind_daily_cost * len(self.state_gdf[self.state_gdf['installation_type'] == 'wind'])\n",
    "\n",
    "    def transmission_line_cost_per_km(self, distance):\n",
    "        # $2.29 million per mile divided by 1.60934 to get into km, \n",
    "        # then divided by 25 * 365.25 for daily costs with 25 year decommission time\n",
    "        distance /= 1000 # Convert to km\n",
    "        KM_PER_MILE = 1.60934\n",
    "        COST_PER_KM = 2.29 * 1000000 / (KM_PER_MILE * 25 * 365.25) \n",
    "        SHORT_DISTANCE_THRESHOLD = 3 * KM_PER_MILE # Threshold for short distance \n",
    "        MEDIUM_DISTANCE_THRESHOLD = 10 * KM_PER_MILE # Threshold for medium distance \n",
    "        \n",
    "        if distance < SHORT_DISTANCE_THRESHOLD:\n",
    "            cost_modifier = 1.5  # 50% increase for less than 3 miles\n",
    "        elif distance < MEDIUM_DISTANCE_THRESHOLD:\n",
    "            cost_modifier = 1.2  # 20% increase for 3-10 miles\n",
    "        else:\n",
    "            cost_modifier = 1  # No modification for more than 10 miles\n",
    "        return -distance * COST_PER_KM * cost_modifier\n",
    "    \n",
    "    def calculate_transmission_build_cost(self):\n",
    "        occupied_cells = self.state_gdf[self.state_gdf['occupied'] == 1]\n",
    "        \n",
    "        # Check if there is only one occupied cell\n",
    "        if len(occupied_cells) == 0:\n",
    "            return 0\n",
    "        elif len(occupied_cells) == 1:\n",
    "            # For a single occupied cell, use the distance to transmission line for cost calculation\n",
    "            occupied_cell = occupied_cells.iloc[0]\n",
    "            distance_km = occupied_cell['distance_to_transmission_line'] * 1000\n",
    "            build_cost = self.transmission_line_cost_per_km(distance_km)\n",
    "        else:\n",
    "            # Get coordinates of occupied cells\n",
    "            coords = np.array(list(zip(occupied_cells.geometry.centroid.x, occupied_cells.geometry.centroid.y)))\n",
    "    \n",
    "            # Calculate pairwise distances between occupied cells\n",
    "            distances = cdist(coords, coords)\n",
    "    \n",
    "            # Replace zeros in distance matrix with np.inf to avoid zero distance to itself\n",
    "            np.fill_diagonal(distances, np.inf)\n",
    "    \n",
    "            # Find the nearest installation for each installation\n",
    "            nearest_installation_distances = np.min(distances, axis=1)\n",
    "    \n",
    "            # Determine the relevant distance for cost calculation\n",
    "            relevant_distances = np.minimum(nearest_installation_distances, occupied_cells['distance_to_transmission_line'].to_numpy()) * 1000\n",
    "    \n",
    "            # Calculate build cost\n",
    "            build_costs = [self.transmission_line_cost_per_km(distance) for distance in relevant_distances]\n",
    "            build_cost = sum(build_costs)\n",
    "            \n",
    "        return build_cost\n",
    "\n",
    "    def calculate_cyclone_risk_cost(self):\n",
    "        if len(self.state_gdf[self.state_gdf.installation_type == 'wind']) == 0:\n",
    "            return 0\n",
    "        # Wind operational expenses = $40/kW/yr\n",
    "        # Wind turbine capacity is 3 MW\n",
    "        # To get daily cost, multiply by 3000 (3 MW = 3000 kW) and divide by 365.25 days in a year\n",
    "        wind_opex = 4 * 40 * 3000 / 365.25\n",
    "        # Wind capital expenses = $1501/kW\n",
    "        # To get daily cost, multiply by 3000 (3 MW = 3000 kW) and divide by (25 years of lifetime * 365.25 days in a year)\n",
    "        wind_capex = 4 * 1501 * 3000 / (25 * 365.25)\n",
    "        cyclone_risk_cost = -(self.state_gdf[self.state_gdf.installation_type == 'wind']['cyclone_risk'] * (wind_capex - wind_opex)).sum()\n",
    "        return cyclone_risk_cost\n",
    "    \n",
    "    def calculate_distributed_grid_reward(self):\n",
    "        # Extract the coordinates of the occupied cells (where installations are located)\n",
    "        occupied_cells = self.state_gdf[self.state_gdf['occupied'] == 1]\n",
    "        if len(occupied_cells) < 2:\n",
    "            # If there are less than two installations, we cannot calculate distances\n",
    "            return 1\n",
    "    \n",
    "        coords = np.array(list(zip(occupied_cells.geometry.x, occupied_cells.geometry.y)))\n",
    "    \n",
    "        # Calculate pairwise distances between all occupied cells\n",
    "        distances = pdist(coords)\n",
    "    \n",
    "        # Calculate the average distance. The larger this is, the more distributed the installations are.\n",
    "        avg_distance = np.mean(distances)\n",
    "    \n",
    "        # Normalize the reward such that it ranges between 0 and 1\n",
    "        normalized_reward = avg_distance / self.max_distance\n",
    "    \n",
    "        return normalized_reward\n",
    "    \n",
    "    def time_dependent_reward_factor(self):\n",
    "        # A function to calculate the time-dependent reward factor\n",
    "        # It decreases with each year from the base year\n",
    "        action_number = self.state_gdf.occupied.sum()\n",
    "        return 1 / (1 + self.decay_rate * action_number)\n",
    "        \n",
    "    def is_terminal_state(self):\n",
    "        # The episode ends when there is no unmet demand\n",
    "        self.unmet_demand = np.maximum(self.initial_demand - self.total_energy_output, 0).sum()\n",
    "        return self.unmet_demand <= 0 or self.total_energy_output.sum() >= self.total_demand \n",
    "\n",
    "    def render(self):\n",
    "        # Optional: Implement a method to visualize the current state of the environment\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cb54eb-f64b-4bd0-b00f-1dbe3d5c219a",
   "metadata": {},
   "source": [
    "# Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7a787f8-9e25-4b21-859b-34979915fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected\n",
    "\n",
    "    # def __init__(self, input_shape, num_actions):\n",
    "    #     super(DQN, self).__init__()\n",
    "\n",
    "    #     #self.num_features = input_shape[0]\n",
    "\n",
    "    #     # Fully connected layers\n",
    "    #     self.fc1 = nn.Linear(input_shape[0], 512)\n",
    "    #     self.fc2 = nn.Linear(512, 512)\n",
    "    #     self.fc3 = nn.Linear(512, 512)\n",
    "    #     self.fc4 = nn.Linear(512, num_actions)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     # Flatten the input tensor\n",
    "    #     x = x.view(x.size(0), -1)\n",
    "\n",
    "    #     # Fully connected layers with ReLU activation\n",
    "    #     x = F.relu(self.fc1(x))\n",
    "    #     x = F.relu(self.fc2(x))\n",
    "    #     x = F.relu(self.fc3(x))\n",
    "\n",
    "    #     # Output layer\n",
    "    #     return self.fc4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0a461ea-ffe6-40d2-8e8f-93913535d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla CNN\n",
    "\n",
    "# def __init__(self, input_shape, num_actions):\n",
    "    #     super(DQN, self).__init__()\n",
    "    #     self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=3, stride=2)\n",
    "    #     self.bn1 = nn.BatchNorm2d(32)\n",
    "    #     self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=2)\n",
    "    #     self.bn2 = nn.BatchNorm2d(32)\n",
    "    #     self.conv3 = nn.Conv2d(32, 16, kernel_size=3, stride=2)\n",
    "    #     self.bn3 = nn.BatchNorm2d(16)\n",
    "\n",
    "    #     self.input_shape = input_shape  # Store input_shape for feature size calculation\n",
    "    #     print(f'Input shape: {input_shape[0]}, Feature size: {self._feature_size()}, n Actions: {num_actions}')\n",
    "    #     self.fc1 = nn.Linear(self._feature_size(), 4096)\n",
    "    #     self.fc2 = nn.Linear(4096, num_actions)\n",
    "\n",
    "    # def _feature_size(self):\n",
    "    #     with torch.no_grad():\n",
    "    #         return self.bn3(self.conv3(self.bn2(self.conv2(self.bn1(self.conv1(torch.zeros(1, *self.input_shape))))))).view(1, -1).size(1)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = F.relu(self.bn1(self.conv1(x)))\n",
    "    #     x = F.relu(self.bn2(self.conv2(x)))\n",
    "    #     x = F.relu(self.bn3(self.conv3(x)))\n",
    "    #     x = x.view(x.size(0), -1)\n",
    "    #     x = F.relu(self.fc1(x))\n",
    "    #     return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f382086d-cee9-46f6-8f48-4d6a81c8a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions, demand):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.demand = torch.tensor(demand, dtype=torch.float32)\n",
    "        #print('Demand intialized in DQN:', self.demand)\n",
    " \n",
    "        self.num_nontemporal_channels = 2\n",
    "        #self.num_temporal_channels = input_shape[0] - self.num_nontemporal_channels  # Subtracting the non-temporal channels\n",
    "        self.num_temporal_channels = 48\n",
    "        self.time_dimension = 24\n",
    "        self.grid_height = input_shape[1]\n",
    "        self.grid_width = input_shape[2]\n",
    "\n",
    "        # 3D Convolutional layers for temporal features\n",
    "        self.conv3d1 = nn.Conv3d(self.num_temporal_channels // 24, 2, kernel_size=(3, 3, 3), stride=(1, 2, 2))\n",
    "        self.bn3d1 = nn.BatchNorm3d(2)\n",
    "        self.conv3d2 = nn.Conv3d(2, 2, kernel_size=(3, 3, 3), stride=(1, 2, 2))\n",
    "        self.bn3d2 = nn.BatchNorm3d(2)\n",
    "        # Add more layers if necessary\n",
    "\n",
    "        # 2D Convolutional layers for non-temporal features\n",
    "        self.conv2d1 = nn.Conv2d(self.num_nontemporal_channels, self.num_nontemporal_channels, kernel_size=3, stride=2)\n",
    "        self.bn2d1 = nn.BatchNorm2d(self.num_nontemporal_channels)\n",
    "        self.conv2d2 = nn.Conv2d(self.num_nontemporal_channels, self.num_nontemporal_channels, kernel_size=3, stride=2)\n",
    "        self.bn2d2 = nn.BatchNorm2d(self.num_nontemporal_channels)\n",
    "        # Add more layers if necessary\n",
    "\n",
    "        # Fully connected layer for processing 'occupied' cells feature\n",
    "        # self.fc_occupied = nn.Linear(self.grid_height * self.grid_width, 1024)\n",
    "        # self.fc_demand = nn.Linear(24, 1024)\n",
    "\n",
    "        # Calculate the size of the combined feature vector\n",
    "        # combined_feature_size = self.calculate_combined_feature_size(input_shape)\n",
    "\n",
    "        # Fully connected layers for decision making\n",
    "        self.fc1 = nn.Linear(49416, 1024)\n",
    "        self.fc2 = nn.Linear(21024, num_actions)\n",
    "\n",
    "    def calculate_combined_feature_size(self, input_shape):\n",
    "        # Dummy inputs for calculating feature sizes\n",
    "        dummy_temporal = torch.zeros(1, 72 // 24, self.time_dimension, self.grid_height, self.grid_width)\n",
    "        dummy_nontemporal = torch.zeros(1, self.num_nontemporal_channels, self.grid_height, self.grid_width)\n",
    "        dummy_occupied = torch.zeros(1, 1, self.grid_height, self.grid_width)\n",
    "\n",
    "        # Forward pass through convolutional layers\n",
    "        x_temporal = self.bn3d2(self.conv3d2(self.bn3d1(self.conv3d1(dummy_temporal))))\n",
    "        x_nontemporal = self.bn2d2(self.conv2d2(self.bn2d1(self.conv2d1(dummy_nontemporal))))\n",
    "        x_occupied = dummy_occupied.view(1, -1)\n",
    "\n",
    "        # Calculate the total number of features\n",
    "        total_features = x_temporal.numel() + x_nontemporal.numel() + x_occupied.numel()\n",
    "        return total_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Split input into temporal and non-temporal components\n",
    "        x_temporal = x[:, 3:, :, :].view(x.size(0), self.num_temporal_channels // 24, self.time_dimension, self.grid_height, self.grid_width)\n",
    "        x_nontemporal = x[:, :2, :, :]  # Distance to transmission line & cyclone risk\n",
    "        x_occupied = x[:, 2:3, :, :].view(x.size(0), -1)  # Occupied cells\n",
    "\n",
    "        # Process temporal features\n",
    "        x_temporal = F.relu(self.bn3d1(self.conv3d1(x_temporal)))\n",
    "        x_temporal = F.relu(self.bn3d2(self.conv3d2(x_temporal)))\n",
    "        x_temporal = x_temporal.view(x_temporal.size(0), -1)\n",
    "\n",
    "        # Process non-temporal features\n",
    "        x_nontemporal = F.relu(self.bn2d1(self.conv2d1(x_nontemporal)))\n",
    "        x_nontemporal = F.relu(self.bn2d2(self.conv2d2(x_nontemporal)))\n",
    "        x_nontemporal = x_nontemporal.view(x_nontemporal.size(0), -1)\n",
    "\n",
    "        # Process 'demand' feature\n",
    "        # x_demand = F.relu(self.fc_demand(self.x_demand))\n",
    "        # Process 'occupied' cells feature\n",
    "        #x_occupied = F.relu(self.fc_occupied(x_occupied))\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        x_demand = self.demand.unsqueeze(0).repeat(batch_size, 1)\n",
    "        #print('Demand in forward:', self.demand, x_demand)\n",
    "\n",
    "        # Combine features\n",
    "        x_combined = torch.cat((x_temporal, x_nontemporal, x_demand), dim=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x_combined = F.relu(self.fc1(x_combined))\n",
    "        x_combined = torch.cat((x_combined, x_occupied), dim=1)\n",
    "        return self.fc2(x_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ab3b4f-b218-4ce4-bf5d-482d2eae1923",
   "metadata": {},
   "source": [
    "# DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3b2715-0e1f-4729-9844-fd3aee881f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_space, action_space_size, device, demand):\n",
    "        #print('Demand intialized in DQNAgent:', demand)\n",
    "        self.action_space_size = action_space_size\n",
    "        self.model = DQN(state_space.shape, action_space_size, demand).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            # Choose the best action (exploitation)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.model(state)\n",
    "                action = q_values.max(1)[1].item()  # Select the action with the highest Q-value\n",
    "        else:\n",
    "            # Choose a random action (exploration)\n",
    "            action = random.randrange(self.action_space_size)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, batch):\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "        # Compute Q values\n",
    "        current_q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.model(next_states).max(1)[0]\n",
    "        expected_q_values = rewards + (1 - dones) * next_q_values.detach()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = torch.nn.functional.mse_loss(current_q_values, expected_q_values)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78facf33-0887-46bb-ba62-89ca956e553d",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64ee56fc-3afc-4389-b78f-4bbe022f18ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "\n",
    "        # Stack the tuples of tensors to create a single tensor for each component\n",
    "        states = torch.stack(states).to(device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float).to(device)\n",
    "        next_states = torch.stack(next_states).to(device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float).to(device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccdad5c-e260-423f-9989-0c144f28de9f",
   "metadata": {},
   "source": [
    "# Save the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d58104a-c536-4cf7-817a-4bd064984466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_agent(agent):\n",
    "    # Ensure the models directory exists\n",
    "    dir_path = '../models/'\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    # Save the agent\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    filename = f'agent_model_{current_time}.pth'\n",
    "    torch.save(agent.model.state_dict(), f'../models/{filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254993b-b1b0-4822-82c9-ca9ba3294557",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4036b194-8954-45df-b0aa-aacef4da5c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, environment, episodes, epsilon_start, epsilon_end, epsilon_decay, replay_buffer, batch_size, device):\n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    filename = f'step_log_{current_time}.csv'\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    dir_path = '../data/logs/'\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    # Complete file path\n",
    "    file_path = os.path.join(dir_path, filename)\n",
    "\n",
    "    # Open a file for writing\n",
    "    with open(file_path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the header\n",
    "        writer.writerow(['Episode', 'Total Reward', 'Epsilon', 'Cell Index', 'Action Type', 'Reward', 'Energy Output', 'Unmet Demand'])\n",
    "    \n",
    "        for episode in range(episodes):\n",
    "            state = environment.reset().to(device)\n",
    "            done = False\n",
    "            total_reward = 0  # To keep track of total reward per episode\n",
    "    \n",
    "            while not done:\n",
    "                action = agent.select_action(state.unsqueeze(0), epsilon)\n",
    "                \n",
    "                next_state, reward, done, _ = environment.step(action, writer=writer)\n",
    "\n",
    "                agent.model.demand = torch.tensor(environment.current_demand, dtype=torch.float32)\n",
    "                #print('Demand in training loop:', agent.model.demand)\n",
    "    \n",
    "                # Store experience in replay buffer\n",
    "                replay_buffer.store(state, action, reward, next_state, torch.tensor(bool(done)))\n",
    "    \n",
    "                # Check if buffer is ready for sampling\n",
    "                if len(replay_buffer) > batch_size:\n",
    "                    # Sample a batch from replay buffer\n",
    "                    batch = replay_buffer.sample(batch_size)\n",
    "                    # Learn from the sampled experiences\n",
    "                    agent.learn(batch)\n",
    "    \n",
    "                # Update state\n",
    "                state = next_state.to(device)\n",
    "                total_reward += reward\n",
    "            \n",
    "            # Write episode data to the file\n",
    "            writer.writerow([episode + 1, total_reward, epsilon, None, None, None, None, None])\n",
    "\n",
    "            # Log training progress\n",
    "            print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "            # Decay epsilon\n",
    "            epsilon = max(epsilon_end, epsilon_decay * epsilon)  # Ensure epsilon doesn't go below the minimum\n",
    "\n",
    "    save_agent(agent)\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d0b32-09a3-4aef-bcaf-51f0bc870bcf",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39715d08-6208-4791-9064-e5de7a05abed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Cell: 13221  | Step: 0    | Action: wind  | Reward:  0.01417 | Energy Output: 22178      | Unmet Demand: 57165321  \n",
      "Cell: 10236  | Step: 1    | Action: wind  | Reward:  0.22353 | Energy Output: 139522     | Unmet Demand: 57047977  \n",
      "Cell: 12122  | Step: 2    | Action: wind  | Reward:  0.06665 | Energy Output: 188743     | Unmet Demand: 56998756  \n",
      "Cell: 3268   | Step: 3    | Action: solar | Reward:  1.01625 | Energy Output: 705353     | Unmet Demand: 56482146  \n",
      "Cell: 4725   | Step: 4    | Action: wind  | Reward:  0.04261 | Energy Output: 740303     | Unmet Demand: 56447196  \n",
      "Cell: 14634  | Step: 5    | Action: wind  | Reward: -0.01958 | Energy Output: 752585     | Unmet Demand: 56434914  \n",
      "Cell: 4635   | Step: 6    | Action: wind  | Reward:  0.06841 | Energy Output: 803508     | Unmet Demand: 56383991  \n",
      "Cell: 11948  | Step: 7    | Action: solar | Reward:  0.91819 | Energy Output: 1280503    | Unmet Demand: 55906996  \n",
      "Cell: 14036  | Step: 8    | Action: wind  | Reward:  0.03040 | Energy Output: 1313010    | Unmet Demand: 55874489  \n",
      "Cell: 5920   | Step: 9    | Action: solar | Reward:  1.09400 | Energy Output: 1864960    | Unmet Demand: 55322539  \n",
      "Cell: 14744  | Step: 10   | Action: wind  | Reward: -0.01856 | Energy Output: 1876351    | Unmet Demand: 55311148  \n",
      "Cell: 9923   | Step: 11   | Action: solar | Reward:  1.03293 | Energy Output: 2401606    | Unmet Demand: 54785893  \n",
      "Cell: 6727   | Step: 12   | Action: wind  | Reward:  0.05865 | Energy Output: 2443842    | Unmet Demand: 54743657  \n",
      "Cell: 2741   | Step: 13   | Action: wind  | Reward:  0.01959 | Energy Output: 2468165    | Unmet Demand: 54719334  \n",
      "Cell: 8817   | Step: 14   | Action: solar | Reward:  1.08557 | Energy Output: 3020115    | Unmet Demand: 54167384  \n",
      "Cell: 15649  | Step: 15   | Action: wind  | Reward: -0.00080 | Energy Output: 3035329    | Unmet Demand: 54152170  \n",
      "Cell: 10945  | Step: 16   | Action: wind  | Reward:  0.10051 | Energy Output: 3101002    | Unmet Demand: 54086497  \n",
      "Cell: 12760  | Step: 17   | Action: solar | Reward:  0.96539 | Energy Output: 3596617    | Unmet Demand: 53590882  \n",
      "Cell: 5140   | Step: 18   | Action: wind  | Reward:  0.00780 | Energy Output: 3625978    | Unmet Demand: 53561521  \n",
      "Cell: 9449   | Step: 19   | Action: wind  | Reward: -0.00608 | Energy Output: 3641328    | Unmet Demand: 53546171  \n",
      "Cell: 10853  | Step: 20   | Action: wind  | Reward:  0.02215 | Energy Output: 3666977    | Unmet Demand: 53520522  \n",
      "Cell: 11123  | Step: 21   | Action: wind  | Reward:  0.16782 | Energy Output: 3761961    | Unmet Demand: 53425538  \n",
      "Cell: 13749  | Step: 22   | Action: wind  | Reward: -0.00395 | Energy Output: 3775745    | Unmet Demand: 53411754  \n",
      "Cell: 10141  | Step: 23   | Action: solar | Reward:  0.90077 | Energy Output: 4243050    | Unmet Demand: 52944449  \n",
      "Cell: 8046   | Step: 24   | Action: solar | Reward:  0.92300 | Energy Output: 4718335    | Unmet Demand: 52469164  \n",
      "Cell: 9834   | Step: 25   | Action: wind  | Reward:  0.04269 | Energy Output: 4753476    | Unmet Demand: 52434023  \n",
      "Cell: 10828  | Step: 26   | Action: wind  | Reward:  0.07259 | Energy Output: 4808161    | Unmet Demand: 52379338  \n",
      "Cell: 17651  | Step: 27   | Action: wind  | Reward:  0.00327 | Energy Output: 4827351    | Unmet Demand: 52360148  \n",
      "Cell: 13737  | Step: 28   | Action: wind  | Reward:  0.00844 | Energy Output: 4847989    | Unmet Demand: 52339510  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m replay_buffer \u001b[38;5;241m=\u001b[39m ReplayBuffer(\u001b[38;5;241m1024\u001b[39m)\n\u001b[1;32m     27\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 42\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(agent, environment, episodes, epsilon_start, epsilon_end, epsilon_decay, replay_buffer, batch_size, device)\u001b[0m\n\u001b[1;32m     40\u001b[0m     batch \u001b[38;5;241m=\u001b[39m replay_buffer\u001b[38;5;241m.\u001b[39msample(batch_size)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Learn from the sampled experiences\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Update state\u001b[39;00m\n\u001b[1;32m     45\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m, in \u001b[0;36mDQNAgent.learn\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 33\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Projects/gtomscs/dl/GreenGridPR/slim_environment/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/gtomscs/dl/GreenGridPR/slim_environment/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state_gdf = gpd.read_parquet('../data/processed/state_1km_grid_cells_masked_power.parquet')\n",
    "\n",
    "state_gdf['distance_to_transmission_line'] /= 1000\n",
    "for i in range(1, 25):\n",
    "    state_gdf[f'wind_power_kW_hour_{i}'] /= 1000\n",
    "    state_gdf[f'solar_power_kW_hour_{i}'] /= 1000\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "environment = RenewableEnergyEnvironment(state_gdf)\n",
    "state_space = environment.state_tensor.to(device)\n",
    "action_space_size = environment.action_space_size\n",
    "\n",
    "#print('Demand in exectution:', environment.initial_demand)\n",
    "\n",
    "agent = DQNAgent(state_space, action_space_size, device, environment.initial_demand)\n",
    "\n",
    "episodes = 128\n",
    "\n",
    "epsilon_start = 1\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.95\n",
    "\n",
    "replay_buffer = ReplayBuffer(1024)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train(agent, environment, episodes, epsilon_start, epsilon_end, epsilon_decay, replay_buffer, batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28945f20-0f2b-4300-a1ba-0a720a6ae93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.action_space_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c466640f-81f4-41a9-8b0a-5e77d1b7f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.state_gdf.distance_to_transmission_line.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30109e2a-991d-458e-b6ff-9ab9e2677197",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(state_gdf[(state_gdf.masked == 0) & (state_gdf.slope <= 8.749)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4366bc3c-1eb8-41cc-b83f-86dadfdbd728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
