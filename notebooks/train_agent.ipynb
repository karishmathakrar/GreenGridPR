{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81222bf5-c8bd-4341-8569-71e7b8e0cd86",
   "metadata": {},
   "source": [
    "# Overview\n",
    "## State Space\n",
    "- The state space consists of an 80,000-cell grid, representing different geographical locations in Puerto Rico.\n",
    "- Each cell has attributes like solar PV output, wind power density, elevation, slope, cyclone risk score, building density, road density, and distance to transmission lines.\n",
    "- Approximately 70% of cells are unavailable for development due to environmental or other constraints.\n",
    "\n",
    "## Action Space\n",
    "- Two types of actions are available: building a solar array or a wind turbine.\n",
    "- Actions can be taken on any available cell.\n",
    "\n",
    "## Rewards and Costs\n",
    "The reward function should incorporate:\n",
    "- Energy production potential (solar and wind).\n",
    "- Costs or penalties associated with building on certain terrains (e.g., high elevation or steep slopes).\n",
    "- Penalties for building in high cyclone risk areas.\n",
    "- Incentives for maintaining a balance between solar and wind energy.\n",
    "- Incentives for early deployment and distributed grid development.\n",
    "- Penalties for high building or road density areas.\n",
    "- Distance to transmission lines.\n",
    "\n",
    "## RL Model\n",
    "- Model Choice: Given the size of the state space, a model-based RL algorithm (like Deep Q-Networks or Actor-Critic methods) is suitable.\n",
    "- Representation: The state representation should include the current status of each grid cell (whether it has a solar array, a wind turbine, or is vacant) along with its attributes.\n",
    "- Sequence of Actions: The RL agent will sequentially choose actions (where to build next) based on the current state of the grid.\n",
    "- Terminal State: The agent is done when the environment reaches a certain level of energy capacity or after a fixed number of steps.\n",
    "\n",
    "# Implementation Steps\n",
    "## Environment Setup: \n",
    "- Implement the environment to reflect the grid and its dynamics, including applying the binary mask for unavailable cells.\n",
    "- The step(action) method should update the grid state based on the chosen action and calculate the immediate reward or cost.\n",
    "\n",
    "## Agent Development:\n",
    "- Use PyTorch for implementing the neural network models for the agent.\n",
    "The agent needs to learn a policy that maximizes long-term rewards, considering the complex reward structure and large state space.\n",
    "\n",
    "## Training and Evaluation:\n",
    "- Set up a training loop where the agent interacts with the environment, receives feedback, and improves its policy.\n",
    "- Periodically evaluate the agent's performance, possibly using separate evaluation episodes or metrics like total energy capacity achieved or adherence to environmental constraints.\n",
    "\n",
    "## Hyperparameter Tuning:\n",
    "- Adjust learning rates, exploration rates, discount factors, and network architecture as needed to improve performance.\n",
    " \n",
    "## Scalability:\n",
    "Due to the large state space, may need to:\n",
    "- use function approximation for value functions\n",
    "- prioritizing important experiences in the replay buffer\n",
    "- parallelize computation process\n",
    "\n",
    "## Visualization and Analysis:\n",
    "- Develop tools to visualize the evolving grid layout and analyze the trade-offs made by the RL agent between different objectives (like energy maximization vs. environmental constraints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47a0776a-f41d-41ba-9261-9079553eae65",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c724dafa-ff4d-4fae-babe-283dcb25bb04",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac168609-d297-4963-b880-bcfdc86c2f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import random\n",
    "\n",
    "class RenewableEnergyEnvironment:\n",
    "    def __init__(self, grid_df):\n",
    "        self.grid_df = grid_df\n",
    "        self.state = None\n",
    "        self.total_energy_output = 0  # Example attribute\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment to the initial state\n",
    "        self.state = self.grid_df.copy()\n",
    "        self.total_energy_output = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply the action to the environment and return the result\n",
    "        # action: Tuple (cell_index, action_type) where action_type could be 'solar' or 'wind'\n",
    "        cell_index, action_type = action\n",
    "\n",
    "        # Check if the action is valid\n",
    "        if not self.is_valid_action(cell_index, action_type):\n",
    "            reward = -1  # Penalty for invalid action\n",
    "            done = self.is_terminal_state()\n",
    "            return self.state, reward, done, {}\n",
    "\n",
    "        # Apply the action\n",
    "        self.apply_action(cell_index, action_type)\n",
    "\n",
    "        # Calculate reward\n",
    "        reward = self.calculate_reward(cell_index, action_type)\n",
    "\n",
    "        # Update total energy output or other state attributes as needed\n",
    "        self.total_energy_output += self.calculate_energy_output(cell_index, action_type)\n",
    "\n",
    "        # Check if the state is terminal\n",
    "        done = self.is_terminal_state()\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def is_valid_action(self, cell_index, action_type):\n",
    "        # Implement logic to check if an action is valid\n",
    "        # Example: Check if the cell is not masked and not already occupied\n",
    "        cell = self.state.iloc[cell_index]\n",
    "        return not cell['masked'] and not cell['occupied']\n",
    "\n",
    "    def apply_action(self, cell_index, action_type):\n",
    "        # Implement the changes to the environment based on the action\n",
    "        # Example: Mark the cell as occupied and record the type of installation\n",
    "        self.state.at[cell_index, 'occupied'] = True\n",
    "        self.state.at[cell_index, 'installation_type'] = action_type\n",
    "\n",
    "    def calculate_reward(self, cell_index, action_type):\n",
    "        # Calculate the reward for the current action\n",
    "        # Example: Using building density as a factor in the reward\n",
    "        cell = self.state.iloc[cell_index]\n",
    "        reward = cell['building_density']  # Simplified example\n",
    "        return reward\n",
    "\n",
    "    def calculate_energy_output(self, cell_index, action_type):\n",
    "        # Calculate the energy output for the action\n",
    "        # Example: Different output for solar and wind\n",
    "        if action_type == 'solar':\n",
    "            return self.state.iloc[cell_index]['solar_output']\n",
    "        elif action_type == 'wind':\n",
    "            return self.state.iloc[cell_index]['wind_output']\n",
    "        return 0\n",
    "\n",
    "    def is_terminal_state(self):\n",
    "        # Define the terminal condition\n",
    "        # Example: Terminal state when a certain total energy output is reached\n",
    "        required_energy_output = 10000  # Example value\n",
    "        return self.total_energy_output >= required_energy_output\n",
    "\n",
    "    def render(self):\n",
    "        # Optional: Implement a method to visualize the current state of the environment\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cb54eb-f64b-4bd0-b00f-1dbe3d5c219a",
   "metadata": {},
   "source": [
    "# Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f382086d-cee9-46f6-8f48-4d6a81c8a39a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDQN\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_shape, num_actions):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(DQN, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        self.input_shape = input_shape  # Store input_shape for feature size calculation\n",
    "        self.fc1 = nn.Linear(self._feature_size(), 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def _feature_size(self):\n",
    "        with torch.no_grad():  # No need to track gradients here\n",
    "            return self.conv3(self.conv2(self.conv1(torch.zeros(1, *self.input_shape)))).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ab3b4f-b218-4ce4-bf5d-482d2eae1923",
   "metadata": {},
   "source": [
    "# DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "507095e3-fc00-4972-9f51-7a04b0b59813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_space, action_space):\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.model = DQN(state_space.shape, action_space.n)\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            # Choose the best action (exploitation)\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor([state], dtype=torch.float32)\n",
    "                q_values = self.model(state_tensor)\n",
    "                action = q_values.max(1)[1].item()  # Select the action with the highest Q-value\n",
    "        else:\n",
    "            # Choose a random action (exploration)\n",
    "            action = random.randrange(self.action_space.n)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, batch):\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Compute Q values\n",
    "        current_q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.model(next_states).max(1)[0]\n",
    "        expected_q_values = rewards + (1 - dones) * next_q_values.detach()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = torch.nn.functional.mse_loss(current_q_values, expected_q_values)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254993b-b1b0-4822-82c9-ca9ba3294557",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4036b194-8954-45df-b0aa-aacef4da5c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, environment, episodes, epsilon_start, epsilon_end, epsilon_decay, replay_buffer, batch_size):\n",
    "    epsilon = epsilon_start\n",
    "    for episode in range(episodes):\n",
    "        state = environment.reset()\n",
    "        done = False\n",
    "        total_reward = 0  # To keep track of total reward per episode\n",
    "\n",
    "        while not done:\n",
    "            action = agent.select_action(state, epsilon)\n",
    "            next_state, reward, done, _ = environment.step(action)\n",
    "\n",
    "            # Store experience in replay buffer\n",
    "            replay_buffer.store(state, action, reward, next_state, done)\n",
    "\n",
    "            # Check if buffer is ready for sampling\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                # Sample a batch from replay buffer\n",
    "                batch = replay_buffer.sample(batch_size)\n",
    "                # Learn from the sampled experiences\n",
    "                agent.learn(batch)\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_end, epsilon_decay * epsilon)  # Ensure epsilon doesn't go below the minimum\n",
    "\n",
    "        # Optional: Log training progress\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97292df-63f3-4f2d-8c91-831df7a79826",
   "metadata": {},
   "source": [
    "# Calculating Final Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5556a28c-5340-4e7c-beaf-cdaad6366122",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_transmission_distance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     21\u001b[0m weights \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransmission_line_distance\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwind_solar_balance\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Add other components here\u001b[39;00m\n\u001b[1;32m     30\u001b[0m }\n\u001b[1;32m     32\u001b[0m normalization_bounds \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransmission_line_distance\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0\u001b[39m, \u001b[43mmax_transmission_distance\u001b[49m),\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwind_solar_balance\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0\u001b[39m, max_balance_score),\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuilding_density\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0\u001b[39m, max_building_density),\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroad_density\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0\u001b[39m, max_road_density),\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcyclone_risk\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0\u001b[39m, max_cyclone_risk),\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearly_choice_reward\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0\u001b[39m, max_early_choice_reward),\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistributed_grid_reward\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0\u001b[39m, max_distributed_grid_reward),\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Define bounds for other components\u001b[39;00m\n\u001b[1;32m     41\u001b[0m }\n\u001b[1;32m     43\u001b[0m grid_gdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_reward\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m calculate_final_reward(grid_gdf, weights, normalization_bounds)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_transmission_distance' is not defined"
     ]
    }
   ],
   "source": [
    "def calculate_final_reward(grid_gdf, weights, normalization_bounds):\n",
    "    \"\"\"\n",
    "    Calculate the final reward by combining various costs and rewards.\n",
    "\n",
    "    :param grid_gdf: GeoDataFrame containing the grid cells with their respective costs and rewards.\n",
    "    :param weights: Dictionary containing the weights for each cost/reward component.\n",
    "    :param normalization_bounds: Dictionary containing min and max values for normalization of each component.\n",
    "    :return: Array of final reward values for each grid cell.\n",
    "    \"\"\"\n",
    "    final_reward = np.zeros(len(grid_gdf))\n",
    "\n",
    "    for component, weight in weights.items():\n",
    "        component_values = grid_gdf[component].to_numpy()\n",
    "        min_val, max_val = normalization_bounds.get(component, (None, None))\n",
    "        normalized_values = normalize(component_values, min_val, max_val)\n",
    "        final_reward += weight * normalized_values\n",
    "\n",
    "    return final_reward\n",
    "\n",
    "# Example usage\n",
    "weights = {\n",
    "    'transmission_line_distance': -1.0,\n",
    "    'wind_solar_balance': 1.0,\n",
    "    'building_density': -0.5,\n",
    "    'road_density': -0.3,\n",
    "    'cyclone_risk': -0.7,\n",
    "    'early_choice_reward': 1.0,\n",
    "    'distributed_grid_reward': 1.0,\n",
    "    # Add other components here\n",
    "}\n",
    "\n",
    "normalization_bounds = {\n",
    "    'transmission_line_distance': (0, max_transmission_distance),\n",
    "    'wind_solar_balance': (0, max_balance_score),\n",
    "    'building_density': (0, max_building_density),\n",
    "    'road_density': (0, max_road_density),\n",
    "    'cyclone_risk': (0, max_cyclone_risk),\n",
    "    'early_choice_reward': (0, max_early_choice_reward),\n",
    "    'distributed_grid_reward': (0, max_distributed_grid_reward),\n",
    "    # Define bounds for other components\n",
    "}\n",
    "\n",
    "grid_gdf['final_reward'] = calculate_final_reward(grid_gdf, weights, normalization_bounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98271b3e-7f86-4a1a-9a19-c3089e3af3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transmission_cost(state, cell_index, global_params):\n",
    "    # Logic to calculate cost based on the entire state\n",
    "    pass\n",
    "\n",
    "def calculate_wind_solar_balance(state, cell_index, global_params):\n",
    "    # Logic to calculate balance\n",
    "    pass\n",
    "\n",
    "# ... and so on for other functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e61a7e03-e99a-4ec2-8066-5aa455b99426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_costs_rewards(grid_gdf, global_params):\n",
    "    for idx, row in grid_gdf.iterrows():\n",
    "        # Calculate individual costs and rewards\n",
    "        transmission_cost = calculate_transmission_cost(grid_gdf, idx, global_params)\n",
    "        wind_solar_balance = calculate_wind_solar_balance(grid_gdf, idx, global_params)\n",
    "        building_density_cost = calculate_building_density_cost(grid_gdf, idx, global_params)\n",
    "        # ... other calculations\n",
    "\n",
    "        # Aggregate costs and rewards\n",
    "        total_cost_reward = (transmission_cost + wind_solar_balance +\n",
    "                             building_density_cost + ...)  # Add other components\n",
    "\n",
    "        # Store the result in the grid DataFrame\n",
    "        grid_gdf.at[idx, 'total_cost_reward'] = total_cost_reward\n",
    "\n",
    "    return grid_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0d4d4-8d78-4e6d-a0e8-d2c98a8ab453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
