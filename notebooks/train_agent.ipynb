{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81222bf5-c8bd-4341-8569-71e7b8e0cd86",
   "metadata": {},
   "source": [
    "# Overview\n",
    "## State Space\n",
    "- The state space consists of an 80,000-cell grid, representing different geographical locations in Puerto Rico.\n",
    "- Each cell has attributes like solar PV output, wind power density, elevation, slope, cyclone risk score, building density, road density, and distance to transmission lines.\n",
    "- Approximately 70% of cells are unavailable for development due to environmental or other constraints.\n",
    "\n",
    "## Action Space\n",
    "- Two types of actions are available: building a solar array or a wind turbine.\n",
    "- Actions can be taken on any available cell.\n",
    "\n",
    "## Rewards and Costs\n",
    "The reward function should incorporate:\n",
    "- Energy production potential (solar and wind).\n",
    "- Costs or penalties associated with building on certain terrains (e.g., high elevation or steep slopes).\n",
    "- Penalties for building in high cyclone risk areas.\n",
    "- Incentives for maintaining a balance between solar and wind energy.\n",
    "- Incentives for early deployment and distributed grid development.\n",
    "- Penalties for high building or road density areas.\n",
    "- Distance to transmission lines.\n",
    "\n",
    "## RL Model\n",
    "- Model Choice: Given the size of the state space, a model-based RL algorithm (like Deep Q-Networks or Actor-Critic methods) is suitable.\n",
    "- Representation: The state representation should include the current status of each grid cell (whether it has a solar array, a wind turbine, or is vacant) along with its attributes.\n",
    "- Sequence of Actions: The RL agent will sequentially choose actions (where to build next) based on the current state of the grid.\n",
    "- Terminal State: The agent is done when the environment reaches a certain level of energy capacity or after a fixed number of steps.\n",
    "\n",
    "# Implementation Steps\n",
    "## Environment Setup: \n",
    "- Implement the environment to reflect the grid and its dynamics, including applying the binary mask for unavailable cells.\n",
    "- The step(action) method should update the grid state based on the chosen action and calculate the immediate reward or cost.\n",
    "\n",
    "## Agent Development:\n",
    "- Use PyTorch for implementing the neural network models for the agent.\n",
    "The agent needs to learn a policy that maximizes long-term rewards, considering the complex reward structure and large state space.\n",
    "\n",
    "## Training and Evaluation:\n",
    "- Set up a training loop where the agent interacts with the environment, receives feedback, and improves its policy.\n",
    "- Periodically evaluate the agent's performance, possibly using separate evaluation episodes or metrics like total energy capacity achieved or adherence to environmental constraints.\n",
    "\n",
    "## Hyperparameter Tuning:\n",
    "- Adjust learning rates, exploration rates, discount factors, and network architecture as needed to improve performance.\n",
    " \n",
    "## Scalability:\n",
    "Due to the large state space, may need to:\n",
    "- use function approximation for value functions\n",
    "- prioritizing important experiences in the replay buffer\n",
    "- parallelize computation process\n",
    "\n",
    "## Visualization and Analysis:\n",
    "- Develop tools to visualize the evolving grid layout and analyze the trade-offs made by the RL agent between different objectives (like energy maximization vs. environmental constraints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a0776a-f41d-41ba-9261-9079553eae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "\n",
    "import random\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c724dafa-ff4d-4fae-babe-283dcb25bb04",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac168609-d297-4963-b880-bcfdc86c2f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenewableEnergyEnvironment:\n",
    "    def __init__(self, grid_gdf):\n",
    "        self.weights = {\n",
    "        'transmission_build_cost': -1.0,\n",
    "        'early_choice_reward': 1.0,\n",
    "        'distributed_grid_reward': 1.0,\n",
    "        }\n",
    "        \n",
    "        self.wind_daily_cost = 821.68\n",
    "        self.solar_daily_cost = 3007.26\n",
    "        \n",
    "        self.grid_columns = ['slope', \n",
    "                             'distance_to_transmission_line', \n",
    "                             'cyclone_risk',\n",
    "                             'water',\n",
    "                             'occupied_solar',\n",
    "                             'occupied_wind',\n",
    "                             'masked']\n",
    "        self.grid_columns += [f'demand_{i}' for i in range(1,25)]\n",
    "        self.grid_columns += [f'wind_power_kW_hour_{i}' for i in range(1,25)]\n",
    "        self.grid_columns += [f'solar_power_kW_hour_{i}' for i in range(1,25)]\n",
    "        \n",
    "        # Initialize the environment\n",
    "        self.starting_environment = grid_gdf\n",
    "        self.state_gdf = grid_gdf.copy()\n",
    "        self.state_tensor = self.gdf_to_tensor(self.state_gdf)\n",
    "        self.mapping, self.action_space_size = self.create_action_to_gdf_mapping()\n",
    "        \n",
    "        self.total_energy_output = 0\n",
    "        self.stored_energy = 0\n",
    "\n",
    "        demand_df = pd.read_csv('../data/generation_and_demand/demand_profile.csv')\n",
    "        self.demand = demand_df['demand_MW'].to_numpy() * 1000\n",
    "        self.unmet_demand = self.demand.sum()\n",
    "        self.total_demand = self.unmet_demand\n",
    "        \n",
    "        self.decay_rate = 0.1 #TODO determine good decay rate\n",
    "        self.max_distance = 1000 #TODO get max distance between two cells\n",
    "        \n",
    "    def reset(self):\n",
    "        # Reset the environment to the initial state\n",
    "        self.state_gdf = self.starting_environment.copy()\n",
    "        self.state_tensor = self.gdf_to_tensor(self.starting_environment)\n",
    "        self.total_energy_output = 0\n",
    "        return self.state_tensor\n",
    "\n",
    "    def gdf_to_tensor(self, gdf):\n",
    "        # Calculate grid dimensions\n",
    "        x_start = 100000\n",
    "        x_end = 300000\n",
    "        y_start = 200000\n",
    "        y_end = 300000\n",
    "        square_size = 500\n",
    "        \n",
    "        grid_width = int((x_end - x_start) / square_size)\n",
    "        grid_height = int((y_end - y_start) / square_size)\n",
    "        \n",
    "        # Flatten the grid data\n",
    "        flat_data = self.state_gdf[self.grid_columns].values.reshape(-1, grid_height, grid_width)\n",
    "    \n",
    "        # Create a 4D Tensor (batch size is the 4th dimension)\n",
    "        tensor = torch.tensor(flat_data, dtype=torch.float32)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def create_action_to_gdf_mapping(self):\n",
    "        unmasked_gdf = self.state_gdf[self.state_gdf['masked'] == 0]\n",
    "        \n",
    "        mapping = {}\n",
    "        action_idx = 0  # Initialize action index\n",
    "    \n",
    "        for _, row in unmasked_gdf.iterrows():\n",
    "            # Check for valid solar action\n",
    "            if row['slope'] <= 8.749:  # Slope check for solar\n",
    "                mapping[action_idx] = (row.name, 'solar')\n",
    "                action_idx += 1\n",
    "    \n",
    "            # Check for valid wind action\n",
    "            if row['slope'] <= 26.795:  # Slope check for wind\n",
    "                mapping[action_idx] = (row.name, 'wind')\n",
    "                action_idx += 1\n",
    "        \n",
    "        action_space_size = action_idx  # Total number of valid actions\n",
    "        return mapping, action_space_size\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Apply the action to the environment and return the result\n",
    "        # action: Tuple (cell_index, action_type) where action_type could be 'solar' or 'wind'\n",
    "        \n",
    "        cell_index, action_type = self.mapping[action]\n",
    "\n",
    "        # Check if the action is valid\n",
    "        if not self.is_valid_action(cell_index, action_type):\n",
    "            reward = -1  # Penalty for invalid action\n",
    "            done = self.is_terminal_state()\n",
    "            return self.state_tensor, reward, done, {}\n",
    "\n",
    "        # Calculate the total reward before applying the action\n",
    "        total_reward_before_action = self.calculate_reward()\n",
    "    \n",
    "        # Apply the action\n",
    "        self.apply_action(cell_index, action_type)\n",
    "    \n",
    "        # Calculate the total reward after applying the action\n",
    "        total_reward_after_action = self.calculate_reward()\n",
    "    \n",
    "        # The reward for the action is the difference in total reward\n",
    "        reward = total_reward_after_action - total_reward_before_action\n",
    "\n",
    "        # Update total energy output or other state attributes as needed\n",
    "        self.total_energy_output = self.calculate_energy_output()\n",
    "        \n",
    "        # Check if the state is terminal\n",
    "        done = self.is_terminal_state()\n",
    "\n",
    "        print(f'Cell: {cell_index:<6d} | Action: {action_type:<5} | Reward: {int(reward):<7} | Energy Output: {int(self.total_energy_output.sum()):<10} | Unmet Demand: {int(self.unmet_demand):<10}')\n",
    "\n",
    "        # Update the state tensor with the new state gdf\n",
    "        self.state_tensor = self.gdf_to_tensor(self.state_gdf)\n",
    "        \n",
    "        return self.state_tensor, reward, done, {}\n",
    "\n",
    "    def is_valid_action(self, cell_index, action_type):\n",
    "        # Implement logic to check if an action is valid\n",
    "        # Example: Check if the cell is not masked and not already occupied\n",
    "        cell = self.state_gdf.iloc[cell_index]\n",
    "        if cell['masked']: # This should never occur\n",
    "            print('Something has gone wrong. Attempting to build on a masked cell')\n",
    "            return False\n",
    "        elif cell['occupied_solar']:\n",
    "            return False\n",
    "        elif cell['occupied_wind']:\n",
    "            return False\n",
    "        elif action_type == 'solar' and cell['slope'] > 8.749:\n",
    "            return False\n",
    "        elif action_type == 'wind' and cell['slope'] > 26.795:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def apply_action(self, cell_index, action_type):\n",
    "        # Implement the changes to the environment based on the action\n",
    "        # Example: Mark the cell as occupied and record the type of installation\n",
    "        if action_type == 'solar':\n",
    "            self.state_gdf.at[cell_index, 'occupied_solar'] = 1\n",
    "        elif action_type == 'wind':\n",
    "            self.state_gdf.at[cell_index, 'occupied_wind'] = 1\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        # Solar installation cost\n",
    "        solar_cost = self.calculate_solar_cost()\n",
    "\n",
    "        # Wind turbine installation cost\n",
    "        wind_cost = self.calculate_wind_cost()\n",
    "        \n",
    "        # Solar power Reward\n",
    "        power_output_reward = self.calculate_power_output_reward()\n",
    "\n",
    "        # Transmission build cost\n",
    "        transmission_build_cost = self.calculate_transmission_build_cost()\n",
    "        \n",
    "        # Cyclone risk cost\n",
    "        cyclone_risk_cost = self.calculate_cyclone_risk_cost()\n",
    "    \n",
    "        # Distribution reward\n",
    "        # distributed_grid_reward = self.calculate_distributed_grid_reward()\n",
    "    \n",
    "        # Early choice reward\n",
    "        # early_choice_reward = self.time_dependent_reward_factor()\n",
    "\n",
    "        total_reward = power_output_reward + solar_cost + wind_cost + cyclone_risk_cost + transmission_build_cost\n",
    "    \n",
    "        return total_reward\n",
    "\n",
    "    def calculate_energy_output(self):\n",
    "        # Filter for solar and wind installations\n",
    "        solar_gdf = self.state_gdf[self.state_gdf['occupied_solar'] == 1]\n",
    "        wind_gdf = self.state_gdf[self.state_gdf['occupied_wind'] == 1]\n",
    "    \n",
    "        # Prepare column names for solar and wind power\n",
    "        solar_power_columns = [f'solar_power_kW_hour_{i}' for i in range(1, 25)]\n",
    "        wind_power_columns = [f'wind_power_kW_hour_{i}' for i in range(1, 25)]\n",
    "    \n",
    "        # Vectorized sum of power output for solar and wind for each hour\n",
    "        total_solar_power = solar_gdf[solar_power_columns].sum().to_numpy()\n",
    "        total_wind_power = wind_gdf[wind_power_columns].sum().to_numpy()\n",
    "    \n",
    "        total_power = total_solar_power + total_wind_power\n",
    "        \n",
    "        return total_power\n",
    "\n",
    "    def calculate_power_output_reward(self):\n",
    "        \"\"\"Uses supply and demand curve to determine the amount of demand satisfied by\n",
    "        the solar and wind installations\"\"\"\n",
    "        cost_kWh = .22  # TODO\n",
    "        # Storage cost = $400/kWh over the 15 year lifetime of a 4 hour battery\n",
    "        # cost_storage_kWh = 400 / (365.25 * 15)\n",
    "        demand = self.demand\n",
    "\n",
    "        # Calculate the reward using vectorized minimum\n",
    "        total_power = self.calculate_energy_output()\n",
    "        demand_satisfied = np.minimum(demand, total_power)\n",
    "        demand_satisfied_ratio = demand_satisfied / demand\n",
    "\n",
    "        self.update_demand(demand_satisfied_ratio)\n",
    "        \n",
    "        power_reward = demand_satisfied.sum() * cost_kWh\n",
    "        # self.stored_energy = np.maximum(total_power - demand, 0).sum()\n",
    "        # storage_cost = self.stored_energy * cost_storage_kWh\n",
    "\n",
    "        reward = power_reward # - storage_cost\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def update_demand(self, demand_satisfied_ratio):\n",
    "        unsatisfied_demand_ratio = 1 - demand_satisfied_ratio\n",
    "        for i in range(1, 25):\n",
    "            self.state_gdf[f'demand_{i}'] *= unsatisfied_demand_ratio[i-1]\n",
    "    \n",
    "    def calculate_solar_cost(self):\n",
    "        \"\"\"Cost of installing solar array on cell. Based on elevation and slope\"\"\"\n",
    "        return -self.solar_daily_cost * len(self.state_gdf[self.state_gdf['occupied_solar'] == 1])\n",
    "\n",
    "    def calculate_wind_cost(self):\n",
    "        \"\"\"Cost of installing wind turbine on cell. Based on elevation and slope\"\"\"\n",
    "        return -self.wind_daily_cost * len(self.state_gdf[self.state_gdf['occupied_wind'] == 1])\n",
    "\n",
    "    def transmission_line_cost_per_km(self, distance):\n",
    "        # $2.29 million per mile divided by 1.60934 to get into km, \n",
    "        # then divided by 25 * 365.25 for daily costs with 25 year decommission time\n",
    "        distance /= 1000 # Convert to km\n",
    "        KM_PER_MILE = 1.60934\n",
    "        COST_PER_KM = 2.29 * 1000000 / (KM_PER_MILE * 25 * 365.25) \n",
    "        SHORT_DISTANCE_THRESHOLD = 3 * KM_PER_MILE # Threshold for short distance \n",
    "        MEDIUM_DISTANCE_THRESHOLD = 10 * KM_PER_MILE # Threshold for medium distance \n",
    "        \n",
    "        if distance < SHORT_DISTANCE_THRESHOLD:\n",
    "            cost_modifier = 1.5  # 50% increase for less than 3 miles\n",
    "        elif distance < MEDIUM_DISTANCE_THRESHOLD:\n",
    "            cost_modifier = 1.2  # 20% increase for 3-10 miles\n",
    "        else:\n",
    "            cost_modifier = 1  # No modification for more than 10 miles\n",
    "        return -distance * COST_PER_KM * cost_modifier\n",
    "    \n",
    "    def calculate_transmission_build_cost(self):\n",
    "        occupied_cells = self.state_gdf[(self.state_gdf['occupied_solar'] == 1) | (self.state_gdf['occupied_wind'] == 1)]\n",
    "        \n",
    "        # Check if there is only one occupied cell\n",
    "        if len(occupied_cells) == 0:\n",
    "            return 0\n",
    "        elif len(occupied_cells) == 1:\n",
    "            # For a single occupied cell, use the distance to transmission line for cost calculation\n",
    "            occupied_cell = occupied_cells.iloc[0]\n",
    "            distance_km = occupied_cell['distance_to_transmission_line']\n",
    "            build_cost = self.transmission_line_cost_per_km(distance_km)\n",
    "        else:\n",
    "            # Get coordinates of occupied cells\n",
    "            coords = np.array(list(zip(occupied_cells.geometry.centroid.x, occupied_cells.geometry.centroid.y)))\n",
    "    \n",
    "            # Calculate pairwise distances between occupied cells\n",
    "            distances = cdist(coords, coords)\n",
    "    \n",
    "            # Replace zeros in distance matrix with np.inf to avoid zero distance to itself\n",
    "            np.fill_diagonal(distances, np.inf)\n",
    "    \n",
    "            # Find the nearest installation for each installation\n",
    "            nearest_installation_distances = np.min(distances, axis=1)\n",
    "    \n",
    "            # Determine the relevant distance for cost calculation\n",
    "            relevant_distances = np.minimum(nearest_installation_distances, occupied_cells['distance_to_transmission_line'].to_numpy())\n",
    "    \n",
    "            # Calculate build cost\n",
    "            build_costs = [self.transmission_line_cost_per_km(distance) for distance in relevant_distances]\n",
    "            build_cost = sum(build_costs)\n",
    "            \n",
    "        return build_cost\n",
    "\n",
    "    def calculate_cyclone_risk_cost(self):\n",
    "        if len(self.state_gdf[self.state_gdf.occupied_wind == 1]) == 0:\n",
    "            return 0\n",
    "        # Wind operational expenses = $40/kW/yr\n",
    "        # Wind turbine capacity is 3 MW\n",
    "        # To get daily cost, multiply by 3000 (3 MW = 3000 kW) and divide by 365.25 days in a year\n",
    "        wind_opex = 40 * 3000 / 365.25\n",
    "        # Wind capital expenses = $1501/kW\n",
    "        # To get daily cost, multiply by 3000 (3 MW = 3000 kW) and divide by (25 years of lifetime * 365.25 days in a year)\n",
    "        wind_capex = 1501 * 3000 / (25 * 365.25)\n",
    "        cyclone_risk_cost = -(self.state_gdf[self.state_gdf.occupied_wind == 1]['cyclone_risk'] * (wind_capex - wind_opex)).sum()\n",
    "        return cyclone_risk_cost\n",
    "    \n",
    "    def calculate_distributed_grid_reward(self):\n",
    "        # Extract the coordinates of the occupied cells (where installations are located)\n",
    "        occupied_cells = self.state_gdf[(self.state_gdf['occupied_solar'] == 1) | (self.state_gdf['occupied_wind'] == 1)]\n",
    "        if len(occupied_cells) < 2:\n",
    "            # If there are less than two installations, we cannot calculate distances\n",
    "            return 1\n",
    "    \n",
    "        coords = np.array(list(zip(occupied_cells.geometry.x, occupied_cells.geometry.y)))\n",
    "    \n",
    "        # Calculate pairwise distances between all occupied cells\n",
    "        distances = pdist(coords)\n",
    "    \n",
    "        # Calculate the average distance. The larger this is, the more distributed the installations are.\n",
    "        avg_distance = np.mean(distances)\n",
    "    \n",
    "        # Normalize the reward such that it ranges between 0 and 1\n",
    "        normalized_reward = avg_distance / self.max_distance\n",
    "    \n",
    "        return normalized_reward\n",
    "    \n",
    "    def time_dependent_reward_factor(self):\n",
    "        # A function to calculate the time-dependent reward factor\n",
    "        # It decreases with each year from the base year\n",
    "        action_number = self.state_gdf.occupied_solar.sum() + self.state_gdf.occupied_wind.sum()\n",
    "        return 1 / (1 + self.decay_rate * action_number)\n",
    "        \n",
    "    def is_terminal_state(self):\n",
    "        # The episode ends when there is no unmet demand\n",
    "        self.unmet_demand = np.maximum(self.demand - self.total_energy_output, 0).sum()\n",
    "        # self.unmet_demand -= self.stored_energy\n",
    "        return self.unmet_demand <= 0 or self.total_energy_output.sum() >= 2 * self.total_demand \n",
    "\n",
    "    def render(self):\n",
    "        # Optional: Implement a method to visualize the current state of the environment\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cb54eb-f64b-4bd0-b00f-1dbe3d5c219a",
   "metadata": {},
   "source": [
    "# Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f382086d-cee9-46f6-8f48-4d6a81c8a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 128, kernel_size=3, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.input_shape = input_shape  # Store input_shape for feature size calculation\n",
    "        print(f'Input shape: {input_shape[0]}, Feature size: {self._feature_size()}, n Actions: {num_actions}')\n",
    "        self.fc1 = nn.Linear(self._feature_size(), 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_actions)\n",
    "\n",
    "    def _feature_size(self):\n",
    "        with torch.no_grad():\n",
    "            return self.bn3(self.conv3(self.bn2(self.conv2(self.bn1(self.conv1(torch.zeros(1, *self.input_shape))))))).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ab3b4f-b218-4ce4-bf5d-482d2eae1923",
   "metadata": {},
   "source": [
    "# DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "507095e3-fc00-4972-9f51-7a04b0b59813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_space, action_space_size):\n",
    "        self.action_space_size = action_space_size\n",
    "        self.model = DQN(state_space.shape, action_space_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            # Choose the best action (exploitation)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.model(state)\n",
    "                action = q_values.max(1)[1].item()  # Select the action with the highest Q-value\n",
    "        else:\n",
    "            # Choose a random action (exploration)\n",
    "            action = random.randrange(self.action_space_size)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, batch):\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        \n",
    "        # Compute Q values\n",
    "        current_q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.model(next_states).max(1)[0]\n",
    "        expected_q_values = rewards + (1 - dones) * next_q_values.detach()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = torch.nn.functional.mse_loss(current_q_values, expected_q_values)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78facf33-0887-46bb-ba62-89ca956e553d",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ee56fc-3afc-4389-b78f-4bbe022f18ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "\n",
    "        # Stack the tuples of tensors to create a single tensor for each component\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float)\n",
    "        next_states = torch.stack(next_states)\n",
    "        dones = torch.tensor(dones, dtype=torch.float)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254993b-b1b0-4822-82c9-ca9ba3294557",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4036b194-8954-45df-b0aa-aacef4da5c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, environment, episodes, epsilon_start, epsilon_end, epsilon_decay, replay_buffer, batch_size):\n",
    "    epsilon = epsilon_start\n",
    "    for episode in range(episodes):\n",
    "        state = environment.reset()\n",
    "        done = False\n",
    "        total_reward = 0  # To keep track of total reward per episode\n",
    "\n",
    "        while not done:\n",
    "            action = agent.select_action(state.unsqueeze(0), epsilon)\n",
    "            next_state, reward, done, _ = environment.step(action)\n",
    "\n",
    "            # Store experience in replay buffer\n",
    "            replay_buffer.store(state, action, reward, next_state, torch.tensor(bool(done)))\n",
    "\n",
    "            # Check if buffer is ready for sampling\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                # Sample a batch from replay buffer\n",
    "                batch = replay_buffer.sample(batch_size)\n",
    "                # Learn from the sampled experiences\n",
    "                agent.learn(batch)\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_end, epsilon_decay * epsilon)  # Ensure epsilon doesn't go below the minimum\n",
    "\n",
    "        # Optional: Log training progress\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d0b32-09a3-4aef-bcaf-51f0bc870bcf",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39715d08-6208-4791-9064-e5de7a05abed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: 79, Feature size: 132352, n Actions: 29515\n",
      "Cell: 55508  | Action: wind  | Reward: 1119    | Energy Output: 10044      | Unmet Demand: 57177455  \n",
      "Cell: 54282  | Action: wind  | Reward: 344     | Energy Output: 16732      | Unmet Demand: 57170767  \n",
      "Cell: 48832  | Action: solar | Reward: 26851   | Energy Output: 152986     | Unmet Demand: 57034513  \n",
      "Cell: 62485  | Action: solar | Reward: 24782   | Energy Output: 279835     | Unmet Demand: 56907664  \n",
      "Cell: 13111  | Action: wind  | Reward: 461     | Energy Output: 289660     | Unmet Demand: 56897839  \n",
      "Cell: 8500   | Action: solar | Reward: 24324   | Energy Output: 416271     | Unmet Demand: 56771228  \n",
      "Cell: 14517  | Action: solar | Reward: 23571   | Energy Output: 540840     | Unmet Demand: 56646659  \n",
      "Cell: 58261  | Action: wind  | Reward: -822    | Energy Output: 545787     | Unmet Demand: 56641712  \n",
      "Cell: 51032  | Action: solar | Reward: 27309   | Energy Output: 684653     | Unmet Demand: 56502846  \n",
      "Cell: 16706  | Action: wind  | Reward: -19     | Energy Output: 691113     | Unmet Demand: 56496386  \n",
      "Cell: 15649  | Action: wind  | Reward: 2974    | Energy Output: 708491     | Unmet Demand: 56479008  \n",
      "Cell: 36502  | Action: solar | Reward: 22940   | Energy Output: 826433     | Unmet Demand: 56361066  \n",
      "Cell: 30901  | Action: wind  | Reward: 60      | Energy Output: 830601     | Unmet Demand: 56356898  \n",
      "Cell: 32884  | Action: wind  | Reward: 1134    | Energy Output: 839653     | Unmet Demand: 56347846  \n",
      "Cell: 30337  | Action: wind  | Reward: 1670    | Energy Output: 853055     | Unmet Demand: 56334444  \n",
      "Cell: 14854  | Action: wind  | Reward: 1351    | Energy Output: 863583     | Unmet Demand: 56323916  \n",
      "Cell: 15101  | Action: wind  | Reward: 229     | Energy Output: 869982     | Unmet Demand: 56317517  \n",
      "Cell: 64858  | Action: wind  | Reward: 2113    | Energy Output: 884276     | Unmet Demand: 56303223  \n",
      "Cell: 60850  | Action: solar | Reward: 23098   | Energy Output: 1004855    | Unmet Demand: 56182644  \n",
      "Cell: 51851  | Action: solar | Reward: 24136   | Energy Output: 1129423    | Unmet Demand: 56058076  \n",
      "Cell: 22866  | Action: wind  | Reward: 3655    | Energy Output: 1155473    | Unmet Demand: 56032026  \n",
      "Cell: 43071  | Action: wind  | Reward: 2019    | Energy Output: 1170268    | Unmet Demand: 56017231  \n",
      "Cell: 13258  | Action: wind  | Reward: 189     | Energy Output: 1178080    | Unmet Demand: 56009419  \n",
      "Cell: 25055  | Action: wind  | Reward: 1158    | Energy Output: 1190100    | Unmet Demand: 55997399  \n",
      "Cell: 28462  | Action: wind  | Reward: 2641    | Energy Output: 1208709    | Unmet Demand: 55978790  \n",
      "Cell: 38444  | Action: wind  | Reward: 1974    | Energy Output: 1222111    | Unmet Demand: 55965388  \n",
      "Cell: 27892  | Action: solar | Reward: 23368   | Energy Output: 1342001    | Unmet Demand: 55845498  \n",
      "Cell: 66492  | Action: wind  | Reward: 259     | Energy Output: 1347608    | Unmet Demand: 55839891  \n",
      "Cell: 19284  | Action: wind  | Reward: -1469   | Energy Output: 1354746    | Unmet Demand: 55832753  \n",
      "Cell: 9471   | Action: wind  | Reward: 32      | Energy Output: 1361844    | Unmet Demand: 55825655  \n",
      "Cell: 14491  | Action: wind  | Reward: -290    | Energy Output: 1368817    | Unmet Demand: 55818682  \n",
      "Cell: 24280  | Action: wind  | Reward: 475     | Energy Output: 1380821    | Unmet Demand: 55806678  \n",
      "Cell: 60256  | Action: wind  | Reward: 787     | Energy Output: 1388337    | Unmet Demand: 55799162  \n",
      "Cell: 41438  | Action: wind  | Reward: 2124    | Energy Output: 1402950    | Unmet Demand: 55784549  \n",
      "Cell: 33082  | Action: wind  | Reward: 2403    | Energy Output: 1417771    | Unmet Demand: 55769728  \n",
      "Cell: 29933  | Action: solar | Reward: 24894   | Energy Output: 1544596    | Unmet Demand: 55642903  \n",
      "Cell: 52925  | Action: wind  | Reward: 460     | Energy Output: 1551113    | Unmet Demand: 55636386  \n",
      "Cell: 48437  | Action: wind  | Reward: 3263    | Energy Output: 1569880    | Unmet Demand: 55617619  \n",
      "Cell: 60501  | Action: wind  | Reward: 118     | Energy Output: 1576439    | Unmet Demand: 55611060  \n",
      "Cell: 11092  | Action: solar | Reward: 24120   | Energy Output: 1699749    | Unmet Demand: 55487750  \n",
      "Cell: 27090  | Action: wind  | Reward: -586    | Energy Output: 1702038    | Unmet Demand: 55485461  \n",
      "Cell: 13865  | Action: wind  | Reward: 317     | Energy Output: 1707869    | Unmet Demand: 55479630  \n",
      "Cell: 27099  | Action: solar | Reward: 23565   | Energy Output: 1830157    | Unmet Demand: 55357342  \n",
      "Cell: 63723  | Action: solar | Reward: 24706   | Energy Output: 1959999    | Unmet Demand: 55227500  \n",
      "Cell: 55086  | Action: wind  | Reward: -668    | Energy Output: 1962043    | Unmet Demand: 55225456  \n",
      "Cell: 66066  | Action: solar | Reward: 25819   | Energy Output: 2093072    | Unmet Demand: 55094427  \n",
      "Cell: 44693  | Action: wind  | Reward: 1416    | Energy Output: 2108956    | Unmet Demand: 55078543  \n",
      "Cell: 28838  | Action: wind  | Reward: 1431    | Energy Output: 2119890    | Unmet Demand: 55067609  \n",
      "Cell: 22673  | Action: wind  | Reward: 4176    | Energy Output: 2141994    | Unmet Demand: 55045505  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_gdf = gpd.read_parquet('../data/processed/state.parquet')\n",
    "state_gdf['installation_type'] = None\n",
    "\n",
    "environment = RenewableEnergyEnvironment(state_gdf)\n",
    "state_space = environment.state_tensor\n",
    "action_space_size = environment.action_space_size\n",
    "\n",
    "agent = DQNAgent(state_space, action_space_size)\n",
    "\n",
    "episodes = 16\n",
    "\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.5\n",
    "\n",
    "replay_buffer = ReplayBuffer(64)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train(agent, environment, episodes, epsilon_start, epsilon_end, epsilon_decay, replay_buffer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4cd1d-e7a7-4dc0-afba-26e344cc17d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
