{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "81222bf5-c8bd-4341-8569-71e7b8e0cd86",
      "metadata": {
        "id": "81222bf5-c8bd-4341-8569-71e7b8e0cd86"
      },
      "source": [
        "# Overview\n",
        "## State Space\n",
        "- The state space consists of an 80,000-cell grid, representing different geographical locations in Puerto Rico.\n",
        "- Each cell has attributes like solar PV output, wind power density, elevation, slope, cyclone risk score, building density, road density, and distance to transmission lines.\n",
        "- Approximately 70% of cells are unavailable for development due to environmental or other constraints.\n",
        "\n",
        "## Action Space\n",
        "- Two types of actions are available: building a solar array or a wind turbine.\n",
        "- Actions can be taken on any available cell.\n",
        "\n",
        "## Rewards and Costs\n",
        "The reward function should incorporate:\n",
        "- Energy production potential (solar and wind).\n",
        "- Costs or penalties associated with building on certain terrains (e.g., high elevation or steep slopes).\n",
        "- Penalties for building in high cyclone risk areas.\n",
        "- Incentives for maintaining a balance between solar and wind energy.\n",
        "- Incentives for early deployment and distributed grid development.\n",
        "- Penalties for high building or road density areas.\n",
        "- Distance to transmission lines.\n",
        "\n",
        "## RL Model\n",
        "- Model Choice: Given the size of the state space, a model-based RL algorithm (like Deep Q-Networks or Actor-Critic methods) is suitable.\n",
        "- Representation: The state representation should include the current status of each grid cell (whether it has a solar array, a wind turbine, or is vacant) along with its attributes.\n",
        "- Sequence of Actions: The RL agent will sequentially choose actions (where to build next) based on the current state of the grid.\n",
        "- Terminal State: The agent is done when the environment reaches a certain level of energy capacity or after a fixed number of steps.\n",
        "\n",
        "# Implementation Steps\n",
        "## Environment Setup:\n",
        "- Implement the environment to reflect the grid and its dynamics, including applying the binary mask for unavailable cells.\n",
        "- The step(action) method should update the grid state based on the chosen action and calculate the immediate reward or cost.\n",
        "\n",
        "## Agent Development:\n",
        "- Use PyTorch for implementing the neural network models for the agent.\n",
        "The agent needs to learn a policy that maximizes long-term rewards, considering the complex reward structure and large state space.\n",
        "\n",
        "## Training and Evaluation:\n",
        "- Set up a training loop where the agent interacts with the environment, receives feedback, and improves its policy.\n",
        "- Periodically evaluate the agent's performance, possibly using separate evaluation episodes or metrics like total energy capacity achieved or adherence to environmental constraints.\n",
        "\n",
        "## Hyperparameter Tuning:\n",
        "- Adjust learning rates, exploration rates, discount factors, and network architecture as needed to improve performance.\n",
        "\n",
        "## Scalability:\n",
        "Due to the large state space, may need to:\n",
        "- use function approximation for value functions\n",
        "- prioritizing important experiences in the replay buffer\n",
        "- parallelize computation process\n",
        "\n",
        "## Visualization and Analysis:\n",
        "- Develop tools to visualize the evolving grid layout and analyze the trade-offs made by the RL agent between different objectives (like energy maximization vs. environmental constraints)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "mrJv5LZDIu8s",
      "metadata": {
        "id": "mrJv5LZDIu8s"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import torch.optim as optim\n",
        "\n",
        "from scipy.spatial.distance import cdist, pdist, squareform\n",
        "\n",
        "import random\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "import copy\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "uPsaoSerH3hW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPsaoSerH3hW",
        "outputId": "968646bc-0efe-482a-abf9-bff0a7fd4683"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c724dafa-ff4d-4fae-babe-283dcb25bb04",
      "metadata": {
        "id": "c724dafa-ff4d-4fae-babe-283dcb25bb04"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "56ec6ed0",
      "metadata": {},
      "outputs": [],
      "source": [
        "class RenewableEnergyEnvironment:\n",
        "    def __init__(self, grid_gdf):\n",
        "        self.step_count = 0\n",
        "\n",
        "        # Daily cost calculated in data_preprocessing notebook\n",
        "        # Multiplying by 4 because grid cell size was changed from 500m to 1 km\n",
        "        self.wind_daily_cost = 4 * 821.68\n",
        "        self.solar_daily_cost = 4 * 3007.26\n",
        "        \n",
        "        self.grid_columns = ['distance_to_transmission_line', \n",
        "                             'cyclone_risk',\n",
        "                             'occupied']\n",
        "        self.grid_columns += [f'demand_{i}' for i in range(1,25)]\n",
        "        self.grid_columns += [f'wind_power_kW_hour_{i}' for i in range(1,25)]\n",
        "        self.grid_columns += [f'solar_power_kW_hour_{i}' for i in range(1,25)]\n",
        "        \n",
        "        self.starting_environment = grid_gdf.copy()\n",
        "        # Initialize 'occupied' column to zero\n",
        "        self.starting_environment['occupied'] = 0\n",
        "        self.starting_environment['installation_type'] = None\n",
        "        self.state_gdf = self.starting_environment.copy()\n",
        "        self.state_tensor = self.gdf_to_tensor(self.state_gdf)\n",
        "        self.mapping, self.action_space_size = self.create_action_to_gdf_mapping()\n",
        "\n",
        "        self.total_energy_output = 0\n",
        "        self.stored_energy = 0\n",
        "\n",
        "        demand_df = pd.read_csv('../data/generation_and_demand/demand_profile.csv')\n",
        "        self.demand = demand_df['demand_MW'].to_numpy() * 1000\n",
        "        self.unmet_demand = self.demand.sum()\n",
        "        self.total_demand = self.unmet_demand\n",
        "\n",
        "        # LEGACY - the following parameters are no longer in use\n",
        "        self.decay_rate = 0.1 #TODO determine good decay rate\n",
        "        self.max_distance = 1000 #TODO get max distance between two cells\n",
        "        self.weights = {\n",
        "        'transmission_build_cost': -1.0,\n",
        "        'early_choice_reward': 1.0,\n",
        "        'distributed_grid_reward': 1.0,\n",
        "        }\n",
        "        \n",
        "    def reset(self):\n",
        "        # Reset the environment to the initial state\n",
        "        self.state_gdf = self.starting_environment.copy()\n",
        "        self.state_tensor = self.gdf_to_tensor(self.starting_environment)\n",
        "        self.total_energy_output = 0\n",
        "        self.step_count = 0\n",
        "        return self.state_tensor\n",
        "\n",
        "    def gdf_to_tensor(self, gdf):\n",
        "        # Calculate grid dimensions\n",
        "        x_start = 100000\n",
        "        x_end = 300000\n",
        "        y_start = 200000\n",
        "        y_end = 300000\n",
        "        square_size = 1000\n",
        "        \n",
        "        grid_width = int((x_end - x_start) / square_size)\n",
        "        grid_height = int((y_end - y_start) / square_size)\n",
        "\n",
        "        transposed_data = self.state_gdf[self.grid_columns].values.T\n",
        "        grid_data = np.array([feature.reshape(100, 200) for feature in transposed_data])\n",
        "        tensor = torch.tensor(grid_data, dtype=torch.float16)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def create_action_to_gdf_mapping(self):\n",
        "        unmasked_gdf = self.state_gdf[self.state_gdf['masked'] == 0]\n",
        "        \n",
        "        mapping = {}\n",
        "        action_idx = 0  # Initialize action index\n",
        "    \n",
        "        for _, row in unmasked_gdf.iterrows():\n",
        "            # Check for valid solar action\n",
        "            if row['slope'] <= 8.749:  # Slope check for solar\n",
        "                mapping[action_idx] = (row.name, 'solar')\n",
        "                action_idx += 1\n",
        "    \n",
        "            # Check for valid wind action\n",
        "            if row['slope'] <= 26.795:  # Slope check for wind\n",
        "                mapping[action_idx] = (row.name, 'wind')\n",
        "                action_idx += 1\n",
        "        \n",
        "        action_space_size = action_idx  # Total number of valid actions\n",
        "        return mapping, action_space_size\n",
        "\n",
        "    def output_stats(self, writer, cell_index, action_type, reward, invalid=False):\n",
        "        if writer:\n",
        "            writer.writerow([None,None,None,cell_index, action_type, int(reward), int(self.total_energy_output.sum()), int(self.unmet_demand)])\n",
        "        if invalid:\n",
        "            print(f'Cell: {cell_index:<6d} | Step: INV  | Action: {action_type:<5} | Reward: -9999999 | Energy Output: {int(self.total_energy_output.sum()):<10} | Unmet Demand: {int(self.unmet_demand):<10}')\n",
        "        else:\n",
        "            print(f'Cell: {cell_index:<6d} | Step: {self.step_count:<4d} | Action: {action_type:<5} | Reward: {reward:8.5f} | Energy Output: {int(self.total_energy_output.sum()):<10} | Unmet Demand: {int(self.unmet_demand):<10}')\n",
        "\n",
        "    def step(self, action, writer=None):\n",
        "        # Apply the action to the environment and return the result\n",
        "        # action: Tuple (cell_index, action_type) where action_type could be 'solar' or 'wind'\n",
        "\n",
        "        # Map action from agent output to action in terms of state_gdf\n",
        "        cell_index, action_type = self.mapping[action]\n",
        "\n",
        "        # Check if the action is valid\n",
        "        if not self.is_valid_action(cell_index, action_type):\n",
        "            reward = -9999999 # Penalty for invalid action\n",
        "            done = self.is_terminal_state() # Check if in terminal state\n",
        "            self.total_energy_output = self.calculate_energy_output() # Calculate total energy output\n",
        "            self.output_stats(writer, cell_index, action_type, reward, invalid=True) # Print results and write results to file\n",
        "            return self.state_tensor, reward, done, {}\n",
        "\n",
        "        # Calculate the total reward before applying the action\n",
        "        total_reward_before_action = self.calculate_reward()\n",
        "    \n",
        "        # Apply the action\n",
        "        self.apply_action(cell_index, action_type)\n",
        "    \n",
        "        # Calculate the total reward after applying the action\n",
        "        total_reward_after_action = self.calculate_reward()\n",
        "    \n",
        "        # The reward for the action is the difference in total reward\n",
        "        reward = total_reward_after_action - total_reward_before_action\n",
        "        reward = reward / 100000\n",
        "\n",
        "        # Update total energy output or other state attributes as needed\n",
        "        self.total_energy_output = self.calculate_energy_output()\n",
        "        \n",
        "        # Check if the state is terminal\n",
        "        done = self.is_terminal_state()\n",
        "\n",
        "        # Print results and write results to file\n",
        "        self.output_stats(writer, cell_index, action_type, reward)\n",
        "        \n",
        "        # Update the state tensor with the new state gdf\n",
        "        self.state_tensor = self.gdf_to_tensor(self.state_gdf)\n",
        "\n",
        "        self.step_count += 1\n",
        "        \n",
        "        return self.state_tensor, reward, done, {}\n",
        "\n",
        "    def is_valid_action(self, cell_index, action_type):\n",
        "        # Implement logic to check if an action is valid\n",
        "        cell = self.state_gdf.iloc[cell_index]\n",
        "        if cell['masked']: # This should never occur\n",
        "            print('Error: Attempting to build on a masked cell')\n",
        "            return False\n",
        "        elif cell['occupied']:\n",
        "            return False\n",
        "        elif action_type == 'solar' and cell['slope'] > 8.749: # This should never occur\n",
        "            print('Error: Attempting to build solar on slope of more than 5%')\n",
        "            return False\n",
        "        elif action_type == 'wind' and cell['slope'] > 26.795: # This should never occur\n",
        "            print('Error: Attempting to build wind on slope of more than 15%')\n",
        "            return False\n",
        "        \n",
        "        return True\n",
        "\n",
        "    def apply_action(self, cell_index, action_type):\n",
        "        # Implement the changes to the environment based on the action\n",
        "        # Example: Mark the cell as occupied and record the type of installation\n",
        "        self.state_gdf.at[cell_index, 'occupied'] = 1\n",
        "        self.state_gdf.at[cell_index, 'installation_type'] = action_type\n",
        "\n",
        "    def calculate_reward(self):\n",
        "        # Solar installation cost\n",
        "        solar_cost = self.calculate_solar_cost()\n",
        "\n",
        "        # Wind turbine installation cost\n",
        "        wind_cost = self.calculate_wind_cost()\n",
        "        \n",
        "        # Solar power Reward\n",
        "        power_output_reward = self.calculate_power_output_reward()\n",
        "\n",
        "        # Transmission build cost\n",
        "        transmission_build_cost = self.calculate_transmission_build_cost()\n",
        "        \n",
        "        # Cyclone risk cost\n",
        "        cyclone_risk_cost = self.calculate_cyclone_risk_cost()\n",
        "    \n",
        "        # Distribution reward\n",
        "        # distributed_grid_reward = self.calculate_distributed_grid_reward()\n",
        "    \n",
        "        # Early choice reward\n",
        "        # early_choice_reward = self.time_dependent_reward_factor()\n",
        "\n",
        "        total_reward = power_output_reward + solar_cost + wind_cost + cyclone_risk_cost + transmission_build_cost\n",
        "        \n",
        "        print(f'Power Output Reward: {power_output_reward:8.5f} | Solar Cost: {solar_cost:8.5f} | Wind Cost: {wind_cost:8.5f} | Transmission Build Cost: {transmission_build_cost:8.5f} | Cyclone Risk Cost: {cyclone_risk_cost:8.5f} | Total Reward: {total_reward:8.5f}')\n",
        "\n",
        "        return total_reward\n",
        "\n",
        "    def calculate_energy_output(self):\n",
        "        # Filter for solar and wind installations\n",
        "        solar_gdf = self.state_gdf[self.state_gdf['installation_type'] == 'solar']\n",
        "        wind_gdf = self.state_gdf[self.state_gdf['installation_type'] == 'wind']\n",
        "    \n",
        "        # Prepare column names for solar and wind power\n",
        "        solar_power_columns = [f'solar_power_kW_hour_{i}' for i in range(1, 25)]\n",
        "        wind_power_columns = [f'wind_power_kW_hour_{i}' for i in range(1, 25)]\n",
        "    \n",
        "        # Vectorized sum of power output for solar and wind for each hour\n",
        "        total_solar_power = solar_gdf[solar_power_columns].sum().to_numpy() * 1000\n",
        "        total_wind_power = wind_gdf[wind_power_columns].sum().to_numpy() * 1000\n",
        "    \n",
        "        total_power = total_solar_power + total_wind_power\n",
        "        \n",
        "        return total_power\n",
        "\n",
        "    def calculate_power_output_reward(self):\n",
        "        \"\"\"Uses supply and demand curve to determine the amount of demand satisfied by\n",
        "        the solar and wind installations\"\"\"\n",
        "        cost_kWh = .22  # TODO get more rigorous number \n",
        "        # Storage cost = $400/kWh over the 15 year lifetime of a 4 hour battery\n",
        "        # cost_storage_kWh = 400 / (365.25 * 15)\n",
        "        demand = self.demand\n",
        "\n",
        "        # Calculate the reward using vectorized minimum\n",
        "        total_power = self.calculate_energy_output()\n",
        "        demand_satisfied = np.minimum(demand, total_power)\n",
        "        demand_satisfied_ratio = demand_satisfied / demand\n",
        "\n",
        "        self.update_demand(demand_satisfied_ratio)\n",
        "        \n",
        "        power_reward = demand_satisfied.sum() * cost_kWh\n",
        "        # self.stored_energy = np.maximum(total_power - demand, 0).sum()\n",
        "        # storage_cost = self.stored_energy * cost_storage_kWh\n",
        "\n",
        "        reward = power_reward # - storage_cost\n",
        "        \n",
        "        return reward\n",
        "\n",
        "    def update_demand(self, demand_satisfied_ratio):\n",
        "        unsatisfied_demand_ratio = 1 - demand_satisfied_ratio\n",
        "        for i in range(1, 25):\n",
        "            self.state_gdf[f'demand_{i}'] *= unsatisfied_demand_ratio[i-1]\n",
        "    \n",
        "    def calculate_solar_cost(self):\n",
        "        \"\"\"Cost of installing solar array on cell. Based on elevation and slope\"\"\"\n",
        "        return -self.solar_daily_cost * len(self.state_gdf[self.state_gdf['installation_type'] == 'solar'])\n",
        "\n",
        "    def calculate_wind_cost(self):\n",
        "        \"\"\"Cost of installing wind turbine on cell. Based on elevation and slope\"\"\"\n",
        "        return -self.wind_daily_cost * len(self.state_gdf[self.state_gdf['installation_type'] == 'wind'])\n",
        "\n",
        "    def transmission_line_cost_per_km(self, distance):\n",
        "        # $2.29 million per mile divided by 1.60934 to get into km, \n",
        "        # then divided by 25 * 365.25 for daily costs with 25 year decommission time\n",
        "        distance /= 1000 # Convert to km\n",
        "        KM_PER_MILE = 1.60934\n",
        "        COST_PER_KM = 2.29 * 1000000 / (KM_PER_MILE * 25 * 365.25) \n",
        "        SHORT_DISTANCE_THRESHOLD = 3 * KM_PER_MILE # Threshold for short distance \n",
        "        MEDIUM_DISTANCE_THRESHOLD = 10 * KM_PER_MILE # Threshold for medium distance \n",
        "        \n",
        "        if distance < SHORT_DISTANCE_THRESHOLD:\n",
        "            cost_modifier = 1.5  # 50% increase for less than 3 miles\n",
        "        elif distance < MEDIUM_DISTANCE_THRESHOLD:\n",
        "            cost_modifier = 1.2  # 20% increase for 3-10 miles\n",
        "        else:\n",
        "            cost_modifier = 1  # No modification for more than 10 miles\n",
        "        return -distance * COST_PER_KM * cost_modifier\n",
        "    \n",
        "    def calculate_transmission_build_cost(self):\n",
        "        occupied_cells = self.state_gdf[self.state_gdf['occupied'] == 1]\n",
        "        \n",
        "        # Check if there is only one occupied cell\n",
        "        if len(occupied_cells) == 0:\n",
        "            return 0\n",
        "        elif len(occupied_cells) == 1:\n",
        "            # For a single occupied cell, use the distance to transmission line for cost calculation\n",
        "            occupied_cell = occupied_cells.iloc[0]\n",
        "            distance_km = occupied_cell['distance_to_transmission_line'] * 1000\n",
        "            build_cost = self.transmission_line_cost_per_km(distance_km)\n",
        "        else:\n",
        "            # Get coordinates of occupied cells\n",
        "            coords = np.array(list(zip(occupied_cells.geometry.centroid.x, occupied_cells.geometry.centroid.y)))\n",
        "    \n",
        "            # Calculate pairwise distances between occupied cells\n",
        "            distances = cdist(coords, coords)\n",
        "    \n",
        "            # Replace zeros in distance matrix with np.inf to avoid zero distance to itself\n",
        "            np.fill_diagonal(distances, np.inf)\n",
        "    \n",
        "            # Find the nearest installation for each installation\n",
        "            nearest_installation_distances = np.min(distances, axis=1)\n",
        "    \n",
        "            # Determine the relevant distance for cost calculation\n",
        "            relevant_distances = np.minimum(nearest_installation_distances, occupied_cells['distance_to_transmission_line'].to_numpy()) * 1000\n",
        "    \n",
        "            # Calculate build cost\n",
        "            build_costs = [self.transmission_line_cost_per_km(distance) for distance in relevant_distances]\n",
        "            build_cost = sum(build_costs)\n",
        "            \n",
        "        return build_cost\n",
        "\n",
        "    def calculate_cyclone_risk_cost(self):\n",
        "        if len(self.state_gdf[self.state_gdf.installation_type == 'wind']) == 0:\n",
        "            return 0\n",
        "        # Wind operational expenses = $40/kW/yr\n",
        "        # Wind turbine capacity is 3 MW\n",
        "        # To get daily cost, multiply by 3000 (3 MW = 3000 kW) and divide by 365.25 days in a year\n",
        "        wind_opex = 4 * 40 * 3000 / 365.25\n",
        "        # Wind capital expenses = $1501/kW\n",
        "        # To get daily cost, multiply by 3000 (3 MW = 3000 kW) and divide by (25 years of lifetime * 365.25 days in a year)\n",
        "        wind_capex = 4 * 1501 * 3000 / (25 * 365.25)\n",
        "        cyclone_risk_cost = -(self.state_gdf[self.state_gdf.installation_type == 'wind']['cyclone_risk'] * (wind_capex - wind_opex)).sum()\n",
        "        return cyclone_risk_cost\n",
        "    \n",
        "    def calculate_distributed_grid_reward(self):\n",
        "        # Extract the coordinates of the occupied cells (where installations are located)\n",
        "        occupied_cells = self.state_gdf[self.state_gdf['occupied'] == 1]\n",
        "        if len(occupied_cells) < 2:\n",
        "            # If there are less than two installations, we cannot calculate distances\n",
        "            return 1\n",
        "    \n",
        "        coords = np.array(list(zip(occupied_cells.geometry.x, occupied_cells.geometry.y)))\n",
        "    \n",
        "        # Calculate pairwise distances between all occupied cells\n",
        "        distances = pdist(coords)\n",
        "    \n",
        "        # Calculate the average distance. The larger this is, the more distributed the installations are.\n",
        "        avg_distance = np.mean(distances)\n",
        "    \n",
        "        # Normalize the reward such that it ranges between 0 and 1\n",
        "        normalized_reward = avg_distance / self.max_distance\n",
        "    \n",
        "        return normalized_reward\n",
        "    \n",
        "    def time_dependent_reward_factor(self):\n",
        "        # A function to calculate the time-dependent reward factor\n",
        "        # It decreases with each year from the base year\n",
        "        action_number = self.state_gdf.occupied.sum()\n",
        "        return 1 / (1 + self.decay_rate * action_number)\n",
        "        \n",
        "    def is_terminal_state(self):\n",
        "        # The episode ends when there is no unmet demand\n",
        "        self.unmet_demand = np.maximum(self.demand - self.total_energy_output, 0).sum()\n",
        "        return self.unmet_demand <= 0 or self.total_energy_output.sum() >= self.total_demand \n",
        "\n",
        "    def render(self):\n",
        "        # Optional: Implement a method to visualize the current state of the environment\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ac168609-d297-4963-b880-bcfdc86c2f6e",
      "metadata": {
        "id": "ac168609-d297-4963-b880-bcfdc86c2f6e"
      },
      "outputs": [],
      "source": [
        "# class RenewableEnergyEnvironment:\n",
        "#     def __init__(self, grid_gdf):\n",
        "#         self.weights = {\n",
        "#         'transmission_build_cost': -1.0,\n",
        "#         'early_choice_reward': 1.0,\n",
        "#         'distributed_grid_reward': 1.0,\n",
        "#         }\n",
        "\n",
        "#         self.wind_daily_cost = 821.68\n",
        "#         self.solar_daily_cost = 3007.26\n",
        "\n",
        "#         self.grid_columns = ['slope',\n",
        "#                              'distance_to_transmission_line',\n",
        "#                              'cyclone_risk',\n",
        "#                              'water',\n",
        "#                              'occupied_solar',\n",
        "#                              'occupied_wind',\n",
        "#                              'masked']\n",
        "#         self.grid_columns += [f'demand_{i}' for i in range(1,25)]\n",
        "#         self.grid_columns += [f'wind_power_kW_hour_{i}' for i in range(1,25)]\n",
        "#         self.grid_columns += [f'solar_power_kW_hour_{i}' for i in range(1,25)]\n",
        "\n",
        "#         # Initialize the environment\n",
        "#         self.starting_environment = grid_gdf\n",
        "#         self.state_gdf = grid_gdf.copy()\n",
        "#         self.state_tensor = self.gdf_to_tensor(self.state_gdf)\n",
        "#         self.mapping, self.action_space_size = self.create_action_to_gdf_mapping()\n",
        "\n",
        "#         self.total_energy_output = 0\n",
        "#         self.stored_energy = 0\n",
        "\n",
        "#         demand_df = pd.read_csv('../data/generation_and_demand/demand_profile.csv')\n",
        "#         # demand_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/GreenGridPR/data/generation_and_demand/demand_profile.csv')\n",
        "#         self.demand = demand_df['demand_MW'].to_numpy() * 1000\n",
        "#         self.unmet_demand = self.demand.sum()\n",
        "#         self.total_demand = self.unmet_demand\n",
        "\n",
        "#         self.decay_rate = 0.1 #TODO determine good decay rate\n",
        "#         self.max_distance = 1000 #TODO get max distance between two cells\n",
        "\n",
        "#     def reset(self):\n",
        "#         # Reset the environment to the initial state\n",
        "#         self.state_gdf = self.starting_environment.copy()\n",
        "#         self.state_tensor = self.gdf_to_tensor(self.starting_environment)\n",
        "#         self.total_energy_output = 0\n",
        "#         return self.state_tensor\n",
        "\n",
        "#     def gdf_to_tensor(self, gdf):\n",
        "#         # Calculate grid dimensions\n",
        "#         x_start = 100000\n",
        "#         x_end = 300000\n",
        "#         y_start = 200000\n",
        "#         y_end = 300000\n",
        "#         square_size = 500\n",
        "\n",
        "#         grid_width = int((x_end - x_start) / square_size)\n",
        "#         grid_height = int((y_end - y_start) / square_size)\n",
        "\n",
        "#         # Flatten the grid data\n",
        "#         flat_data = self.state_gdf[self.grid_columns].values.reshape(-1, grid_height, grid_width)\n",
        "\n",
        "#         # Create a 4D Tensor (batch size is the 4th dimension)\n",
        "#         tensor = torch.tensor(flat_data, dtype=torch.float32)\n",
        "\n",
        "#         return tensor\n",
        "\n",
        "#     def create_action_to_gdf_mapping(self):\n",
        "#         unmasked_gdf = self.state_gdf[self.state_gdf['masked'] == 0]\n",
        "\n",
        "#         mapping = {}\n",
        "#         action_idx = 0  # Initialize action index\n",
        "\n",
        "#         for _, row in unmasked_gdf.iterrows():\n",
        "#             # Check for valid solar action\n",
        "#             if row['slope'] <= 8.749:  # Slope check for solar\n",
        "#                 mapping[action_idx] = (row.name, 'solar')\n",
        "#                 action_idx += 1\n",
        "\n",
        "#             # Check for valid wind action\n",
        "#             if row['slope'] <= 26.795:  # Slope check for wind\n",
        "#                 mapping[action_idx] = (row.name, 'wind')\n",
        "#                 action_idx += 1\n",
        "\n",
        "#         action_space_size = action_idx  # Total number of valid actions\n",
        "#         return mapping, action_space_size\n",
        "\n",
        "\n",
        "#     def step(self, action):\n",
        "#         # Apply the action to the environment and return the result\n",
        "#         # action: Tuple (cell_index, action_type) where action_type could be 'solar' or 'wind'\n",
        "\n",
        "#         cell_index, action_type = self.mapping[action]\n",
        "\n",
        "#         # Check if the action is valid\n",
        "#         if not self.is_valid_action(cell_index, action_type):\n",
        "#             reward = -1  # Penalty for invalid action\n",
        "#             done = self.is_terminal_state()\n",
        "#             return self.state_tensor, reward, done, {}\n",
        "\n",
        "#         # Calculate the total reward before applying the action\n",
        "#         total_reward_before_action = self.calculate_reward()\n",
        "\n",
        "#         # Apply the action\n",
        "#         self.apply_action(cell_index, action_type)\n",
        "\n",
        "#         # Calculate the total reward after applying the action\n",
        "#         total_reward_after_action = self.calculate_reward()\n",
        "\n",
        "#         # The reward for the action is the difference in total reward\n",
        "#         reward = total_reward_after_action - total_reward_before_action\n",
        "\n",
        "#         # Update total energy output or other state attributes as needed\n",
        "#         self.total_energy_output = self.calculate_energy_output()\n",
        "\n",
        "#         # Check if the state is terminal\n",
        "#         done = self.is_terminal_state()\n",
        "\n",
        "#         print(f'Cell: {cell_index:<6d} | Action: {action_type:<5} | Reward: {int(reward):<7} | Energy Output: {int(self.total_energy_output.sum()):<10} | Unmet Demand: {int(self.unmet_demand):<10}')\n",
        "\n",
        "#         # Update the state tensor with the new state gdf\n",
        "#         self.state_tensor = self.gdf_to_tensor(self.state_gdf)\n",
        "\n",
        "#         return self.state_tensor, reward, done, {}\n",
        "\n",
        "#     def is_valid_action(self, cell_index, action_type):\n",
        "#         # Implement logic to check if an action is valid\n",
        "#         # Example: Check if the cell is not masked and not already occupied\n",
        "#         cell = self.state_gdf.iloc[cell_index]\n",
        "#         if cell['masked']: # This should never occur\n",
        "#             print('Something has gone wrong. Attempting to build on a masked cell')\n",
        "#             return False\n",
        "#         elif cell['occupied_solar']:\n",
        "#             return False\n",
        "#         elif cell['occupied_wind']:\n",
        "#             return False\n",
        "#         elif action_type == 'solar' and cell['slope'] > 8.749:\n",
        "#             return False\n",
        "#         elif action_type == 'wind' and cell['slope'] > 26.795:\n",
        "#             return False\n",
        "\n",
        "#         return True\n",
        "\n",
        "#     def apply_action(self, cell_index, action_type):\n",
        "#         # Implement the changes to the environment based on the action\n",
        "#         # Example: Mark the cell as occupied and record the type of installation\n",
        "#         if action_type == 'solar':\n",
        "#             self.state_gdf.at[cell_index, 'occupied_solar'] = 1\n",
        "#         elif action_type == 'wind':\n",
        "#             self.state_gdf.at[cell_index, 'occupied_wind'] = 1\n",
        "\n",
        "#     def calculate_reward(self):\n",
        "#         # Solar installation cost\n",
        "#         solar_cost = self.calculate_solar_cost()\n",
        "\n",
        "#         # Wind turbine installation cost\n",
        "#         wind_cost = self.calculate_wind_cost()\n",
        "\n",
        "#         # Solar power Reward\n",
        "#         power_output_reward = self.calculate_power_output_reward()\n",
        "\n",
        "#         # Transmission build cost\n",
        "#         transmission_build_cost = self.calculate_transmission_build_cost()\n",
        "\n",
        "#         # Cyclone risk cost\n",
        "#         cyclone_risk_cost = self.calculate_cyclone_risk_cost()\n",
        "\n",
        "#         # Distribution reward\n",
        "#         # distributed_grid_reward = self.calculate_distributed_grid_reward()\n",
        "\n",
        "#         # Early choice reward\n",
        "#         # early_choice_reward = self.time_dependent_reward_factor()\n",
        "\n",
        "#         total_reward = power_output_reward + solar_cost + wind_cost + cyclone_risk_cost + transmission_build_cost\n",
        "\n",
        "#         return total_reward\n",
        "\n",
        "#     def calculate_energy_output(self):\n",
        "#         # Filter for solar and wind installations\n",
        "#         solar_gdf = self.state_gdf[self.state_gdf['occupied_solar'] == 1]\n",
        "#         wind_gdf = self.state_gdf[self.state_gdf['occupied_wind'] == 1]\n",
        "\n",
        "#         # Prepare column names for solar and wind power\n",
        "#         solar_power_columns = [f'solar_power_kW_hour_{i}' for i in range(1, 25)]\n",
        "#         wind_power_columns = [f'wind_power_kW_hour_{i}' for i in range(1, 25)]\n",
        "\n",
        "#         # Vectorized sum of power output for solar and wind for each hour\n",
        "#         total_solar_power = solar_gdf[solar_power_columns].sum().to_numpy()\n",
        "#         total_wind_power = wind_gdf[wind_power_columns].sum().to_numpy()\n",
        "\n",
        "#         total_power = total_solar_power + total_wind_power\n",
        "\n",
        "#         return total_power\n",
        "\n",
        "#     def calculate_power_output_reward(self):\n",
        "#         \"\"\"Uses supply and demand curve to determine the amount of demand satisfied by\n",
        "#         the solar and wind installations\"\"\"\n",
        "#         cost_kWh = .22  # TODO\n",
        "#         # Storage cost = $400/kWh over the 15 year lifetime of a 4 hour battery\n",
        "#         # cost_storage_kWh = 400 / (365.25 * 15)\n",
        "#         demand = self.demand\n",
        "\n",
        "#         # Calculate the reward using vectorized minimum\n",
        "#         total_power = self.calculate_energy_output()\n",
        "#         demand_satisfied = np.minimum(demand, total_power)\n",
        "#         demand_satisfied_ratio = demand_satisfied / demand\n",
        "\n",
        "#         self.update_demand(demand_satisfied_ratio)\n",
        "\n",
        "#         power_reward = demand_satisfied.sum() * cost_kWh\n",
        "#         # self.stored_energy = np.maximum(total_power - demand, 0).sum()\n",
        "#         # storage_cost = self.stored_energy * cost_storage_kWh\n",
        "\n",
        "#         reward = power_reward # - storage_cost\n",
        "\n",
        "#         return reward\n",
        "\n",
        "#     def update_demand(self, demand_satisfied_ratio):\n",
        "#         unsatisfied_demand_ratio = 1 - demand_satisfied_ratio\n",
        "#         for i in range(1, 25):\n",
        "#             self.state_gdf[f'demand_{i}'] *= unsatisfied_demand_ratio[i-1]\n",
        "\n",
        "#     def calculate_solar_cost(self):\n",
        "#         \"\"\"Cost of installing solar array on cell. Based on elevation and slope\"\"\"\n",
        "#         return -self.solar_daily_cost * len(self.state_gdf[self.state_gdf['occupied_solar'] == 1])\n",
        "\n",
        "#     def calculate_wind_cost(self):\n",
        "#         \"\"\"Cost of installing wind turbine on cell. Based on elevation and slope\"\"\"\n",
        "#         return -self.wind_daily_cost * len(self.state_gdf[self.state_gdf['occupied_wind'] == 1])\n",
        "\n",
        "#     def transmission_line_cost_per_km(self, distance):\n",
        "#         # $2.29 million per mile divided by 1.60934 to get into km,\n",
        "#         # then divided by 25 * 365.25 for daily costs with 25 year decommission time\n",
        "#         distance /= 1000 # Convert to km\n",
        "#         KM_PER_MILE = 1.60934\n",
        "#         COST_PER_KM = 2.29 * 1000000 / (KM_PER_MILE * 25 * 365.25)\n",
        "#         SHORT_DISTANCE_THRESHOLD = 3 * KM_PER_MILE # Threshold for short distance\n",
        "#         MEDIUM_DISTANCE_THRESHOLD = 10 * KM_PER_MILE # Threshold for medium distance\n",
        "\n",
        "#         if distance < SHORT_DISTANCE_THRESHOLD:\n",
        "#             cost_modifier = 1.5  # 50% increase for less than 3 miles\n",
        "#         elif distance < MEDIUM_DISTANCE_THRESHOLD:\n",
        "#             cost_modifier = 1.2  # 20% increase for 3-10 miles\n",
        "#         else:\n",
        "#             cost_modifier = 1  # No modification for more than 10 miles\n",
        "#         return -distance * COST_PER_KM * cost_modifier\n",
        "\n",
        "#     def calculate_transmission_build_cost(self):\n",
        "#         occupied_cells = self.state_gdf[(self.state_gdf['occupied_solar'] == 1) | (self.state_gdf['occupied_wind'] == 1)]\n",
        "\n",
        "#         # Check if there is only one occupied cell\n",
        "#         if len(occupied_cells) == 0:\n",
        "#             return 0\n",
        "#         elif len(occupied_cells) == 1:\n",
        "#             # For a single occupied cell, use the distance to transmission line for cost calculation\n",
        "#             occupied_cell = occupied_cells.iloc[0]\n",
        "#             distance_km = occupied_cell['distance_to_transmission_line']\n",
        "#             build_cost = self.transmission_line_cost_per_km(distance_km)\n",
        "#         else:\n",
        "#             # Get coordinates of occupied cells\n",
        "#             coords = np.array(list(zip(occupied_cells.geometry.centroid.x, occupied_cells.geometry.centroid.y)))\n",
        "\n",
        "#             # Calculate pairwise distances between occupied cells\n",
        "#             distances = cdist(coords, coords)\n",
        "\n",
        "#             # Replace zeros in distance matrix with np.inf to avoid zero distance to itself\n",
        "#             np.fill_diagonal(distances, np.inf)\n",
        "\n",
        "#             # Find the nearest installation for each installation\n",
        "#             nearest_installation_distances = np.min(distances, axis=1)\n",
        "\n",
        "#             # Determine the relevant distance for cost calculation\n",
        "#             relevant_distances = np.minimum(nearest_installation_distances, occupied_cells['distance_to_transmission_line'].to_numpy())\n",
        "\n",
        "#             # Calculate build cost\n",
        "#             build_costs = [self.transmission_line_cost_per_km(distance) for distance in relevant_distances]\n",
        "#             build_cost = sum(build_costs)\n",
        "\n",
        "#         return build_cost\n",
        "\n",
        "#     def calculate_cyclone_risk_cost(self):\n",
        "#         if len(self.state_gdf[self.state_gdf.occupied_wind == 1]) == 0:\n",
        "#             return 0\n",
        "#         # Wind operational expenses = $40/kW/yr\n",
        "#         # Wind turbine capacity is 3 MW\n",
        "#         # To get daily cost, multiply by 3000 (3 MW = 3000 kW) and divide by 365.25 days in a year\n",
        "#         wind_opex = 40 * 3000 / 365.25\n",
        "#         # Wind capital expenses = $1501/kW\n",
        "#         # To get daily cost, multiply by 3000 (3 MW = 3000 kW) and divide by (25 years of lifetime * 365.25 days in a year)\n",
        "#         wind_capex = 1501 * 3000 / (25 * 365.25)\n",
        "#         cyclone_risk_cost = -(self.state_gdf[self.state_gdf.occupied_wind == 1]['cyclone_risk'] * (wind_capex - wind_opex)).sum()\n",
        "#         return cyclone_risk_cost\n",
        "\n",
        "#     def calculate_distributed_grid_reward(self):\n",
        "#         # Extract the coordinates of the occupied cells (where installations are located)\n",
        "#         occupied_cells = self.state_gdf[(self.state_gdf['occupied_solar'] == 1) | (self.state_gdf['occupied_wind'] == 1)]\n",
        "#         if len(occupied_cells) < 2:\n",
        "#             # If there are less than two installations, we cannot calculate distances\n",
        "#             return 1\n",
        "\n",
        "#         coords = np.array(list(zip(occupied_cells.geometry.x, occupied_cells.geometry.y)))\n",
        "\n",
        "#         # Calculate pairwise distances between all occupied cells\n",
        "#         distances = pdist(coords)\n",
        "\n",
        "#         # Calculate the average distance. The larger this is, the more distributed the installations are.\n",
        "#         avg_distance = np.mean(distances)\n",
        "\n",
        "#         # Normalize the reward such that it ranges between 0 and 1\n",
        "#         normalized_reward = avg_distance / self.max_distance\n",
        "\n",
        "#         return normalized_reward\n",
        "\n",
        "#     def time_dependent_reward_factor(self):\n",
        "#         # A function to calculate the time-dependent reward factor\n",
        "#         # It decreases with each year from the base year\n",
        "#         action_number = self.state_gdf.occupied_solar.sum() + self.state_gdf.occupied_wind.sum()\n",
        "#         return 1 / (1 + self.decay_rate * action_number)\n",
        "\n",
        "#     def is_terminal_state(self):\n",
        "#         # The episode ends when there is no unmet demand\n",
        "#         self.unmet_demand = np.maximum(self.demand - self.total_energy_output, 0).sum()\n",
        "#         # self.unmet_demand -= self.stored_energy\n",
        "#         return self.unmet_demand <= 0 or self.total_energy_output.sum() >= 2 * self.total_demand\n",
        "\n",
        "#     def render(self):\n",
        "#         # Optional: Implement a method to visualize the current state of the environment\n",
        "#         pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i4ouI5o064pg",
      "metadata": {
        "id": "i4ouI5o064pg"
      },
      "source": [
        "NN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "QyS8SzYIHrcK",
      "metadata": {
        "id": "QyS8SzYIHrcK"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, net_width):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        # self.conv1 = nn.Conv2d(state_dim, 128, kernel_size=3, stride=2)\n",
        "        # self.bn1 = nn.BatchNorm2d(128)\n",
        "        # self.conv2 = nn.Conv2d(128, 128, kernel_size=3, stride=2)\n",
        "        # self.bn2 = nn.BatchNorm2d(128)\n",
        "        # self.conv3 = nn.Conv2d(128, 128, kernel_size=3, stride=2)\n",
        "        # self.bn3 = nn.BatchNorm2d(128)\n",
        "        \n",
        "        self.num_nontemporal_channels = 2\n",
        "        self.num_temporal_channels = state_dim[0] - self.num_nontemporal_channels  # Subtracting the non-temporal channels\n",
        "        print(state_dim[0])\n",
        "        self.time_dimension = 24\n",
        "        self.grid_height = state_dim[1]\n",
        "        self.grid_width = state_dim[2]\n",
        "\n",
        "        # 3D Convolutional layers for temporal features\n",
        "        # print(self.num_temporal_channels // 24)\n",
        "        self.conv3d1 = nn.Conv3d(self.num_temporal_channels // 24, 3, kernel_size=(3, 3, 3), stride=(1, 2, 2))\n",
        "        self.bn3d1 = nn.BatchNorm3d(3)\n",
        "        self.conv3d2 = nn.Conv3d(3, 3, kernel_size=(3, 3, 3), stride=(1, 2, 2))\n",
        "        self.bn3d2 = nn.BatchNorm3d(3)\n",
        "        # Add more layers if necessary\n",
        "\n",
        "        # 2D Convolutional layers for non-temporal features\n",
        "        self.conv2d1 = nn.Conv2d(self.num_nontemporal_channels, self.num_nontemporal_channels, kernel_size=3, stride=2)\n",
        "        self.bn2d1 = nn.BatchNorm2d(self.num_nontemporal_channels)\n",
        "        self.conv2d2 = nn.Conv2d(self.num_nontemporal_channels, self.num_nontemporal_channels, kernel_size=3, stride=2)\n",
        "        self.bn2d2 = nn.BatchNorm2d(self.num_nontemporal_channels)\n",
        "        # Add more layers if necessary\n",
        "\n",
        "        # Fully connected layer for processing 'occupied' cells feature\n",
        "        self.fc_occupied = nn.Linear(self.grid_height * self.grid_width, 1024)\n",
        "\n",
        "        # Calculate the size of the combined feature vector\n",
        "        combined_feature_size = self.calculate_combined_feature_size(state_dim)\n",
        "\n",
        "        # Fully connected layers for decision making\n",
        "        self.fc1 = nn.Linear(73936, 1024)\n",
        "        self.fc2 = nn.Linear(1024, action_dim)\n",
        "\n",
        "    def calculate_combined_feature_size(self, state_dim):\n",
        "        # Dummy inputs for calculating feature sizes\n",
        "        dummy_temporal = torch.zeros(1, 72 // 24, self.time_dimension, self.grid_height, self.grid_width)\n",
        "        dummy_nontemporal = torch.zeros(1, self.num_nontemporal_channels, self.grid_height, self.grid_width)\n",
        "        dummy_occupied = torch.zeros(1, 1, self.grid_height, self.grid_width)\n",
        "\n",
        "        # Forward pass through convolutional layers\n",
        "        x_temporal = self.bn3d2(self.conv3d2(self.bn3d1(self.conv3d1(dummy_temporal))))\n",
        "        x_nontemporal = self.bn2d2(self.conv2d2(self.bn2d1(self.conv2d1(dummy_nontemporal))))\n",
        "        x_occupied = dummy_occupied.view(1, -1)\n",
        "\n",
        "        # Calculate the total number of features\n",
        "        total_features = x_temporal.numel() + x_nontemporal.numel() + x_occupied.numel()\n",
        "        return total_features\n",
        "    \n",
        "    #     self.input_shape = state_dim  # Store input_shape for feature size calculation\n",
        "    #     print(f'Input shape: {state_dim}, n Actions: {action_dim}')\n",
        "    #     self.fc1 = nn.Linear(33792, 1024)\n",
        "    #     self.fc2 = nn.Linear(1024, action_dim)\n",
        "\n",
        "    def pi(self, state, softmax_dim = 0):\n",
        "        x =  state.view(-1, 75, 100, 200)\n",
        "        print( f'x1: {x}')\n",
        "\n",
        "        # Split input into temporal and non-temporal components\n",
        "        x_temporal = x[:, 3:, :, :].view(x.size(0), self.num_temporal_channels // 24, self.time_dimension, self.grid_height, self.grid_width)\n",
        "        print( f'x_temporal1: {x_temporal}')\n",
        "\n",
        "        x_nontemporal = x[:, :2, :, :]  # Distance to transmission line & cyclone risk\n",
        "        print( f'x_nontemporal1: {x_nontemporal}')\n",
        "\n",
        "        x_occupied = x[:, 2:3, :, :].view(x.size(0), -1)  # Occupied cells\n",
        "        print( f'x_occupied: {x_occupied}')\n",
        "\n",
        "        # Process temporal features\n",
        "        x_temporal = F.relu(self.bn3d1(self.conv3d1(x_temporal)))\n",
        "        print( f'x_temporal2: {x_temporal}')\n",
        "\n",
        "        x_temporal = F.relu(self.bn3d2(self.conv3d2(x_temporal)))\n",
        "        print( f'x_temporal2: {x_temporal}')\n",
        "\n",
        "        x_temporal = x_temporal.view(x_temporal.size(0), -1)\n",
        "        print( f'x_temporal2: {x_temporal}')\n",
        "\n",
        "        # Process non-temporal features\n",
        "        x_nontemporal = F.relu(self.bn2d1(self.conv2d1(x_nontemporal)))\n",
        "        print( f'x_nontemporal2: {x_nontemporal}')\n",
        "\n",
        "        x_nontemporal = F.relu(self.bn2d2(self.conv2d2(x_nontemporal)))\n",
        "        print( f'x_nontemporal3: {x_nontemporal}')\n",
        "\n",
        "        x_nontemporal = x_nontemporal.view(x_nontemporal.size(0), -1)\n",
        "        print( f'x_nontemporal4: {x_nontemporal}')\n",
        "\n",
        "        # Process 'occupied' cells feature\n",
        "        x_occupied = F.relu(self.fc_occupied(x_occupied))\n",
        "        print(f'x_occupied2: {x_occupied}')\n",
        "\n",
        "        # Combine features\n",
        "        x_combined = torch.cat((x_temporal, x_nontemporal, x_occupied), dim=1)\n",
        "        print(f'x_combined1: {x_combined}')\n",
        "\n",
        "        # Fully connected layers\n",
        "        x_combined = F.relu(self.fc1(x_combined))\n",
        "        print(f'x_combined2: {x_combined}')\n",
        "\n",
        "        return self.fc2(x_combined)\n",
        "    \n",
        "    #     print('start : ',state.size())\n",
        "    #     print('start : ',state)\n",
        "\n",
        "        # x =  state.view(-1, 75, 100, 200)\n",
        "        \n",
        "    #     print( f'x1 shape: {x.size()}')\n",
        "    #     print( f'x1 shape: {x}')\n",
        "\n",
        "    #     x = F.relu(self.bn1(self.conv1(x)))\n",
        "    #     print( f'x2 shape: {x.size()}')\n",
        "    #     print( f'x2 shape: {x}')\n",
        "\n",
        "    #     x = F.relu(self.bn2(self.conv2(x)))\n",
        "    #     print( f'x3 shape: {x.size()}')\n",
        "    #     print( f'x3 shape: {x}')\n",
        "\n",
        "    #     x = F.relu(self.bn3(self.conv3(x)))\n",
        "    #     print( f'x4 shape: {x.size()}')\n",
        "    #     print( f'x4 shape: {x}')\n",
        "\n",
        "    #     x = x.view(x.size(0), -1)\n",
        "    #     print( f'x5 shape: {x.size()}')\n",
        "    #     print( f'x5 shape: {x}')\n",
        "\n",
        "    #     x = F.relu(self.fc1(x))\n",
        "    #     print( f'x6 shape: {x.size()}')\n",
        "    #     print( f'x6 shape: {x}')\n",
        "\n",
        "    #     x = self.fc2(x)\n",
        "    #     print( f'x shape: {x.size()}')\n",
        "    #     print( f'x shape: {x}')\n",
        "        \n",
        "    #     prob = F.softmax(x,dim=softmax_dim)\n",
        "    #     print( f'prob shape: {prob.size()}')\n",
        "    #     print( f'prob: {prob}')\n",
        "    #     return prob\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim,net_width):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.C1 = nn.Linear(state_dim, net_width)\n",
        "        self.C2 = nn.Linear(net_width, net_width)\n",
        "        self.C3 = nn.Linear(net_width, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        v = torch.relu(self.C1(state))\n",
        "        v = torch.relu(self.C2(v))\n",
        "        v = self.C3(v)\n",
        "        return v\n",
        "\n",
        "def evaluate_policy(env, agent, turns = 3):\n",
        "    total_scores = 0\n",
        "    for j in range(turns):\n",
        "        s, info = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Take deterministic actions at test time\n",
        "            a, logprob_a = agent.select_action(s, deterministic=True)\n",
        "            s_next, r, dw, tr, info = env.step(a)\n",
        "            done = (dw or tr)\n",
        "\n",
        "            total_scores += r\n",
        "            s = s_next\n",
        "    return int(total_scores/turns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "7ee4cd1d-e7a7-4dc0-afba-26e344cc17d1",
      "metadata": {
        "id": "7ee4cd1d-e7a7-4dc0-afba-26e344cc17d1"
      },
      "outputs": [],
      "source": [
        "class PPO_discrete():\n",
        "    def __init__(self, **kwargs):\n",
        "        required_params = ['state_dim', 'action_dim', 'net_width', 'lr', 'device']\n",
        "        for param in required_params:\n",
        "            if param not in kwargs:\n",
        "                raise ValueError(f\"Parameter '{param}' must be specified\")\n",
        "\n",
        "        # Set attributes from kwargs\n",
        "        self.state_dim = kwargs['state_dim']\n",
        "        self.action_dim = kwargs['action_dim']\n",
        "        self.net_width = kwargs['net_width']\n",
        "        self.lr = kwargs['lr']\n",
        "        self.dvc = kwargs['device']\n",
        "        self.T_horizon = kwargs['T_horizon']\n",
        "\n",
        "        # print('state dim :  ',self.state_dim.size())\n",
        "        # print('action dim :  ',self.action_dim)\n",
        "        # print('net width :  ',self.net_width)\n",
        "\n",
        "        '''Build Actor and Critic'''\n",
        "        self.actor = Actor(self.state_dim, self.action_dim, self.net_width).to(self.dvc)\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.lr)\n",
        "        self.critic = Critic(self.state_dim[0], self.net_width).to(self.dvc)\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.lr)\n",
        "\n",
        "        '''Build Trajectory holder'''\n",
        "        self.s_hoder = np.zeros((self.T_horizon, 75, 100, 200), dtype=np.float16)\n",
        "        self.a_hoder = np.zeros((self.T_horizon, 1), dtype=np.int64)\n",
        "        self.r_hoder = np.zeros((self.T_horizon, 1), dtype=np.float16)\n",
        "        self.s_next_hoder = np.zeros((self.T_horizon, 75, 100, 200), dtype=np.float16)\n",
        "        self.logprob_a_hoder = np.zeros((self.T_horizon, 1), dtype=np.float16)\n",
        "        self.done_hoder = np.zeros((self.T_horizon, 1), dtype=np.bool_)\n",
        "        self.dw_hoder = np.zeros((self.T_horizon, 1), dtype=np.bool_)\n",
        "\n",
        "    def select_action(self, s, deterministic):\n",
        "        s = s.float().to(self.dvc)\n",
        "        with torch.no_grad():\n",
        "            pi = self.actor.pi(s, softmax_dim=1)\n",
        "            \n",
        "            if deterministic:\n",
        "                a = torch.argmax(pi, dim=1).item()\n",
        "                return a, None\n",
        "            else:\n",
        "                print(pi)\n",
        "                m = Categorical(pi)\n",
        "                a = m.sample().item()\n",
        "            \n",
        "\n",
        "                pi_a = pi[0,a].item()\n",
        "                return a, pi_a\n",
        "\n",
        "    def train(self):\n",
        "        self.entropy_coef *= self.entropy_coef_decay #exploring decay\n",
        "        '''Prepare PyTorch data from Numpy data'''\n",
        "        s = torch.from_numpy(self.s_hoder).to(self.dvc)\n",
        "        a = torch.from_numpy(self.a_hoder).to(self.dvc)\n",
        "        r = torch.from_numpy(self.r_hoder).to(self.dvc)\n",
        "        s_next = torch.from_numpy(self.s_next_hoder).to(self.dvc)\n",
        "        old_prob_a = torch.from_numpy(self.logprob_a_hoder).to(self.dvc)\n",
        "        done = torch.from_numpy(self.done_hoder).to(self.dvc)\n",
        "        dw = torch.from_numpy(self.dw_hoder).to(self.dvc)\n",
        "\n",
        "        ''' Use TD+GAE+LongTrajectory to compute Advantage and TD target'''\n",
        "        with torch.no_grad():\n",
        "            vs = self.critic(s)\n",
        "            vs_ = self.critic(s_next)\n",
        "\n",
        "            '''dw(dead and win) for TD_target and Adv'''\n",
        "            deltas = r + self.gamma * vs_ * (~dw) - vs\n",
        "            deltas = deltas.cpu().flatten().numpy()\n",
        "            adv = [0]\n",
        "\n",
        "            '''done for GAE'''\n",
        "            for dlt, done in zip(deltas[::-1], done.cpu().flatten().numpy()[::-1]):\n",
        "                advantage = dlt + self.gamma * self.lambd * adv[-1] * (~done)\n",
        "                adv.append(advantage)\n",
        "            adv.reverse()\n",
        "            adv = copy.deepcopy(adv[0:-1])\n",
        "            adv = torch.tensor(adv).unsqueeze(1).float().to(self.dvc)\n",
        "            td_target = adv + vs\n",
        "            if self.adv_normalization:\n",
        "                adv = (adv - adv.mean()) / ((adv.std() + 1e-4))  #sometimes helps\n",
        "\n",
        "        \"\"\"PPO update\"\"\"\n",
        "        #Slice long trajectopy into short trajectory and perform mini-batch PPO update\n",
        "        optim_iter_num = int(math.ceil(s.shape[0] / self.batch_size))\n",
        "\n",
        "        for _ in range(self.K_epochs):\n",
        "            #Shuffle the trajectory, Good for training\n",
        "            perm = np.arange(s.shape[0])\n",
        "            np.random.shuffle(perm)\n",
        "            perm = torch.LongTensor(perm).to(self.dvc)\n",
        "            s, a, td_target, adv, old_prob_a = \\\n",
        "                s[perm].clone(), a[perm].clone(), td_target[perm].clone(), adv[perm].clone(), old_prob_a[perm].clone()\n",
        "\n",
        "            '''mini-batch PPO update'''\n",
        "            for i in range(optim_iter_num):\n",
        "                index = slice(i * self.batch_size, min((i + 1) * self.batch_size, s.shape[0]))\n",
        "\n",
        "                '''actor update'''\n",
        "                prob = self.actor.pi(s[index], softmax_dim=1)\n",
        "                entropy = Categorical(prob).entropy().sum(0, keepdim=True)\n",
        "                prob_a = prob.gather(1, a[index])\n",
        "                ratio = torch.exp(torch.log(prob_a) - torch.log(old_prob_a[index]))  # a/b == exp(log(a)-log(b))\n",
        "\n",
        "                surr1 = ratio * adv[index]\n",
        "                surr2 = torch.clamp(ratio, 1 - self.clip_rate, 1 + self.clip_rate) * adv[index]\n",
        "                a_loss = -torch.min(surr1, surr2) - self.entropy_coef * entropy\n",
        "\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                a_loss.mean().backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 40)\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                '''critic update'''\n",
        "                c_loss = (self.critic(s[index]) - td_target[index]).pow(2).mean()\n",
        "                for name, param in self.critic.named_parameters():\n",
        "                    if 'weight' in name:\n",
        "                        c_loss += param.pow(2).sum() * self.l2_reg\n",
        "\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                c_loss.backward()\n",
        "                self.critic_optimizer.step()\n",
        "\n",
        "    def put_data(self, s, a, r, s_next, logprob_a, done, dw, idx):\n",
        "        # print(logprob_a)\n",
        "        # print(done)\n",
        "        # print(dw)\n",
        "        # print(idx)\n",
        "\n",
        "        self.s_hoder[idx] = s\n",
        "        self.a_hoder[idx] = a\n",
        "        self.r_hoder[idx] = r\n",
        "        self.s_next_hoder[idx] = s_next\n",
        "        self.logprob_a_hoder[idx] = logprob_a\n",
        "        self.done_hoder[idx] = done\n",
        "        self.dw_hoder[idx] = dw\n",
        "\n",
        "    def save(self, episode):\n",
        "        torch.save(self.critic.state_dict(), \"./model/ppo_critic{}.pth\".format(episode))\n",
        "        torch.save(self.actor.state_dict(), \"./model/ppo_actor{}.pth\".format(episode))\n",
        "\n",
        "    def load(self, episode):\n",
        "        self.critic.load_state_dict(torch.load(\"./model/ppo_critic{}.pth\".format(episode)))\n",
        "        self.actor.load_state_dict(torch.load(\"./model/ppo_actor{}.pth\".format(episode)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "INexYvSfHrcL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "INexYvSfHrcL",
        "outputId": "c8aafb44-69b8-44fd-fcea-81ecfa61f719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "75\n",
            "x1: tensor([[[[3.9200e+04, 3.8432e+04, 3.7632e+04,  ..., 3.8848e+04,\n",
            "           3.9648e+04, 4.0512e+04],\n",
            "          [3.8016e+04, 3.7216e+04, 3.6416e+04,  ..., 3.7728e+04,\n",
            "           3.8592e+04, 3.9456e+04],\n",
            "          [3.6896e+04, 3.6064e+04, 3.5232e+04,  ..., 3.6704e+04,\n",
            "           3.7600e+04, 3.8464e+04],\n",
            "          ...,\n",
            "          [3.8272e+04, 3.7728e+04, 3.7216e+04,  ..., 4.0864e+04,\n",
            "           4.1792e+04, 4.2720e+04],\n",
            "          [3.9968e+04, 3.9456e+04, 3.8944e+04,  ..., 4.1632e+04,\n",
            "           4.2560e+04, 4.3456e+04],\n",
            "          [4.1664e+04, 4.1184e+04, 4.0704e+04,  ..., 4.2496e+04,\n",
            "           4.3392e+04, 4.4288e+04]],\n",
            "\n",
            "         [[2.9370e-01, 2.9370e-01, 2.9370e-01,  ..., 1.0681e-01,\n",
            "           1.0681e-01, 1.0681e-01],\n",
            "          [2.6709e-01, 2.6709e-01, 2.6709e-01,  ..., 1.0681e-01,\n",
            "           1.0681e-01, 1.0681e-01],\n",
            "          [2.6709e-01, 2.6709e-01, 2.6709e-01,  ..., 1.0681e-01,\n",
            "           1.0681e-01, 1.0681e-01],\n",
            "          ...,\n",
            "          [2.6709e-01, 2.6709e-01, 2.6709e-01,  ..., 2.6709e-01,\n",
            "           2.6709e-01, 2.6709e-01],\n",
            "          [2.6709e-01, 2.6709e-01, 2.6709e-01,  ..., 2.6709e-01,\n",
            "           2.6709e-01, 2.6709e-01],\n",
            "          [2.6709e-01, 2.6709e-01, 2.6709e-01,  ..., 2.6709e-01,\n",
            "           2.6709e-01, 2.6709e-01]],\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]]]], device='cuda:0')\n",
            "x_temporal1: tensor([[[[[   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           ...,\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.]],\n",
            "\n",
            "          [[   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           ...,\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.]],\n",
            "\n",
            "          [[   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           ...,\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           ...,\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.]],\n",
            "\n",
            "          [[   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           ...,\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.]],\n",
            "\n",
            "          [[   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           ...,\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.]]],\n",
            "\n",
            "\n",
            "         [[[4968., 4948., 4876.,  ..., 7580., 7596., 7596.],\n",
            "           [4972., 4952., 4880.,  ..., 7636., 7648., 7648.],\n",
            "           [5008., 4964., 4884.,  ..., 7672., 7684., 7688.],\n",
            "           ...,\n",
            "           [5916., 5912., 5916.,  ..., 6248., 6264., 6256.],\n",
            "           [5940., 5936., 5912.,  ..., 6236., 6236., 6236.],\n",
            "           [5976., 5960., 5936.,  ..., 6224., 6224., 6220.]],\n",
            "\n",
            "          [[5000., 4980., 4908.,  ..., 7276., 7292., 7288.],\n",
            "           [5000., 4984., 4908.,  ..., 7332., 7340., 7344.],\n",
            "           [5040., 4996., 4916.,  ..., 7364., 7376., 7380.],\n",
            "           ...,\n",
            "           [5936., 5936., 5940.,  ..., 6180., 6196., 6188.],\n",
            "           [5960., 5956., 5936.,  ..., 6168., 6168., 6168.],\n",
            "           [6000., 5980., 5956.,  ..., 6156., 6156., 6152.]],\n",
            "\n",
            "          [[5008., 4992., 4920.,  ..., 6984., 7000., 7000.],\n",
            "           [5012., 4996., 4920.,  ..., 7040., 7048., 7052.],\n",
            "           [5048., 5008., 4928.,  ..., 7076., 7084., 7088.],\n",
            "           ...,\n",
            "           [5888., 5888., 5892.,  ..., 6108., 6124., 6116.],\n",
            "           [5912., 5908., 5888.,  ..., 6100., 6100., 6100.],\n",
            "           [5952., 5932., 5912.,  ..., 6084., 6084., 6084.]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[4268., 4252., 4192.,  ..., 8180., 8200., 8192.],\n",
            "           [4272., 4256., 4192.,  ..., 8240., 8248., 8248.],\n",
            "           [4300., 4268., 4200.,  ..., 8280., 8288., 8296.],\n",
            "           ...,\n",
            "           [5468., 5468., 5472.,  ..., 6100., 6116., 6108.],\n",
            "           [5488., 5484., 5468.,  ..., 6088., 6092., 6088.],\n",
            "           [5520., 5508., 5488.,  ..., 6076., 6076., 6076.]],\n",
            "\n",
            "          [[4644., 4624., 4560.,  ..., 8172., 8188., 8184.],\n",
            "           [4644., 4628., 4560.,  ..., 8232., 8240., 8240.],\n",
            "           [4680., 4640., 4568.,  ..., 8264., 8280., 8280.],\n",
            "           ...,\n",
            "           [5724., 5720., 5724.,  ..., 6260., 6276., 6268.],\n",
            "           [5748., 5744., 5720.,  ..., 6248., 6252., 6248.],\n",
            "           [5784., 5768., 5744.,  ..., 6236., 6236., 6236.]],\n",
            "\n",
            "          [[4864., 4848., 4776.,  ..., 7928., 7944., 7944.],\n",
            "           [4868., 4852., 4776.,  ..., 7984., 7996., 8000.],\n",
            "           [4904., 4864., 4784.,  ..., 8020., 8032., 8036.],\n",
            "           ...,\n",
            "           [5864., 5864., 5868.,  ..., 6320., 6336., 6328.],\n",
            "           [5888., 5884., 5864.,  ..., 6308., 6312., 6308.],\n",
            "           [5928., 5908., 5884.,  ..., 6296., 6296., 6296.]]],\n",
            "\n",
            "\n",
            "         [[[   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           ...,\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.]],\n",
            "\n",
            "          [[   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           ...,\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.]],\n",
            "\n",
            "          [[   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           ...,\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           ...,\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.]],\n",
            "\n",
            "          [[   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           ...,\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.]],\n",
            "\n",
            "          [[   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           ...,\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
            "           [   0.,    0.,    0.,  ...,    0.,    0.,    0.]]]]],\n",
            "       device='cuda:0')\n",
            "x_nontemporal1: tensor([[[[3.9200e+04, 3.8432e+04, 3.7632e+04,  ..., 3.8848e+04,\n",
            "           3.9648e+04, 4.0512e+04],\n",
            "          [3.8016e+04, 3.7216e+04, 3.6416e+04,  ..., 3.7728e+04,\n",
            "           3.8592e+04, 3.9456e+04],\n",
            "          [3.6896e+04, 3.6064e+04, 3.5232e+04,  ..., 3.6704e+04,\n",
            "           3.7600e+04, 3.8464e+04],\n",
            "          ...,\n",
            "          [3.8272e+04, 3.7728e+04, 3.7216e+04,  ..., 4.0864e+04,\n",
            "           4.1792e+04, 4.2720e+04],\n",
            "          [3.9968e+04, 3.9456e+04, 3.8944e+04,  ..., 4.1632e+04,\n",
            "           4.2560e+04, 4.3456e+04],\n",
            "          [4.1664e+04, 4.1184e+04, 4.0704e+04,  ..., 4.2496e+04,\n",
            "           4.3392e+04, 4.4288e+04]],\n",
            "\n",
            "         [[2.9370e-01, 2.9370e-01, 2.9370e-01,  ..., 1.0681e-01,\n",
            "           1.0681e-01, 1.0681e-01],\n",
            "          [2.6709e-01, 2.6709e-01, 2.6709e-01,  ..., 1.0681e-01,\n",
            "           1.0681e-01, 1.0681e-01],\n",
            "          [2.6709e-01, 2.6709e-01, 2.6709e-01,  ..., 1.0681e-01,\n",
            "           1.0681e-01, 1.0681e-01],\n",
            "          ...,\n",
            "          [2.6709e-01, 2.6709e-01, 2.6709e-01,  ..., 2.6709e-01,\n",
            "           2.6709e-01, 2.6709e-01],\n",
            "          [2.6709e-01, 2.6709e-01, 2.6709e-01,  ..., 2.6709e-01,\n",
            "           2.6709e-01, 2.6709e-01],\n",
            "          [2.6709e-01, 2.6709e-01, 2.6709e-01,  ..., 2.6709e-01,\n",
            "           2.6709e-01, 2.6709e-01]]]], device='cuda:0')\n",
            "x_occupied: tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
            "x_temporal2: tensor([[[[[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "         [[[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "         [[[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]]]]], device='cuda:0')\n",
            "x_temporal2: tensor([[[[[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "         [[[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "         [[[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
            "\n",
            "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           ...,\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan],\n",
            "           [nan, nan, nan,  ..., nan, nan, nan]]]]], device='cuda:0')\n",
            "x_temporal2: tensor([[nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')\n",
            "x_nontemporal2: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
            "x_nontemporal3: tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.6551, 0.6551, 0.6551,  ..., 0.6551, 0.6551, 0.6551],\n",
            "          [0.6551, 0.6551, 0.6551,  ..., 0.6551, 0.6551, 0.6551],\n",
            "          [0.6551, 0.6551, 0.6551,  ..., 0.6551, 0.6551, 0.6551],\n",
            "          ...,\n",
            "          [0.6551, 0.6551, 0.6544,  ..., 0.6551, 0.6551, 0.6551],\n",
            "          [0.6551, 0.6551, 0.6551,  ..., 0.6551, 0.6551, 0.6551],\n",
            "          [0.6551, 0.6551, 0.6551,  ..., 0.6551, 0.6551, 0.6551]]]],\n",
            "       device='cuda:0')\n",
            "x_nontemporal4: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.6551, 0.6551, 0.6551]],\n",
            "       device='cuda:0')\n",
            "x_occupied2: tensor([[0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0050, 0.0046]],\n",
            "       device='cuda:0')\n",
            "x_combined1: tensor([[   nan,    nan,    nan,  ..., 0.0000, 0.0050, 0.0046]],\n",
            "       device='cuda:0')\n",
            "x_combined2: tensor([[nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Expected parameter probs (Tensor of shape (1, 4849)) of distribution Categorical(probs: torch.Size([1, 4849])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 29\u001b[0m     action, logprob_action \u001b[38;5;241m=\u001b[39m \u001b[43mppo_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     next_state, reward, done, info \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Store data in PPO buffer\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[21], line 45\u001b[0m, in \u001b[0;36mPPO_discrete.select_action\u001b[1;34m(self, s, deterministic)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(pi)\n\u001b[1;32m---> 45\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     a \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39msample()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     49\u001b[0m     pi_a \u001b[38;5;241m=\u001b[39m pi[\u001b[38;5;241m0\u001b[39m,a]\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[1;32mc:\\Users\\karishma\\anaconda3\\envs\\green-grid-pr\\Lib\\site-packages\\torch\\distributions\\categorical.py:70\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     67\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[0;32m     69\u001b[0m )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\karishma\\anaconda3\\envs\\green-grid-pr\\Lib\\site-packages\\torch\\distributions\\distribution.py:68\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     66\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m---> 68\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m             )\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
            "\u001b[1;31mValueError\u001b[0m: Expected parameter probs (Tensor of shape (1, 4849)) of distribution Categorical(probs: torch.Size([1, 4849])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')"
          ]
        }
      ],
      "source": [
        "# state_gdf = gpd.read_parquet('/content/drive/MyDrive/Colab Notebooks/GreenGridPR/data/processed/state.parquet')\n",
        "state_gdf = gpd.read_parquet('../data/processed/state_1km_grid_cells.parquet')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "environment = RenewableEnergyEnvironment(state_gdf)\n",
        "state_space = environment.state_tensor.to(device)\n",
        "action_space_size = environment.action_space_size\n",
        "\n",
        "episodes = 16\n",
        "learning_rate = 0.000001 \n",
        "\n",
        "T_horizon_value = 2048 # number of interactions agent collects before updating its policy network\n",
        "\n",
        "# Example of initializing PPO agent\n",
        "ppo_agent = PPO_discrete(state_dim=environment.state_tensor.size(), \n",
        "                         action_dim=environment.action_space_size, \n",
        "                         net_width=128,\n",
        "                         device=device, \n",
        "                         lr = learning_rate,\n",
        "                         T_horizon=T_horizon_value)\n",
        "idx = 0\n",
        "for episode in range(episodes):\n",
        "    state = environment.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action, logprob_action = ppo_agent.select_action(state, deterministic=False)\n",
        "        next_state, reward, done, info = environment.step(action)\n",
        "\n",
        "        # Store data in PPO buffer\n",
        "        ppo_agent.put_data(state, action, reward, next_state, logprob_action, done, done, idx)\n",
        "        idx += 1\n",
        "\n",
        "        state = next_state\n",
        "    \n",
        "    # After each episode, train the PPO agent\n",
        "    ppo_agent.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60fca6ad",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
