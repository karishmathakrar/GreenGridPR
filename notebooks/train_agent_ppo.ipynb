{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "81222bf5-c8bd-4341-8569-71e7b8e0cd86",
      "metadata": {
        "id": "81222bf5-c8bd-4341-8569-71e7b8e0cd86"
      },
      "source": [
        "# Overview\n",
        "## State Space\n",
        "- The state space consists of an 80,000-cell grid, representing different geographical locations in Puerto Rico.\n",
        "- Each cell has attributes like solar PV output, wind power density, elevation, slope, cyclone risk score, building density, road density, and distance to transmission lines.\n",
        "- Approximately 70% of cells are unavailable for development due to environmental or other constraints.\n",
        "\n",
        "## Action Space\n",
        "- Two types of actions are available: building a solar array or a wind turbine.\n",
        "- Actions can be taken on any available cell.\n",
        "\n",
        "## Rewards and Costs\n",
        "The reward function should incorporate:\n",
        "- Energy production potential (solar and wind).\n",
        "- Costs or penalties associated with building on certain terrains (e.g., high elevation or steep slopes).\n",
        "- Penalties for building in high cyclone risk areas.\n",
        "- Incentives for maintaining a balance between solar and wind energy.\n",
        "- Incentives for early deployment and distributed grid development.\n",
        "- Penalties for high building or road density areas.\n",
        "- Distance to transmission lines.\n",
        "\n",
        "## RL Model\n",
        "- Model Choice: Given the size of the state space, a model-based RL algorithm (like Deep Q-Networks or Actor-Critic methods) is suitable.\n",
        "- Representation: The state representation should include the current status of each grid cell (whether it has a solar array, a wind turbine, or is vacant) along with its attributes.\n",
        "- Sequence of Actions: The RL agent will sequentially choose actions (where to build next) based on the current state of the grid.\n",
        "- Terminal State: The agent is done when the environment reaches a certain level of energy capacity or after a fixed number of steps.\n",
        "\n",
        "# Implementation Steps\n",
        "## Environment Setup:\n",
        "- Implement the environment to reflect the grid and its dynamics, including applying the binary mask for unavailable cells.\n",
        "- The step(action) method should update the grid state based on the chosen action and calculate the immediate reward or cost.\n",
        "\n",
        "## Agent Development:\n",
        "- Use PyTorch for implementing the neural network models for the agent.\n",
        "The agent needs to learn a policy that maximizes long-term rewards, considering the complex reward structure and large state space.\n",
        "\n",
        "## Training and Evaluation:\n",
        "- Set up a training loop where the agent interacts with the environment, receives feedback, and improves its policy.\n",
        "- Periodically evaluate the agent's performance, possibly using separate evaluation episodes or metrics like total energy capacity achieved or adherence to environmental constraints.\n",
        "\n",
        "## Hyperparameter Tuning:\n",
        "- Adjust learning rates, exploration rates, discount factors, and network architecture as needed to improve performance.\n",
        "\n",
        "## Scalability:\n",
        "Due to the large state space, may need to:\n",
        "- use function approximation for value functions\n",
        "- prioritizing important experiences in the replay buffer\n",
        "- parallelize computation process\n",
        "\n",
        "## Visualization and Analysis:\n",
        "- Develop tools to visualize the evolving grid layout and analyze the trade-offs made by the RL agent between different objectives (like energy maximization vs. environmental constraints)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "mrJv5LZDIu8s",
      "metadata": {
        "id": "mrJv5LZDIu8s"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import torch.optim as optim\n",
        "\n",
        "from scipy.spatial.distance import cdist, pdist, squareform\n",
        "\n",
        "import random\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "import copy\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "id": "uPsaoSerH3hW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPsaoSerH3hW",
        "outputId": "968646bc-0efe-482a-abf9-bff0a7fd4683"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c724dafa-ff4d-4fae-babe-283dcb25bb04",
      "metadata": {
        "id": "c724dafa-ff4d-4fae-babe-283dcb25bb04"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "id": "56ec6ed0",
      "metadata": {},
      "outputs": [],
      "source": [
        "class RenewableEnergyEnvironment:\n",
        "    def __init__(self, grid_gdf):\n",
        "        self.step_count = 0\n",
        "        \n",
        "\n",
        "        # Daily cost calculated in data_preprocessing notebook\n",
        "        # Multiplying by 4 because grid cell size was changed from 500m to 1 km\n",
        "        self.wind_daily_cost = 4 * 821.68\n",
        "        self.solar_daily_cost = 4 * 3007.26\n",
        "        \n",
        "        self.grid_columns = ['distance_to_transmission_line', \n",
        "                             'cyclone_risk',\n",
        "                             'occupied']\n",
        "        self.grid_columns += [f'demand_{i}' for i in range(1,25)]\n",
        "        self.grid_columns += [f'wind_power_kW_hour_{i}' for i in range(1,25)]\n",
        "        self.grid_columns += [f'solar_power_kW_hour_{i}' for i in range(1,25)]\n",
        "        \n",
        "        # Initialize the environment\n",
        "        self.starting_environment = grid_gdf.copy()\n",
        "        # Initialize 'occupied' column to zero\n",
        "        self.starting_environment['occupied'] = 0\n",
        "        self.state_gdf = self.starting_environment.copy()\n",
        "        self.state_gdf['installation_type'] = None\n",
        "        self.state_tensor = self.gdf_to_tensor(self.state_gdf)\n",
        "        self.mapping, self.action_space_size = self.create_action_to_gdf_mapping()\n",
        "        \n",
        "        self.total_energy_output = 0\n",
        "        self.stored_energy = 0\n",
        "\n",
        "        demand_df = pd.read_csv('../data/generation_and_demand/demand_profile.csv')\n",
        "        self.demand = demand_df['demand_MW'].to_numpy() * 1000\n",
        "        self.unmet_demand = self.demand.sum()\n",
        "        self.total_demand = self.unmet_demand\n",
        "\n",
        "        # LEGACY - the following parameters are no longer in use\n",
        "        self.decay_rate = 0.1 #TODO determine good decay rate\n",
        "        self.max_distance = 1000 #TODO get max distance between two cells\n",
        "        self.weights = {\n",
        "        'transmission_build_cost': -1.0,\n",
        "        'early_choice_reward': 1.0,\n",
        "        'distributed_grid_reward': 1.0,\n",
        "        }\n",
        "        \n",
        "    def reset(self):\n",
        "        # Reset the environment to the initial state\n",
        "        self.state_gdf = self.starting_environment.copy()\n",
        "        self.state_tensor = self.gdf_to_tensor(self.starting_environment)\n",
        "        self.total_energy_output = 0\n",
        "        self.step_count = 0\n",
        "        return self.state_tensor\n",
        "\n",
        "    def gdf_to_tensor(self, gdf):\n",
        "        # Calculate grid dimensions\n",
        "        x_start = 100000\n",
        "        x_end = 300000\n",
        "        y_start = 200000\n",
        "        y_end = 300000\n",
        "        square_size = 1000\n",
        "        \n",
        "        grid_width = int((x_end - x_start) / square_size)\n",
        "        grid_height = int((y_end - y_start) / square_size)\n",
        "        \n",
        "        # Flatten the grid data\n",
        "        flat_data = self.state_gdf[self.grid_columns].values.reshape(-1, grid_height, grid_width)\n",
        "        #flat_data = self.state_gdf[self.grid_columns].values.flatten()\n",
        "    \n",
        "        # Create a 4D Tensor (batch size is the 4th dimension)\n",
        "        tensor = torch.tensor(flat_data, dtype=torch.float32)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def create_action_to_gdf_mapping(self):\n",
        "        unmasked_gdf = self.state_gdf[self.state_gdf['masked'] == 0]\n",
        "        \n",
        "        mapping = {}\n",
        "        action_idx = 0  # Initialize action index\n",
        "    \n",
        "        for _, row in unmasked_gdf.iterrows():\n",
        "            # Check for valid solar action\n",
        "            if row['slope'] <= 8.749:  # Slope check for solar\n",
        "                mapping[action_idx] = (row.name, 'solar')\n",
        "                action_idx += 1\n",
        "    \n",
        "            # Check for valid wind action\n",
        "            if row['slope'] <= 26.795:  # Slope check for wind\n",
        "                mapping[action_idx] = (row.name, 'wind')\n",
        "                action_idx += 1\n",
        "        \n",
        "        action_space_size = action_idx  # Total number of valid actions\n",
        "        return mapping, action_space_size\n",
        "\n",
        "    def output_stats(self, writer, cell_index, action_type, reward, invalid=False):\n",
        "        if writer:\n",
        "            writer.writerow([None,None,None,cell_index, action_type, int(reward), int(self.total_energy_output.sum()), int(self.unmet_demand)])\n",
        "        if invalid:\n",
        "            print(f'Cell: {cell_index:<6d} | Step: INV  | Action: {action_type:<5} | Reward: {reward:8.5f} | Energy Output: {int(self.total_energy_output.sum()):<10} | Unmet Demand: {int(self.unmet_demand):<10}')\n",
        "        else:\n",
        "            print(f'Cell: {cell_index:<6d} | Step: {self.step_count:<4d} | Action: {action_type:<5} | Reward: {reward:8.5f} | Energy Output: {int(self.total_energy_output.sum()):<10} | Unmet Demand: {int(self.unmet_demand):<10}')\n",
        "\n",
        "    def step(self, action, writer=None):\n",
        "        # Apply the action to the environment and return the result\n",
        "        # action: Tuple (cell_index, action_type) where action_type could be 'solar' or 'wind'\n",
        "\n",
        "        # Map action from agent output to action in terms of state_gdf\n",
        "        cell_index, action_type = self.mapping[action]\n",
        "\n",
        "        # Check if the action is valid\n",
        "        if not self.is_valid_action(cell_index, action_type):\n",
        "            reward = -1  # Penalty for invalid action\n",
        "            done = self.is_terminal_state() # Check if in terminal state\n",
        "            self.total_energy_output = self.calculate_energy_output() # Calculate total energy output\n",
        "            self.output_stats(writer, cell_index, action_type, reward, invalid=True) # Print results and write results to file\n",
        "            return self.state_tensor, reward, done, {}\n",
        "\n",
        "        # Calculate the total reward before applying the action\n",
        "        total_reward_before_action = self.calculate_reward()\n",
        "    \n",
        "        # Apply the action\n",
        "        self.apply_action(cell_index, action_type)\n",
        "    \n",
        "        # Calculate the total reward after applying the action\n",
        "        total_reward_after_action = self.calculate_reward()\n",
        "    \n",
        "        # The reward for the action is the difference in total reward\n",
        "        reward = total_reward_after_action - total_reward_before_action\n",
        "        reward = reward / 100000\n",
        "\n",
        "        # Update total energy output or other state attributes as needed\n",
        "        self.total_energy_output = self.calculate_energy_output()\n",
        "        \n",
        "        # Check if the state is terminal\n",
        "        done = self.is_terminal_state()\n",
        "\n",
        "        # Print results and write results to file\n",
        "        self.output_stats(writer, cell_index, action_type, reward)\n",
        "        \n",
        "        # Update the state tensor with the new state gdf\n",
        "        self.state_tensor = self.gdf_to_tensor(self.state_gdf)\n",
        "\n",
        "        self.step_count += 1\n",
        "        \n",
        "        return self.state_tensor, reward, done, {}\n",
        "\n",
        "    def is_valid_action(self, cell_index, action_type):\n",
        "        # Implement logic to check if an action is valid\n",
        "        cell = self.state_gdf.iloc[cell_index]\n",
        "        if cell['masked']: # This should never occur\n",
        "            print('Error: Attempting to build on a masked cell')\n",
        "            return False\n",
        "        elif cell['occupied']:\n",
        "            return False\n",
        "        elif action_type == 'solar' and cell['slope'] > 8.749: # This should never occur\n",
        "            print('Error: Attempting to build solar on slope of more than 5%')\n",
        "            return False\n",
        "        elif action_type == 'wind' and cell['slope'] > 26.795: # This should never occur\n",
        "            print('Error: Attempting to build wind on slope of more than 15%')\n",
        "            return False\n",
        "        \n",
        "        return True\n",
        "\n",
        "    def apply_action(self, cell_index, action_type):\n",
        "        # Implement the changes to the environment based on the action\n",
        "        # Example: Mark the cell as occupied and record the type of installation\n",
        "        self.state_gdf.at[cell_index, 'occupied'] = 1\n",
        "        self.state_gdf.at[cell_index, 'installation_type'] = action_type\n",
        "\n",
        "    def calculate_reward(self):\n",
        "        # Solar installation cost\n",
        "        solar_cost = self.calculate_solar_cost()\n",
        "\n",
        "        # Wind turbine installation cost\n",
        "        wind_cost = self.calculate_wind_cost()\n",
        "        \n",
        "        # Solar power Reward\n",
        "        power_output_reward = self.calculate_power_output_reward()\n",
        "\n",
        "        # Transmission build cost\n",
        "        transmission_build_cost = self.calculate_transmission_build_cost()\n",
        "        \n",
        "        # Cyclone risk cost\n",
        "        cyclone_risk_cost = self.calculate_cyclone_risk_cost()\n",
        "    \n",
        "        # Distribution reward\n",
        "        # distributed_grid_reward = self.calculate_distributed_grid_reward()\n",
        "    \n",
        "        # Early choice reward\n",
        "        # early_choice_reward = self.time_dependent_reward_factor()\n",
        "\n",
        "        total_reward = power_output_reward + solar_cost + wind_cost + cyclone_risk_cost + transmission_build_cost\n",
        "        \n",
        "        return total_reward\n",
        "\n",
        "    def calculate_energy_output(self):\n",
        "        # Filter for solar and wind installations\n",
        "        solar_gdf = self.state_gdf[self.state_gdf['installation_type'] == 'solar']\n",
        "        wind_gdf = self.state_gdf[self.state_gdf['installation_type'] == 'wind']\n",
        "    \n",
        "        # Prepare column names for solar and wind power\n",
        "        solar_power_columns = [f'solar_power_kW_hour_{i}' for i in range(1, 25)]\n",
        "        wind_power_columns = [f'wind_power_kW_hour_{i}' for i in range(1, 25)]\n",
        "    \n",
        "        # Vectorized sum of power output for solar and wind for each hour\n",
        "        total_solar_power = solar_gdf[solar_power_columns].sum().to_numpy()\n",
        "        total_wind_power = wind_gdf[wind_power_columns].sum().to_numpy()\n",
        "    \n",
        "        total_power = total_solar_power + total_wind_power\n",
        "        \n",
        "        return total_power\n",
        "\n",
        "    def calculate_power_output_reward(self):\n",
        "        \"\"\"Uses supply and demand curve to determine the amount of demand satisfied by\n",
        "        the solar and wind installations\"\"\"\n",
        "        cost_kWh = .22  # TODO get more rigorous number \n",
        "        # Storage cost = $400/kWh over the 15 year lifetime of a 4 hour battery\n",
        "        # cost_storage_kWh = 400 / (365.25 * 15)\n",
        "        demand = self.demand\n",
        "\n",
        "        # Calculate the reward using vectorized minimum\n",
        "        total_power = self.calculate_energy_output()\n",
        "        demand_satisfied = np.minimum(demand, total_power)\n",
        "        demand_satisfied_ratio = demand_satisfied / demand\n",
        "\n",
        "        self.update_demand(demand_satisfied_ratio)\n",
        "        \n",
        "        power_reward = demand_satisfied.sum() * cost_kWh\n",
        "        # self.stored_energy = np.maximum(total_power - demand, 0).sum()\n",
        "        # storage_cost = self.stored_energy * cost_storage_kWh\n",
        "\n",
        "        reward = power_reward # - storage_cost\n",
        "        \n",
        "        return reward\n",
        "\n",
        "    def update_demand(self, demand_satisfied_ratio):\n",
        "        unsatisfied_demand_ratio = 1 - demand_satisfied_ratio\n",
        "        for i in range(1, 25):\n",
        "            self.state_gdf[f'demand_{i}'] *= unsatisfied_demand_ratio[i-1]\n",
        "    \n",
        "    def calculate_solar_cost(self):\n",
        "        \"\"\"Cost of installing solar array on cell. Based on elevation and slope\"\"\"\n",
        "        # print(len(self.state_gdf[self.state_gdf['installation_type'] == 'solar']))\n",
        "        return -self.solar_daily_cost * len(self.state_gdf[self.state_gdf['installation_type'] == 'solar'])\n",
        "\n",
        "    def calculate_wind_cost(self):\n",
        "        \"\"\"Cost of installing wind turbine on cell. Based on elevation and slope\"\"\"\n",
        "        return -self.wind_daily_cost * len(self.state_gdf[self.state_gdf['installation_type'] == 'wind'])\n",
        "\n",
        "    def transmission_line_cost_per_km(self, distance):\n",
        "        # $2.29 million per mile divided by 1.60934 to get into km, \n",
        "        # then divided by 25 * 365.25 for daily costs with 25 year decommission time\n",
        "        distance /= 1000 # Convert to km\n",
        "        KM_PER_MILE = 1.60934\n",
        "        COST_PER_KM = 2.29 * 1000000 / (KM_PER_MILE * 25 * 365.25) \n",
        "        SHORT_DISTANCE_THRESHOLD = 3 * KM_PER_MILE # Threshold for short distance \n",
        "        MEDIUM_DISTANCE_THRESHOLD = 10 * KM_PER_MILE # Threshold for medium distance \n",
        "        \n",
        "        if distance < SHORT_DISTANCE_THRESHOLD:\n",
        "            cost_modifier = 1.5  # 50% increase for less than 3 miles\n",
        "        elif distance < MEDIUM_DISTANCE_THRESHOLD:\n",
        "            cost_modifier = 1.2  # 20% increase for 3-10 miles\n",
        "        else:\n",
        "            cost_modifier = 1  # No modification for more than 10 miles\n",
        "        return -distance * COST_PER_KM * cost_modifier\n",
        "    \n",
        "    def calculate_transmission_build_cost(self):\n",
        "        occupied_cells = self.state_gdf[self.state_gdf['occupied'] == 1]\n",
        "        \n",
        "        # Check if there is only one occupied cell\n",
        "        if len(occupied_cells) == 0:\n",
        "            return 0\n",
        "        elif len(occupied_cells) == 1:\n",
        "            # For a single occupied cell, use the distance to transmission line for cost calculation\n",
        "            occupied_cell = occupied_cells.iloc[0]\n",
        "            distance_km = occupied_cell['distance_to_transmission_line']\n",
        "            build_cost = self.transmission_line_cost_per_km(distance_km)\n",
        "        else:\n",
        "            # Get coordinates of occupied cells\n",
        "            coords = np.array(list(zip(occupied_cells.geometry.centroid.x, occupied_cells.geometry.centroid.y)))\n",
        "    \n",
        "            # Calculate pairwise distances between occupied cells\n",
        "            distances = cdist(coords, coords)\n",
        "    \n",
        "            # Replace zeros in distance matrix with np.inf to avoid zero distance to itself\n",
        "            np.fill_diagonal(distances, np.inf)\n",
        "    \n",
        "            # Find the nearest installation for each installation\n",
        "            nearest_installation_distances = np.min(distances, axis=1)\n",
        "    \n",
        "            # Determine the relevant distance for cost calculation\n",
        "            relevant_distances = np.minimum(nearest_installation_distances, occupied_cells['distance_to_transmission_line'].to_numpy())\n",
        "    \n",
        "            # Calculate build cost\n",
        "            build_costs = [self.transmission_line_cost_per_km(distance) for distance in relevant_distances]\n",
        "            build_cost = sum(build_costs)\n",
        "            \n",
        "        return build_cost\n",
        "\n",
        "    def calculate_cyclone_risk_cost(self):\n",
        "        if len(self.state_gdf[self.state_gdf.installation_type == 'wind']) == 0:\n",
        "            return 0\n",
        "        # Wind operational expenses = $40/kW/yr\n",
        "        # Wind turbine capacity is 3 MW\n",
        "        # To get daily cost, multiply by 3000 (3 MW = 3000 kW) and divide by 365.25 days in a year\n",
        "        wind_opex = 4 * 40 * 3000 / 365.25\n",
        "        # Wind capital expenses = $1501/kW\n",
        "        # To get daily cost, multiply by 3000 (3 MW = 3000 kW) and divide by (25 years of lifetime * 365.25 days in a year)\n",
        "        wind_capex = 4 * 1501 * 3000 / (25 * 365.25)\n",
        "        cyclone_risk_cost = -(self.state_gdf[self.state_gdf.installation_type == 'wind']['cyclone_risk'] * (wind_capex - wind_opex)).sum()\n",
        "        return cyclone_risk_cost\n",
        "    \n",
        "    def calculate_distributed_grid_reward(self):\n",
        "        # Extract the coordinates of the occupied cells (where installations are located)\n",
        "        occupied_cells = self.state_gdf[self.state_gdf['occupied'] == 1]\n",
        "        if len(occupied_cells) < 2:\n",
        "            # If there are less than two installations, we cannot calculate distances\n",
        "            return 1\n",
        "    \n",
        "        coords = np.array(list(zip(occupied_cells.geometry.x, occupied_cells.geometry.y)))\n",
        "    \n",
        "        # Calculate pairwise distances between all occupied cells\n",
        "        distances = pdist(coords)\n",
        "    \n",
        "        # Calculate the average distance. The larger this is, the more distributed the installations are.\n",
        "        avg_distance = np.mean(distances)\n",
        "    \n",
        "        # Normalize the reward such that it ranges between 0 and 1\n",
        "        normalized_reward = avg_distance / self.max_distance\n",
        "    \n",
        "        return normalized_reward\n",
        "    \n",
        "    def time_dependent_reward_factor(self):\n",
        "        # A function to calculate the time-dependent reward factor\n",
        "        # It decreases with each year from the base year\n",
        "        action_number = self.state_gdf.occupied.sum()\n",
        "        return 1 / (1 + self.decay_rate * action_number)\n",
        "        \n",
        "    def is_terminal_state(self):\n",
        "        # The episode ends when there is no unmet demand\n",
        "        self.unmet_demand = np.maximum(self.demand - self.total_energy_output, 0).sum()\n",
        "        return self.unmet_demand <= 0 or self.total_energy_output.sum() >= self.total_demand \n",
        "\n",
        "    def render(self):\n",
        "        # Optional: Implement a method to visualize the current state of the environment\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "id": "ac168609-d297-4963-b880-bcfdc86c2f6e",
      "metadata": {
        "id": "ac168609-d297-4963-b880-bcfdc86c2f6e"
      },
      "outputs": [],
      "source": [
        "# class RenewableEnergyEnvironment:\n",
        "#     def __init__(self, grid_gdf):\n",
        "#         self.weights = {\n",
        "#         'transmission_build_cost': -1.0,\n",
        "#         'early_choice_reward': 1.0,\n",
        "#         'distributed_grid_reward': 1.0,\n",
        "#         }\n",
        "\n",
        "#         self.wind_daily_cost = 821.68\n",
        "#         self.solar_daily_cost = 3007.26\n",
        "\n",
        "#         self.grid_columns = ['slope',\n",
        "#                              'distance_to_transmission_line',\n",
        "#                              'cyclone_risk',\n",
        "#                              'water',\n",
        "#                              'occupied_solar',\n",
        "#                              'occupied_wind',\n",
        "#                              'masked']\n",
        "#         self.grid_columns += [f'demand_{i}' for i in range(1,25)]\n",
        "#         self.grid_columns += [f'wind_power_kW_hour_{i}' for i in range(1,25)]\n",
        "#         self.grid_columns += [f'solar_power_kW_hour_{i}' for i in range(1,25)]\n",
        "\n",
        "#         # Initialize the environment\n",
        "#         self.starting_environment = grid_gdf\n",
        "#         self.state_gdf = grid_gdf.copy()\n",
        "#         self.state_tensor = self.gdf_to_tensor(self.state_gdf)\n",
        "#         self.mapping, self.action_space_size = self.create_action_to_gdf_mapping()\n",
        "\n",
        "#         self.total_energy_output = 0\n",
        "#         self.stored_energy = 0\n",
        "\n",
        "#         demand_df = pd.read_csv('../data/generation_and_demand/demand_profile.csv')\n",
        "#         # demand_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/GreenGridPR/data/generation_and_demand/demand_profile.csv')\n",
        "#         self.demand = demand_df['demand_MW'].to_numpy() * 1000\n",
        "#         self.unmet_demand = self.demand.sum()\n",
        "#         self.total_demand = self.unmet_demand\n",
        "\n",
        "#         self.decay_rate = 0.1 #TODO determine good decay rate\n",
        "#         self.max_distance = 1000 #TODO get max distance between two cells\n",
        "\n",
        "#     def reset(self):\n",
        "#         # Reset the environment to the initial state\n",
        "#         self.state_gdf = self.starting_environment.copy()\n",
        "#         self.state_tensor = self.gdf_to_tensor(self.starting_environment)\n",
        "#         self.total_energy_output = 0\n",
        "#         return self.state_tensor\n",
        "\n",
        "#     def gdf_to_tensor(self, gdf):\n",
        "#         # Calculate grid dimensions\n",
        "#         x_start = 100000\n",
        "#         x_end = 300000\n",
        "#         y_start = 200000\n",
        "#         y_end = 300000\n",
        "#         square_size = 500\n",
        "\n",
        "#         grid_width = int((x_end - x_start) / square_size)\n",
        "#         grid_height = int((y_end - y_start) / square_size)\n",
        "\n",
        "#         # Flatten the grid data\n",
        "#         flat_data = self.state_gdf[self.grid_columns].values.reshape(-1, grid_height, grid_width)\n",
        "\n",
        "#         # Create a 4D Tensor (batch size is the 4th dimension)\n",
        "#         tensor = torch.tensor(flat_data, dtype=torch.float32)\n",
        "\n",
        "#         return tensor\n",
        "\n",
        "#     def create_action_to_gdf_mapping(self):\n",
        "#         unmasked_gdf = self.state_gdf[self.state_gdf['masked'] == 0]\n",
        "\n",
        "#         mapping = {}\n",
        "#         action_idx = 0  # Initialize action index\n",
        "\n",
        "#         for _, row in unmasked_gdf.iterrows():\n",
        "#             # Check for valid solar action\n",
        "#             if row['slope'] <= 8.749:  # Slope check for solar\n",
        "#                 mapping[action_idx] = (row.name, 'solar')\n",
        "#                 action_idx += 1\n",
        "\n",
        "#             # Check for valid wind action\n",
        "#             if row['slope'] <= 26.795:  # Slope check for wind\n",
        "#                 mapping[action_idx] = (row.name, 'wind')\n",
        "#                 action_idx += 1\n",
        "\n",
        "#         action_space_size = action_idx  # Total number of valid actions\n",
        "#         return mapping, action_space_size\n",
        "\n",
        "\n",
        "#     def step(self, action):\n",
        "#         # Apply the action to the environment and return the result\n",
        "#         # action: Tuple (cell_index, action_type) where action_type could be 'solar' or 'wind'\n",
        "\n",
        "#         cell_index, action_type = self.mapping[action]\n",
        "\n",
        "#         # Check if the action is valid\n",
        "#         if not self.is_valid_action(cell_index, action_type):\n",
        "#             reward = -1  # Penalty for invalid action\n",
        "#             done = self.is_terminal_state()\n",
        "#             return self.state_tensor, reward, done, {}\n",
        "\n",
        "#         # Calculate the total reward before applying the action\n",
        "#         total_reward_before_action = self.calculate_reward()\n",
        "\n",
        "#         # Apply the action\n",
        "#         self.apply_action(cell_index, action_type)\n",
        "\n",
        "#         # Calculate the total reward after applying the action\n",
        "#         total_reward_after_action = self.calculate_reward()\n",
        "\n",
        "#         # The reward for the action is the difference in total reward\n",
        "#         reward = total_reward_after_action - total_reward_before_action\n",
        "\n",
        "#         # Update total energy output or other state attributes as needed\n",
        "#         self.total_energy_output = self.calculate_energy_output()\n",
        "\n",
        "#         # Check if the state is terminal\n",
        "#         done = self.is_terminal_state()\n",
        "\n",
        "#         print(f'Cell: {cell_index:<6d} | Action: {action_type:<5} | Reward: {int(reward):<7} | Energy Output: {int(self.total_energy_output.sum()):<10} | Unmet Demand: {int(self.unmet_demand):<10}')\n",
        "\n",
        "#         # Update the state tensor with the new state gdf\n",
        "#         self.state_tensor = self.gdf_to_tensor(self.state_gdf)\n",
        "\n",
        "#         return self.state_tensor, reward, done, {}\n",
        "\n",
        "#     def is_valid_action(self, cell_index, action_type):\n",
        "#         # Implement logic to check if an action is valid\n",
        "#         # Example: Check if the cell is not masked and not already occupied\n",
        "#         cell = self.state_gdf.iloc[cell_index]\n",
        "#         if cell['masked']: # This should never occur\n",
        "#             print('Something has gone wrong. Attempting to build on a masked cell')\n",
        "#             return False\n",
        "#         elif cell['occupied_solar']:\n",
        "#             return False\n",
        "#         elif cell['occupied_wind']:\n",
        "#             return False\n",
        "#         elif action_type == 'solar' and cell['slope'] > 8.749:\n",
        "#             return False\n",
        "#         elif action_type == 'wind' and cell['slope'] > 26.795:\n",
        "#             return False\n",
        "\n",
        "#         return True\n",
        "\n",
        "#     def apply_action(self, cell_index, action_type):\n",
        "#         # Implement the changes to the environment based on the action\n",
        "#         # Example: Mark the cell as occupied and record the type of installation\n",
        "#         if action_type == 'solar':\n",
        "#             self.state_gdf.at[cell_index, 'occupied_solar'] = 1\n",
        "#         elif action_type == 'wind':\n",
        "#             self.state_gdf.at[cell_index, 'occupied_wind'] = 1\n",
        "\n",
        "#     def calculate_reward(self):\n",
        "#         # Solar installation cost\n",
        "#         solar_cost = self.calculate_solar_cost()\n",
        "\n",
        "#         # Wind turbine installation cost\n",
        "#         wind_cost = self.calculate_wind_cost()\n",
        "\n",
        "#         # Solar power Reward\n",
        "#         power_output_reward = self.calculate_power_output_reward()\n",
        "\n",
        "#         # Transmission build cost\n",
        "#         transmission_build_cost = self.calculate_transmission_build_cost()\n",
        "\n",
        "#         # Cyclone risk cost\n",
        "#         cyclone_risk_cost = self.calculate_cyclone_risk_cost()\n",
        "\n",
        "#         # Distribution reward\n",
        "#         # distributed_grid_reward = self.calculate_distributed_grid_reward()\n",
        "\n",
        "#         # Early choice reward\n",
        "#         # early_choice_reward = self.time_dependent_reward_factor()\n",
        "\n",
        "#         total_reward = power_output_reward + solar_cost + wind_cost + cyclone_risk_cost + transmission_build_cost\n",
        "\n",
        "#         return total_reward\n",
        "\n",
        "#     def calculate_energy_output(self):\n",
        "#         # Filter for solar and wind installations\n",
        "#         solar_gdf = self.state_gdf[self.state_gdf['occupied_solar'] == 1]\n",
        "#         wind_gdf = self.state_gdf[self.state_gdf['occupied_wind'] == 1]\n",
        "\n",
        "#         # Prepare column names for solar and wind power\n",
        "#         solar_power_columns = [f'solar_power_kW_hour_{i}' for i in range(1, 25)]\n",
        "#         wind_power_columns = [f'wind_power_kW_hour_{i}' for i in range(1, 25)]\n",
        "\n",
        "#         # Vectorized sum of power output for solar and wind for each hour\n",
        "#         total_solar_power = solar_gdf[solar_power_columns].sum().to_numpy()\n",
        "#         total_wind_power = wind_gdf[wind_power_columns].sum().to_numpy()\n",
        "\n",
        "#         total_power = total_solar_power + total_wind_power\n",
        "\n",
        "#         return total_power\n",
        "\n",
        "#     def calculate_power_output_reward(self):\n",
        "#         \"\"\"Uses supply and demand curve to determine the amount of demand satisfied by\n",
        "#         the solar and wind installations\"\"\"\n",
        "#         cost_kWh = .22  # TODO\n",
        "#         # Storage cost = $400/kWh over the 15 year lifetime of a 4 hour battery\n",
        "#         # cost_storage_kWh = 400 / (365.25 * 15)\n",
        "#         demand = self.demand\n",
        "\n",
        "#         # Calculate the reward using vectorized minimum\n",
        "#         total_power = self.calculate_energy_output()\n",
        "#         demand_satisfied = np.minimum(demand, total_power)\n",
        "#         demand_satisfied_ratio = demand_satisfied / demand\n",
        "\n",
        "#         self.update_demand(demand_satisfied_ratio)\n",
        "\n",
        "#         power_reward = demand_satisfied.sum() * cost_kWh\n",
        "#         # self.stored_energy = np.maximum(total_power - demand, 0).sum()\n",
        "#         # storage_cost = self.stored_energy * cost_storage_kWh\n",
        "\n",
        "#         reward = power_reward # - storage_cost\n",
        "\n",
        "#         return reward\n",
        "\n",
        "#     def update_demand(self, demand_satisfied_ratio):\n",
        "#         unsatisfied_demand_ratio = 1 - demand_satisfied_ratio\n",
        "#         for i in range(1, 25):\n",
        "#             self.state_gdf[f'demand_{i}'] *= unsatisfied_demand_ratio[i-1]\n",
        "\n",
        "#     def calculate_solar_cost(self):\n",
        "#         \"\"\"Cost of installing solar array on cell. Based on elevation and slope\"\"\"\n",
        "#         return -self.solar_daily_cost * len(self.state_gdf[self.state_gdf['occupied_solar'] == 1])\n",
        "\n",
        "#     def calculate_wind_cost(self):\n",
        "#         \"\"\"Cost of installing wind turbine on cell. Based on elevation and slope\"\"\"\n",
        "#         return -self.wind_daily_cost * len(self.state_gdf[self.state_gdf['occupied_wind'] == 1])\n",
        "\n",
        "#     def transmission_line_cost_per_km(self, distance):\n",
        "#         # $2.29 million per mile divided by 1.60934 to get into km,\n",
        "#         # then divided by 25 * 365.25 for daily costs with 25 year decommission time\n",
        "#         distance /= 1000 # Convert to km\n",
        "#         KM_PER_MILE = 1.60934\n",
        "#         COST_PER_KM = 2.29 * 1000000 / (KM_PER_MILE * 25 * 365.25)\n",
        "#         SHORT_DISTANCE_THRESHOLD = 3 * KM_PER_MILE # Threshold for short distance\n",
        "#         MEDIUM_DISTANCE_THRESHOLD = 10 * KM_PER_MILE # Threshold for medium distance\n",
        "\n",
        "#         if distance < SHORT_DISTANCE_THRESHOLD:\n",
        "#             cost_modifier = 1.5  # 50% increase for less than 3 miles\n",
        "#         elif distance < MEDIUM_DISTANCE_THRESHOLD:\n",
        "#             cost_modifier = 1.2  # 20% increase for 3-10 miles\n",
        "#         else:\n",
        "#             cost_modifier = 1  # No modification for more than 10 miles\n",
        "#         return -distance * COST_PER_KM * cost_modifier\n",
        "\n",
        "#     def calculate_transmission_build_cost(self):\n",
        "#         occupied_cells = self.state_gdf[(self.state_gdf['occupied_solar'] == 1) | (self.state_gdf['occupied_wind'] == 1)]\n",
        "\n",
        "#         # Check if there is only one occupied cell\n",
        "#         if len(occupied_cells) == 0:\n",
        "#             return 0\n",
        "#         elif len(occupied_cells) == 1:\n",
        "#             # For a single occupied cell, use the distance to transmission line for cost calculation\n",
        "#             occupied_cell = occupied_cells.iloc[0]\n",
        "#             distance_km = occupied_cell['distance_to_transmission_line']\n",
        "#             build_cost = self.transmission_line_cost_per_km(distance_km)\n",
        "#         else:\n",
        "#             # Get coordinates of occupied cells\n",
        "#             coords = np.array(list(zip(occupied_cells.geometry.centroid.x, occupied_cells.geometry.centroid.y)))\n",
        "\n",
        "#             # Calculate pairwise distances between occupied cells\n",
        "#             distances = cdist(coords, coords)\n",
        "\n",
        "#             # Replace zeros in distance matrix with np.inf to avoid zero distance to itself\n",
        "#             np.fill_diagonal(distances, np.inf)\n",
        "\n",
        "#             # Find the nearest installation for each installation\n",
        "#             nearest_installation_distances = np.min(distances, axis=1)\n",
        "\n",
        "#             # Determine the relevant distance for cost calculation\n",
        "#             relevant_distances = np.minimum(nearest_installation_distances, occupied_cells['distance_to_transmission_line'].to_numpy())\n",
        "\n",
        "#             # Calculate build cost\n",
        "#             build_costs = [self.transmission_line_cost_per_km(distance) for distance in relevant_distances]\n",
        "#             build_cost = sum(build_costs)\n",
        "\n",
        "#         return build_cost\n",
        "\n",
        "#     def calculate_cyclone_risk_cost(self):\n",
        "#         if len(self.state_gdf[self.state_gdf.occupied_wind == 1]) == 0:\n",
        "#             return 0\n",
        "#         # Wind operational expenses = $40/kW/yr\n",
        "#         # Wind turbine capacity is 3 MW\n",
        "#         # To get daily cost, multiply by 3000 (3 MW = 3000 kW) and divide by 365.25 days in a year\n",
        "#         wind_opex = 40 * 3000 / 365.25\n",
        "#         # Wind capital expenses = $1501/kW\n",
        "#         # To get daily cost, multiply by 3000 (3 MW = 3000 kW) and divide by (25 years of lifetime * 365.25 days in a year)\n",
        "#         wind_capex = 1501 * 3000 / (25 * 365.25)\n",
        "#         cyclone_risk_cost = -(self.state_gdf[self.state_gdf.occupied_wind == 1]['cyclone_risk'] * (wind_capex - wind_opex)).sum()\n",
        "#         return cyclone_risk_cost\n",
        "\n",
        "#     def calculate_distributed_grid_reward(self):\n",
        "#         # Extract the coordinates of the occupied cells (where installations are located)\n",
        "#         occupied_cells = self.state_gdf[(self.state_gdf['occupied_solar'] == 1) | (self.state_gdf['occupied_wind'] == 1)]\n",
        "#         if len(occupied_cells) < 2:\n",
        "#             # If there are less than two installations, we cannot calculate distances\n",
        "#             return 1\n",
        "\n",
        "#         coords = np.array(list(zip(occupied_cells.geometry.x, occupied_cells.geometry.y)))\n",
        "\n",
        "#         # Calculate pairwise distances between all occupied cells\n",
        "#         distances = pdist(coords)\n",
        "\n",
        "#         # Calculate the average distance. The larger this is, the more distributed the installations are.\n",
        "#         avg_distance = np.mean(distances)\n",
        "\n",
        "#         # Normalize the reward such that it ranges between 0 and 1\n",
        "#         normalized_reward = avg_distance / self.max_distance\n",
        "\n",
        "#         return normalized_reward\n",
        "\n",
        "#     def time_dependent_reward_factor(self):\n",
        "#         # A function to calculate the time-dependent reward factor\n",
        "#         # It decreases with each year from the base year\n",
        "#         action_number = self.state_gdf.occupied_solar.sum() + self.state_gdf.occupied_wind.sum()\n",
        "#         return 1 / (1 + self.decay_rate * action_number)\n",
        "\n",
        "#     def is_terminal_state(self):\n",
        "#         # The episode ends when there is no unmet demand\n",
        "#         self.unmet_demand = np.maximum(self.demand - self.total_energy_output, 0).sum()\n",
        "#         # self.unmet_demand -= self.stored_energy\n",
        "#         return self.unmet_demand <= 0 or self.total_energy_output.sum() >= 2 * self.total_demand\n",
        "\n",
        "#     def render(self):\n",
        "#         # Optional: Implement a method to visualize the current state of the environment\n",
        "#         pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i4ouI5o064pg",
      "metadata": {
        "id": "i4ouI5o064pg"
      },
      "source": [
        "NN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "id": "QyS8SzYIHrcK",
      "metadata": {
        "id": "QyS8SzYIHrcK"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, net_width):\n",
        "        super(Actor, self).__init__()\n",
        "        \n",
        "\n",
        "        self.conv1 = nn.Conv2d(state_dim, 128, kernel_size=3, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(128)\n",
        "        self.conv2 = nn.Conv2d(128, 128, kernel_size=3, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.input_shape = state_dim  # Store input_shape for feature size calculation\n",
        "        print(f'Input shape: {state_dim}, n Actions: {action_dim}')\n",
        "        self.fc1 = nn.Linear(33792, 1024)\n",
        "        self.fc2 = nn.Linear(1024, action_dim)\n",
        "\n",
        "    def pi(self, state, softmax_dim = 0):\n",
        "        print('start : ',state.size())\n",
        "\n",
        "        x =  state.view(-1, 75, 200, 100)\n",
        "\n",
        "        print( f'x shape: {x.size()}')\n",
        "\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        print( f'x shape: {x.size()}')\n",
        "\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        print( f'x shape: {x.size()}')\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        print( f'x shape: {x.size()}')\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        print( f'x shape: {x.size()}')\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        print( f'x shape: {x.size()}')\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        print( f'x shape: {x.size()}')\n",
        "\n",
        "        prob = F.softmax(x,dim=softmax_dim)\n",
        "        return prob\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim,net_width):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.C1 = nn.Linear(state_dim, net_width)\n",
        "        self.C2 = nn.Linear(net_width, net_width)\n",
        "        self.C3 = nn.Linear(net_width, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        v = torch.relu(self.C1(state))\n",
        "        v = torch.relu(self.C2(v))\n",
        "        v = self.C3(v)\n",
        "        return v\n",
        "\n",
        "def evaluate_policy(env, agent, turns = 3):\n",
        "    total_scores = 0\n",
        "    for j in range(turns):\n",
        "        s, info = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Take deterministic actions at test time\n",
        "            a, logprob_a = agent.select_action(s, deterministic=True)\n",
        "            s_next, r, dw, tr, info = env.step(a)\n",
        "            done = (dw or tr)\n",
        "\n",
        "            total_scores += r\n",
        "            s = s_next\n",
        "    return int(total_scores/turns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "id": "7ee4cd1d-e7a7-4dc0-afba-26e344cc17d1",
      "metadata": {
        "id": "7ee4cd1d-e7a7-4dc0-afba-26e344cc17d1"
      },
      "outputs": [],
      "source": [
        "class PPO_discrete():\n",
        "    def __init__(self, **kwargs):\n",
        "        required_params = ['state_dim', 'action_dim', 'net_width', 'lr', 'device']\n",
        "        for param in required_params:\n",
        "            if param not in kwargs:\n",
        "                raise ValueError(f\"Parameter '{param}' must be specified\")\n",
        "\n",
        "        # Set attributes from kwargs\n",
        "        self.state_dim = kwargs['state_dim']\n",
        "        self.action_dim = kwargs['action_dim']\n",
        "        self.net_width = kwargs['net_width']\n",
        "        self.lr = kwargs['lr']\n",
        "        self.dvc = kwargs['device']\n",
        "        self.T_horizon = kwargs['T_horizon']\n",
        "\n",
        "        print('state dim :  ',self.state_dim)\n",
        "        print('action dim :  ',self.action_dim)\n",
        "        print('net width :  ',self.net_width)\n",
        "\n",
        "        '''Build Actor and Critic'''\n",
        "        self.actor = Actor(self.state_dim, self.action_dim, self.net_width).to(self.dvc)\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.lr)\n",
        "        self.critic = Critic(self.state_dim, self.net_width).to(self.dvc)\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.lr)\n",
        "\n",
        "        '''Build Trajectory holder'''\n",
        "        self.s_hoder = np.zeros((self.T_horizon, 75, 200, 100), dtype=np.float32)\n",
        "        self.a_hoder = np.zeros((self.T_horizon, 1), dtype=np.int64)\n",
        "        self.r_hoder = np.zeros((self.T_horizon, 1), dtype=np.float32)\n",
        "        self.s_next_hoder = np.zeros((self.T_horizon, 75, 200, 100), dtype=np.float32)\n",
        "        self.logprob_a_hoder = np.zeros((self.T_horizon, 1), dtype=np.float32)\n",
        "        self.done_hoder = np.zeros((self.T_horizon, 1), dtype=np.bool_)\n",
        "        self.dw_hoder = np.zeros((self.T_horizon, 1), dtype=np.bool_)\n",
        "\n",
        "    def select_action(self, s, deterministic):\n",
        "        s = s.float().to(self.dvc)\n",
        "        with torch.no_grad():\n",
        "            pi = self.actor.pi(s, softmax_dim=1)\n",
        "            if deterministic:\n",
        "                a = torch.argmax(pi, dim=1).item()\n",
        "                return a, None\n",
        "            else:\n",
        "                m = Categorical(pi)\n",
        "                a = m.sample().item()\n",
        "\n",
        "\n",
        "                pi_a = pi[0,a].item()\n",
        "                return a, pi_a\n",
        "\n",
        "    def train(self):\n",
        "        self.entropy_coef *= self.entropy_coef_decay #exploring decay\n",
        "        '''Prepare PyTorch data from Numpy data'''\n",
        "        s = torch.from_numpy(self.s_hoder).to(self.dvc)\n",
        "        a = torch.from_numpy(self.a_hoder).to(self.dvc)\n",
        "        r = torch.from_numpy(self.r_hoder).to(self.dvc)\n",
        "        s_next = torch.from_numpy(self.s_next_hoder).to(self.dvc)\n",
        "        old_prob_a = torch.from_numpy(self.logprob_a_hoder).to(self.dvc)\n",
        "        done = torch.from_numpy(self.done_hoder).to(self.dvc)\n",
        "        dw = torch.from_numpy(self.dw_hoder).to(self.dvc)\n",
        "\n",
        "        ''' Use TD+GAE+LongTrajectory to compute Advantage and TD target'''\n",
        "        with torch.no_grad():\n",
        "            vs = self.critic(s)\n",
        "            vs_ = self.critic(s_next)\n",
        "\n",
        "            '''dw(dead and win) for TD_target and Adv'''\n",
        "            deltas = r + self.gamma * vs_ * (~dw) - vs\n",
        "            deltas = deltas.cpu().flatten().numpy()\n",
        "            adv = [0]\n",
        "\n",
        "            '''done for GAE'''\n",
        "            for dlt, done in zip(deltas[::-1], done.cpu().flatten().numpy()[::-1]):\n",
        "                advantage = dlt + self.gamma * self.lambd * adv[-1] * (~done)\n",
        "                adv.append(advantage)\n",
        "            adv.reverse()\n",
        "            adv = copy.deepcopy(adv[0:-1])\n",
        "            adv = torch.tensor(adv).unsqueeze(1).float().to(self.dvc)\n",
        "            td_target = adv + vs\n",
        "            if self.adv_normalization:\n",
        "                adv = (adv - adv.mean()) / ((adv.std() + 1e-4))  #sometimes helps\n",
        "\n",
        "        \"\"\"PPO update\"\"\"\n",
        "        #Slice long trajectopy into short trajectory and perform mini-batch PPO update\n",
        "        optim_iter_num = int(math.ceil(s.shape[0] / self.batch_size))\n",
        "\n",
        "        for _ in range(self.K_epochs):\n",
        "            #Shuffle the trajectory, Good for training\n",
        "            perm = np.arange(s.shape[0])\n",
        "            np.random.shuffle(perm)\n",
        "            perm = torch.LongTensor(perm).to(self.dvc)\n",
        "            s, a, td_target, adv, old_prob_a = \\\n",
        "                s[perm].clone(), a[perm].clone(), td_target[perm].clone(), adv[perm].clone(), old_prob_a[perm].clone()\n",
        "\n",
        "            '''mini-batch PPO update'''\n",
        "            for i in range(optim_iter_num):\n",
        "                index = slice(i * self.batch_size, min((i + 1) * self.batch_size, s.shape[0]))\n",
        "\n",
        "                '''actor update'''\n",
        "                prob = self.actor.pi(s[index], softmax_dim=1)\n",
        "                entropy = Categorical(prob).entropy().sum(0, keepdim=True)\n",
        "                prob_a = prob.gather(1, a[index])\n",
        "                ratio = torch.exp(torch.log(prob_a) - torch.log(old_prob_a[index]))  # a/b == exp(log(a)-log(b))\n",
        "\n",
        "                surr1 = ratio * adv[index]\n",
        "                surr2 = torch.clamp(ratio, 1 - self.clip_rate, 1 + self.clip_rate) * adv[index]\n",
        "                a_loss = -torch.min(surr1, surr2) - self.entropy_coef * entropy\n",
        "\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                a_loss.mean().backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 40)\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                '''critic update'''\n",
        "                c_loss = (self.critic(s[index]) - td_target[index]).pow(2).mean()\n",
        "                for name, param in self.critic.named_parameters():\n",
        "                    if 'weight' in name:\n",
        "                        c_loss += param.pow(2).sum() * self.l2_reg\n",
        "\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                c_loss.backward()\n",
        "                self.critic_optimizer.step()\n",
        "\n",
        "    def put_data(self, s, a, r, s_next, logprob_a, done, dw, idx):\n",
        "        # print(logprob_a)\n",
        "        # print(done)\n",
        "        # print(dw)\n",
        "        # print(idx)\n",
        "\n",
        "        self.s_hoder[idx] = s\n",
        "        self.a_hoder[idx] = a\n",
        "        self.r_hoder[idx] = r\n",
        "        self.s_next_hoder[idx] = s_next\n",
        "        self.logprob_a_hoder[idx] = logprob_a\n",
        "        self.done_hoder[idx] = done\n",
        "        self.dw_hoder[idx] = dw\n",
        "\n",
        "    def save(self, episode):\n",
        "        torch.save(self.critic.state_dict(), \"./model/ppo_critic{}.pth\".format(episode))\n",
        "        torch.save(self.actor.state_dict(), \"./model/ppo_actor{}.pth\".format(episode))\n",
        "\n",
        "    def load(self, episode):\n",
        "        self.critic.load_state_dict(torch.load(\"./model/ppo_critic{}.pth\".format(episode)))\n",
        "        self.actor.load_state_dict(torch.load(\"./model/ppo_actor{}.pth\".format(episode)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "id": "INexYvSfHrcL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "INexYvSfHrcL",
        "outputId": "c8aafb44-69b8-44fd-fcea-81ecfa61f719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "state dim :   75\n",
            "action dim :   4849\n",
            "net width :   128\n",
            "Input shape: 75, n Actions: 4849\n",
            "start :  torch.Size([75, 100, 200])\n",
            "x shape: torch.Size([1, 75, 200, 100])\n",
            "x shape: torch.Size([1, 128, 99, 49])\n",
            "x shape: torch.Size([1, 128, 49, 24])\n",
            "x shape: torch.Size([1, 128, 24, 11])\n",
            "x shape: torch.Size([1, 33792])\n",
            "x shape: torch.Size([1, 1024])\n",
            "x shape: torch.Size([1, 4849])\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'installation_type'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\karishma\\anaconda3\\envs\\green-grid-pr\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'installation_type'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[139], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     29\u001b[0m     action, logprob_action \u001b[38;5;241m=\u001b[39m ppo_agent\u001b[38;5;241m.\u001b[39mselect_action(state, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 30\u001b[0m     next_state, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Store data in PPO buffer\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     ppo_agent\u001b[38;5;241m.\u001b[39mput_data(state, action, reward, next_state, logprob_action, done, done, idx)\n",
            "Cell \u001b[1;32mIn[135], line 116\u001b[0m, in \u001b[0;36mRenewableEnergyEnvironment.step\u001b[1;34m(self, action, writer)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_tensor, reward, done, {}\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Calculate the total reward before applying the action\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m total_reward_before_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Apply the action\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_action(cell_index, action_type)\n",
            "Cell \u001b[1;32mIn[135], line 169\u001b[0m, in \u001b[0;36mRenewableEnergyEnvironment.calculate_reward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_reward\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# Solar installation cost\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m     solar_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_solar_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m# Wind turbine installation cost\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     wind_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_wind_cost()\n",
            "Cell \u001b[1;32mIn[135], line 240\u001b[0m, in \u001b[0;36mRenewableEnergyEnvironment.calculate_solar_cost\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_solar_cost\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    239\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Cost of installing solar array on cell. Based on elevation and slope\"\"\"\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_gdf[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_gdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minstallation_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolar\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolar_daily_cost \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_gdf[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_gdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstallation_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolar\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "File \u001b[1;32mc:\\Users\\karishma\\anaconda3\\envs\\green-grid-pr\\Lib\\site-packages\\geopandas\\geodataframe.py:1474\u001b[0m, in \u001b[0;36mGeoDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;124;03m    If the result is a column containing only 'geometry', return a\u001b[39;00m\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;124;03m    GeoSeries. If it's a DataFrame with any columns of GeometryDtype,\u001b[39;00m\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;124;03m    return a GeoDataFrame.\u001b[39;00m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1474\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1475\u001b[0m     \u001b[38;5;66;03m# Custom logic to avoid waiting for pandas GH51895\u001b[39;00m\n\u001b[0;32m   1476\u001b[0m     \u001b[38;5;66;03m# result is not geometry dtype for multi-indexes\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1478\u001b[0m         pd\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_scalar(key)\n\u001b[0;32m   1479\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1482\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_geometry_type(result)\n\u001b[0;32m   1483\u001b[0m     ):\n",
            "File \u001b[1;32mc:\\Users\\karishma\\anaconda3\\envs\\green-grid-pr\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[1;32mc:\\Users\\karishma\\anaconda3\\envs\\green-grid-pr\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'installation_type'"
          ]
        }
      ],
      "source": [
        "# state_gdf = gpd.read_parquet('/content/drive/MyDrive/Colab Notebooks/GreenGridPR/data/processed/state.parquet')\n",
        "state_gdf = gpd.read_parquet('../data/processed/state_1km_grid_cells.parquet')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "environment = RenewableEnergyEnvironment(state_gdf)\n",
        "state_space = environment.state_tensor.to(device)\n",
        "action_space_size = environment.action_space_size\n",
        "\n",
        "episodes = 16\n",
        "learning_rate = 0.001\n",
        "\n",
        "T_horizon_value = 2048 # number of interactions agent collects before updating its policy network\n",
        "\n",
        "# Example of initializing PPO agent\n",
        "ppo_agent = PPO_discrete(state_dim=environment.state_tensor.shape[0],\n",
        "                         action_dim=environment.action_space_size,\n",
        "                         net_width=128,\n",
        "                         device=device,\n",
        "                         lr = learning_rate,\n",
        "                         T_horizon=T_horizon_value)\n",
        "idx = 0\n",
        "for episode in range(episodes):\n",
        "    state = environment.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action, logprob_action = ppo_agent.select_action(state, deterministic=False)\n",
        "        next_state, reward, done, info = environment.step(action)\n",
        "\n",
        "        # Store data in PPO buffer\n",
        "        ppo_agent.put_data(state, action, reward, next_state, logprob_action, done, done, idx)\n",
        "        idx += 1\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    # After each episode, train the PPO agent\n",
        "    ppo_agent.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e341d5ee",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
