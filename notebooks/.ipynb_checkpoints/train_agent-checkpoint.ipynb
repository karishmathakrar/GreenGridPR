{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81222bf5-c8bd-4341-8569-71e7b8e0cd86",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Overview\n",
    "## State Space\n",
    "- The state space consists of an 80,000-cell grid, representing different geographical locations in Puerto Rico.\n",
    "- Each cell has attributes like solar PV output, wind power density, elevation, slope, cyclone risk score, building density, road density, and distance to transmission lines.\n",
    "- Approximately 70% of cells are unavailable for development due to environmental or other constraints.\n",
    "\n",
    "## Action Space\n",
    "- Two types of actions are available: building a solar array or a wind turbine.\n",
    "- Actions can be taken on any available cell.\n",
    "\n",
    "## Rewards and Costs\n",
    "The reward function should incorporate:\n",
    "- Energy production potential (solar and wind).\n",
    "- Costs or penalties associated with building on certain terrains (e.g., high elevation or steep slopes).\n",
    "- Penalties for building in high cyclone risk areas.\n",
    "- Incentives for maintaining a balance between solar and wind energy.\n",
    "- Incentives for early deployment and distributed grid development.\n",
    "- Penalties for high building or road density areas.\n",
    "- Distance to transmission lines.\n",
    "\n",
    "## RL Model\n",
    "- Model Choice: Given the size of the state space, a model-based RL algorithm (like Deep Q-Networks or Actor-Critic methods) is suitable.\n",
    "- Representation: The state representation should include the current status of each grid cell (whether it has a solar array, a wind turbine, or is vacant) along with its attributes.\n",
    "- Sequence of Actions: The RL agent will sequentially choose actions (where to build next) based on the current state of the grid.\n",
    "- Terminal State: The agent is done when the environment reaches a certain level of energy capacity or after a fixed number of steps.\n",
    "\n",
    "# Implementation Steps\n",
    "## Environment Setup: \n",
    "- Implement the environment to reflect the grid and its dynamics, including applying the binary mask for unavailable cells.\n",
    "- The step(action) method should update the grid state based on the chosen action and calculate the immediate reward or cost.\n",
    "\n",
    "## Agent Development:\n",
    "- Use PyTorch for implementing the neural network models for the agent.\n",
    "The agent needs to learn a policy that maximizes long-term rewards, considering the complex reward structure and large state space.\n",
    "\n",
    "## Training and Evaluation:\n",
    "- Set up a training loop where the agent interacts with the environment, receives feedback, and improves its policy.\n",
    "- Periodically evaluate the agent's performance, possibly using separate evaluation episodes or metrics like total energy capacity achieved or adherence to environmental constraints.\n",
    "\n",
    "## Hyperparameter Tuning:\n",
    "- Adjust learning rates, exploration rates, discount factors, and network architecture as needed to improve performance.\n",
    " \n",
    "## Scalability:\n",
    "Due to the large state space, may need to:\n",
    "- use function approximation for value functions\n",
    "- prioritizing important experiences in the replay buffer\n",
    "- parallelize computation process\n",
    "\n",
    "## Visualization and Analysis:\n",
    "- Develop tools to visualize the evolving grid layout and analyze the trade-offs made by the RL agent between different objectives (like energy maximization vs. environmental constraints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47a0776a-f41d-41ba-9261-9079553eae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c724dafa-ff4d-4fae-babe-283dcb25bb04",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac168609-d297-4963-b880-bcfdc86c2f6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'environment_gdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mRenewableEnergyEnvironment\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Initialize the environment\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_df\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrid_df\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 238\u001b[0m, in \u001b[0;36mRenewableEnergyEnvironment\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m distance \u001b[38;5;241m*\u001b[39m cost_per_km \u001b[38;5;241m*\u001b[39m cost_modifier\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m environment_gdf \u001b[38;5;241m=\u001b[39m calculate_transmission_build_cost(\u001b[43menvironment_gdf\u001b[49m, cost_per_mile\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.29\u001b[39m)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_distributed_grid_reward\u001b[39m(environment_gdf, max_distance):\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# Extract the coordinates of the occupied cells (where installations are located)\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     occupied_cells \u001b[38;5;241m=\u001b[39m environment_gdf[environment_gdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moccupied\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'environment_gdf' is not defined"
     ]
    }
   ],
   "source": [
    "class RenewableEnergyEnvironment:\n",
    "    def __init__(self, grid_df):\n",
    "        # Initialize the environment\n",
    "        self.grid_df = grid_df\n",
    "        self.state = None\n",
    "        self.total_energy_output = 0\n",
    "        self.required_energy_output = [\n",
    "            100, # Hour 01\n",
    "            100, # Hour 02\n",
    "            100, # Hour 03\n",
    "            100, # Hour 04\n",
    "            100, # Hour 05\n",
    "            100, # Hour 06\n",
    "            100, # Hour 07\n",
    "            100, # Hour 08\n",
    "            100, # Hour 09\n",
    "            100, # Hour 10\n",
    "            100, # Hour 11\n",
    "            100, # Hour 12\n",
    "            100, # Hour 13\n",
    "            100, # Hour 14\n",
    "            100, # Hour 15\n",
    "            100, # Hour 16\n",
    "            100, # Hour 17\n",
    "            100, # Hour 18\n",
    "            100, # Hour 19\n",
    "            100, # Hour 20\n",
    "            100, # Hour 21\n",
    "            100, # Hour 22\n",
    "            100, # Hour 23\n",
    "            100 # Hour 24\n",
    "        ]\n",
    "        self.weights = {\n",
    "        'transmission_line_distance': -1.0,\n",
    "        'wind_solar_balance': 1.0,\n",
    "        'building_density': -0.5,\n",
    "        'road_density': -0.3,\n",
    "        'cyclone_risk': -0.7,\n",
    "        'early_choice_reward': 1.0,\n",
    "        'distributed_grid_reward': 1.0,\n",
    "        }\n",
    "        self.bounds = {\n",
    "            'transmission_line_distance': (0, max_transmission_distance),\n",
    "            'wind_solar_balance': (0, max_balance_score),\n",
    "            'building_density': (0, max_building_density),\n",
    "            'road_density': (0, max_road_density),\n",
    "            'cyclone_risk': (0, max_cyclone_risk),\n",
    "            'early_choice_reward': (0, max_early_choice_reward),\n",
    "            'distributed_grid_reward': (0, max_distributed_grid_reward)\n",
    "        }\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment to the initial state\n",
    "        self.state = self.grid_df.copy()\n",
    "        self.total_energy_output = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply the action to the environment and return the result\n",
    "        # action: Tuple (cell_index, action_type) where action_type could be 'solar' or 'wind'\n",
    "        cell_index, action_type = action\n",
    "\n",
    "        # Check if the action is valid\n",
    "        if not self.is_valid_action(cell_index, action_type):\n",
    "            reward = -1  # Penalty for invalid action\n",
    "            done = self.is_terminal_state()\n",
    "            return self.state, reward, done, {}\n",
    "\n",
    "        # Apply the action\n",
    "        self.apply_action(cell_index, action_type)\n",
    "\n",
    "        # Calculate reward\n",
    "        reward = self.calculate_reward(self.state)\n",
    "\n",
    "        # Update total energy output or other state attributes as needed\n",
    "        self.total_energy_output += self.calculate_energy_output(self.state, cell_index, action_type)\n",
    "\n",
    "        # Check if the state is terminal\n",
    "        done = self.is_terminal_state()\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def is_valid_action(self, cell_index, action_type):\n",
    "        # Implement logic to check if an action is valid\n",
    "        # Example: Check if the cell is not masked and not already occupied\n",
    "        cell = self.state.iloc[cell_index]\n",
    "        if cell['masked']:\n",
    "            return False\n",
    "        elif cell['occupied']:\n",
    "            return False\n",
    "        elif action_type == 'solar' and cell['slope'] > 0.05:\n",
    "            return False\n",
    "        elif action_type == 'wind' and cell['slope'] > 0.15:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def apply_action(self, cell_index, action_type):\n",
    "        # Implement the changes to the environment based on the action\n",
    "        # Example: Mark the cell as occupied and record the type of installation\n",
    "        self.state.at[cell_index, 'occupied'] = True\n",
    "        self.state.at[cell_index, 'installation_type'] = action_type\n",
    "\n",
    "    def calculate_reward():\n",
    "        # Solar installation cost\n",
    "        solar_install_cost = calculate_solar_install_cost()\n",
    "\n",
    "        # Wind turbine installation cost\n",
    "        wind_install_cost = calculate_wind_install_cost()\n",
    "        \n",
    "        # Solar power Reward\n",
    "        solar_power_reward = calculate_solar_power_reward()\n",
    "        \n",
    "        # Wind Power Reward\n",
    "        wind_power_reward = calculate_wind_power_reward()\n",
    "        \n",
    "        # Transmission loss cost \n",
    "        transmission_loss_cost = calculate_transmission_loss_cost()\n",
    "\n",
    "        # Transmission build cost\n",
    "        transmission_build_cost = calculate_transmission_build_cost()\n",
    "        \n",
    "        # Cyclone risk cost\n",
    "        cyclone_risk_cost = calculate_cyclone_risk_cost()\n",
    "    \n",
    "        # Distribution reward\n",
    "        distributed_grid_reward = calculate_distributed_grid_reward()\n",
    "    \n",
    "        # Early choice reward\n",
    "        early_choice_reward = calculate_early_choice_reward()\n",
    "    \n",
    "        costs_rewards = (solar_install_cost,\n",
    "                         wind_install_cost,\n",
    "                         solar_power_reward,\n",
    "                         wind_power_reward,\n",
    "                         transmission_loss_cost,\n",
    "                         transmission_build_cost,\n",
    "                         cyclone_risk_cost,\n",
    "                         distributed_grid_reward,\n",
    "                         early_choice_reward)\n",
    "        \n",
    "        total_reward = calculate_total_reward(costs_rewards, self.weights)\n",
    "    \n",
    "        return total_reward\n",
    "\n",
    "    def calculate_energy_output(self, cell_index, action_type):\n",
    "        # Calculate the energy output for the action\n",
    "        # Example: Different output for solar and wind\n",
    "        if action_type == 'solar':\n",
    "            return self.state.iloc[cell_index]['solar_output']\n",
    "        elif action_type == 'wind':\n",
    "            return self.state.iloc[cell_index]['wind_output']\n",
    "        return 0\n",
    "\n",
    "    def calculate_solar_install_cost():\n",
    "        \"\"\"Cost of installing solar array on cell. Based on elevation and slope\"\"\"\n",
    "        state = self.state\n",
    "        bounds = self.bounds\n",
    "        return cost\n",
    "\n",
    "    def calculate_wind_install_cost():\n",
    "        \"\"\"Cost of installing wind turbine on cell. Based on elevation and slope\"\"\"\n",
    "        state = self.state\n",
    "        bounds = self.bounds\n",
    "        return cost\n",
    "\n",
    "    def calculate_power_output_reward(environment_gdf, demand):\n",
    "        \"\"\"Uses supply and demand curve to determine the amount of demand satisfied by\n",
    "        the solar and wind installations\"\"\"\n",
    "        cost_kWh = 9999  # TODO\n",
    "    \n",
    "        # Filter for solar and wind installations\n",
    "        solar_gdf = environment_gdf[(environment_gdf['occupied']) & (environment_gdf['installation'] == 'solar')]\n",
    "        wind_gdf = environment_gdf[(environment_gdf['occupied']) & (environment_gdf['installation'] == 'wind')]\n",
    "    \n",
    "        # Prepare column names for solar and wind power\n",
    "        solar_power_columns = [f'solar_power_{i}' for i in range(1, 25)]\n",
    "        wind_power_columns = [f'wind_power_{i}' for i in range(1, 25)]\n",
    "    \n",
    "        # Vectorized sum of power output for solar and wind for each hour\n",
    "        total_solar_power = solar_gdf[solar_power_columns].sum()\n",
    "        total_wind_power = wind_gdf[wind_power_columns].sum()\n",
    "    \n",
    "        # Calculate the reward using vectorized minimum\n",
    "        total_power = total_solar_power + total_wind_power\n",
    "        reward = np.minimum(total_power, demand[1:]).sum() * cost_kWh\n",
    "    \n",
    "        return reward\n",
    "    \n",
    "    def calculate_transmission_build_cost(environment_gdf):\n",
    "        # Constants for cost adjustment\n",
    "        # Extract occupied cells\n",
    "        occupied_cells = environment_gdf[environment_gdf['occupied'] == True]\n",
    "    \n",
    "        # Check if there is only one occupied cell\n",
    "        if len(occupied_cells) == 1:\n",
    "            # For a single occupied cell, use the distance to transmission line for cost calculation\n",
    "            occupied_cell = occupied_cells.iloc[0]\n",
    "            distance_km = occupied_cell['distance_to_transmission_line']\n",
    "            build_cost = transmission_line_cost_per_km(distance_km)\n",
    "        else:\n",
    "            # Get coordinates of occupied cells\n",
    "            coords = np.array(list(zip(occupied_cells.geometry.x, occupied_cells.geometry.y)))\n",
    "    \n",
    "            # Calculate pairwise distances between occupied cells\n",
    "            distances = cdist(coords, coords)\n",
    "    \n",
    "            # Replace zeros in distance matrix with np.inf to avoid zero distance to itself\n",
    "            np.fill_diagonal(distances, np.inf)\n",
    "    \n",
    "            # Find the nearest installation for each installation\n",
    "            nearest_installation_distances = np.min(distances, axis=1)\n",
    "    \n",
    "            # Determine the relevant distance for cost calculation\n",
    "            relevant_distances = np.minimum(nearest_installation_distances, occupied_cells['distance_to_transmission_line'].to_numpy())\n",
    "    \n",
    "            # Calculate build cost\n",
    "            build_costs = [transmission_line_cost_per_km(distance) for distance in relevant_distances]\n",
    "            build_cost = sum(build_costs)\n",
    "            \n",
    "        return build_cost\n",
    "    \n",
    "    def transmission_line_cost_per_km(distance):\n",
    "        COST_PER_KM = 2.29 / 1.60934  # $ Millions. Convert cost per mile to cost per kilometer\n",
    "        SHORT_DISTANCE_THRESHOLD = 3  # Threshold for short distance in miles\n",
    "        MEDIUM_DISTANCE_THRESHOLD = 10  # Threshold for medium distance in miles\n",
    "        \n",
    "        if distance < SHORT_DISTANCE_THRESHOLD:\n",
    "            cost_modifier = 1.5  # 50% increase for less than 3 miles\n",
    "        elif distance < MEDIUM_DISTANCE_THRESHOLD:\n",
    "            cost_modifier = 1.2  # 20% increase for 3-10 miles\n",
    "        else:\n",
    "            cost_modifier = 1  # No modification for more than 10 miles\n",
    "        return distance * cost_per_km * cost_modifier\n",
    "    \n",
    "    def calculate_distributed_grid_reward(environment_gdf, max_distance):\n",
    "        # Extract the coordinates of the occupied cells (where installations are located)\n",
    "        occupied_cells = environment_gdf[environment_gdf['occupied'] == True]\n",
    "        if len(occupied_cells) < 2:\n",
    "            # If there are less than two installations, we cannot calculate distances\n",
    "            return 1\n",
    "    \n",
    "        coords = np.array(list(zip(occupied_cells.geometry.x, occupied_cells.geometry.y)))\n",
    "    \n",
    "        # Calculate pairwise distances between all occupied cells\n",
    "        distances = pdist(coords)\n",
    "    \n",
    "        # Calculate the average distance. The larger this is, the more distributed the installations are.\n",
    "        avg_distance = np.mean(distances)\n",
    "    \n",
    "        # Normalize the reward such that it ranges between 0 and 1\n",
    "        normalized_reward = avg_distance / max_distance\n",
    "    \n",
    "        return normalized_reward\n",
    "    \n",
    "    def time_dependent_reward_factor(gdf, decay_rate):\n",
    "        # A function to calculate the time-dependent reward factor\n",
    "        # It decreases with each year from the base year\n",
    "        action_number = gdf.occupied.sum()\n",
    "        return 1 / (1 + decay_rate * action_number)\n",
    "    \n",
    "    def calculate_total_reward():\n",
    "        \"\"\"Calculate total cost/reward based on\"\"\"\n",
    "        state = self.state\n",
    "        bounds = self.bounds\n",
    "        return total_cost_reward\n",
    "\n",
    "    def is_terminal_state(self):\n",
    "        # The episode ends when the total energy output meets the requirement\n",
    "        return self.total_energy_output >= self.required_energy_output\n",
    "\n",
    "    def render(self):\n",
    "        # Optional: Implement a method to visualize the current state of the environment\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cb54eb-f64b-4bd0-b00f-1dbe3d5c219a",
   "metadata": {},
   "source": [
    "# Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f382086d-cee9-46f6-8f48-4d6a81c8a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.input_shape = input_shape  # Store input_shape for feature size calculation\n",
    "        self.fc1 = nn.Linear(self._feature_size(), 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def _feature_size(self):\n",
    "        with torch.no_grad():\n",
    "            return self.conv3(self.bn3(self.conv2(self.bn2(self.conv1(torch.zeros(1, *self.input_shape)))))).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ab3b4f-b218-4ce4-bf5d-482d2eae1923",
   "metadata": {},
   "source": [
    "# DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "507095e3-fc00-4972-9f51-7a04b0b59813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_space, action_space):\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.model = DQN(state_space.shape, action_space.n)\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            # Choose the best action (exploitation)\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor([state], dtype=torch.float32)\n",
    "                q_values = self.model(state_tensor)\n",
    "                action = q_values.max(1)[1].item()  # Select the action with the highest Q-value\n",
    "        else:\n",
    "            # Choose a random action (exploration)\n",
    "            action = random.randrange(self.action_space.n)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, batch):\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Compute Q values\n",
    "        current_q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.model(next_states).max(1)[0]\n",
    "        expected_q_values = rewards + (1 - dones) * next_q_values.detach()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = torch.nn.functional.mse_loss(current_q_values, expected_q_values)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254993b-b1b0-4822-82c9-ca9ba3294557",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4036b194-8954-45df-b0aa-aacef4da5c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, environment, episodes, epsilon_start, epsilon_end, epsilon_decay, replay_buffer, batch_size):\n",
    "    epsilon = epsilon_start\n",
    "    for episode in range(episodes):\n",
    "        state = environment.reset()\n",
    "        done = False\n",
    "        total_reward = 0  # To keep track of total reward per episode\n",
    "\n",
    "        while not done:\n",
    "            action = agent.select_action(state, epsilon)\n",
    "            next_state, reward, done, _ = environment.step(action)\n",
    "\n",
    "            # Store experience in replay buffer\n",
    "            replay_buffer.store(state, action, reward, next_state, done)\n",
    "\n",
    "            # Check if buffer is ready for sampling\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                # Sample a batch from replay buffer\n",
    "                batch = replay_buffer.sample(batch_size)\n",
    "                # Learn from the sampled experiences\n",
    "                agent.learn(batch)\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_end, epsilon_decay * epsilon)  # Ensure epsilon doesn't go below the minimum\n",
    "\n",
    "        # Optional: Log training progress\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
