{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81222bf5-c8bd-4341-8569-71e7b8e0cd86",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Overview\n",
    "## State Space\n",
    "- The state space consists of an 80,000-cell grid, representing different geographical locations in Puerto Rico.\n",
    "- Each cell has attributes like solar PV output, wind power density, elevation, slope, cyclone risk score, building density, road density, and distance to transmission lines.\n",
    "- Approximately 70% of cells are unavailable for development due to environmental or other constraints.\n",
    "\n",
    "## Action Space\n",
    "- Two types of actions are available: building a solar array or a wind turbine.\n",
    "- Actions can be taken on any available cell.\n",
    "\n",
    "## Rewards and Costs\n",
    "The reward function should incorporate:\n",
    "- Energy production potential (solar and wind).\n",
    "- Costs or penalties associated with building on certain terrains (e.g., high elevation or steep slopes).\n",
    "- Penalties for building in high cyclone risk areas.\n",
    "- Incentives for maintaining a balance between solar and wind energy.\n",
    "- Incentives for early deployment and distributed grid development.\n",
    "- Penalties for high building or road density areas.\n",
    "- Distance to transmission lines.\n",
    "\n",
    "## RL Model\n",
    "- Model Choice: Given the size of the state space, a model-based RL algorithm (like Deep Q-Networks or Actor-Critic methods) is suitable.\n",
    "- Representation: The state representation should include the current status of each grid cell (whether it has a solar array, a wind turbine, or is vacant) along with its attributes.\n",
    "- Sequence of Actions: The RL agent will sequentially choose actions (where to build next) based on the current state of the grid.\n",
    "- Terminal State: The agent is done when the environment reaches a certain level of energy capacity or after a fixed number of steps.\n",
    "\n",
    "# Implementation Steps\n",
    "## Environment Setup: \n",
    "- Implement the environment to reflect the grid and its dynamics, including applying the binary mask for unavailable cells.\n",
    "- The step(action) method should update the grid state based on the chosen action and calculate the immediate reward or cost.\n",
    "\n",
    "## Agent Development:\n",
    "- Use PyTorch for implementing the neural network models for the agent.\n",
    "The agent needs to learn a policy that maximizes long-term rewards, considering the complex reward structure and large state space.\n",
    "\n",
    "## Training and Evaluation:\n",
    "- Set up a training loop where the agent interacts with the environment, receives feedback, and improves its policy.\n",
    "- Periodically evaluate the agent's performance, possibly using separate evaluation episodes or metrics like total energy capacity achieved or adherence to environmental constraints.\n",
    "\n",
    "## Hyperparameter Tuning:\n",
    "- Adjust learning rates, exploration rates, discount factors, and network architecture as needed to improve performance.\n",
    " \n",
    "## Scalability:\n",
    "Due to the large state space, may need to:\n",
    "- use function approximation for value functions\n",
    "- prioritizing important experiences in the replay buffer\n",
    "- parallelize computation process\n",
    "\n",
    "## Visualization and Analysis:\n",
    "- Develop tools to visualize the evolving grid layout and analyze the trade-offs made by the RL agent between different objectives (like energy maximization vs. environmental constraints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a0776a-f41d-41ba-9261-9079553eae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c724dafa-ff4d-4fae-babe-283dcb25bb04",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac168609-d297-4963-b880-bcfdc86c2f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenewableEnergyEnvironment:\n",
    "    def __init__(self, grid_df):\n",
    "        # Initialize the environment\n",
    "        self.grid_df = grid_df\n",
    "        self.state = None\n",
    "        self.total_energy_output = 0\n",
    "        self.weights = {\n",
    "        'transmission_line_distance': -1.0,\n",
    "        'wind_solar_balance': 1.0,\n",
    "        'building_density': -0.5,\n",
    "        'road_density': -0.3,\n",
    "        'cyclone_risk': -0.7,\n",
    "        'early_choice_reward': 1.0,\n",
    "        'distributed_grid_reward': 1.0,\n",
    "        }\n",
    "        self.bounds = {\n",
    "            'transmission_line_distance': (0, max_transmission_distance),\n",
    "            'wind_solar_balance': (0, max_balance_score),\n",
    "            'building_density': (0, max_building_density),\n",
    "            'road_density': (0, max_road_density),\n",
    "            'cyclone_risk': (0, max_cyclone_risk),\n",
    "            'early_choice_reward': (0, max_early_choice_reward),\n",
    "            'distributed_grid_reward': (0, max_distributed_grid_reward)\n",
    "        }\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment to the initial state\n",
    "        self.state = self.grid_df.copy()\n",
    "        self.total_energy_output = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply the action to the environment and return the result\n",
    "        # action: Tuple (cell_index, action_type) where action_type could be 'solar' or 'wind'\n",
    "        cell_index, action_type = action\n",
    "\n",
    "        # Check if the action is valid\n",
    "        if not self.is_valid_action(cell_index, action_type):\n",
    "            reward = -1  # Penalty for invalid action\n",
    "            done = self.is_terminal_state()\n",
    "            return self.state, reward, done, {}\n",
    "\n",
    "        # Apply the action\n",
    "        self.apply_action(cell_index, action_type)\n",
    "\n",
    "        # Calculate reward\n",
    "        reward = self.calculate_reward(self.state)\n",
    "\n",
    "        # Update total energy output or other state attributes as needed\n",
    "        self.total_energy_output += self.calculate_energy_output(self.state, cell_index, action_type)\n",
    "\n",
    "        # Check if the state is terminal\n",
    "        done = self.is_terminal_state()\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def is_valid_action(self, cell_index, action_type):\n",
    "        # Implement logic to check if an action is valid\n",
    "        # Example: Check if the cell is not masked and not already occupied\n",
    "        cell = self.state.iloc[cell_index]\n",
    "        return not cell['masked'] and not cell['occupied']\n",
    "\n",
    "    def apply_action(self, cell_index, action_type):\n",
    "        # Implement the changes to the environment based on the action\n",
    "        # Example: Mark the cell as occupied and record the type of installation\n",
    "        self.state.at[cell_index, 'occupied'] = True\n",
    "        self.state.at[cell_index, 'installation_type'] = action_type\n",
    "\n",
    "    def calculate_reward():\n",
    "        grid_cost_reward = list()\n",
    "        solar_install_cost = 0\n",
    "        wind_install_cost = 0\n",
    "        solar_power_reward = 0\n",
    "        wind_power_reward = 0\n",
    "        transmission_loss_cost = 0\n",
    "        transmission_build_cost = 0\n",
    "        cyclone_risk_cost = 0\n",
    "    \n",
    "        for idx, row in self.state.iterrows():\n",
    "            # Calculate individual costs and rewards\n",
    "    \n",
    "            # Solar installation cost\n",
    "            solar_install_cost += calculate_solar_install_cost(self.state, idx, self.bounds)\n",
    "    \n",
    "            # Wind turbine installation cost\n",
    "            wind_install_cost += calculate_wind_install_cost(self.state, idx, self.bounds)\n",
    "            \n",
    "            # Solar power Reward\n",
    "            solar_power_reward += calculate_solar_power_reward(self.state, idx, self.bounds)\n",
    "            \n",
    "            # Wind Power Reward\n",
    "            wind_power_reward += calculate_wind_power_reward(self.state, idx, self.bounds)\n",
    "            \n",
    "            # Transmission loss cost \n",
    "            transmission_loss_cost += calculate_transmission_loss_cost(self.state, idx, self.bounds)\n",
    "    \n",
    "            # Transmission build cost\n",
    "            transmission_build_cost += calculate_transmission_build_cost(self.state, idx, self.bounds)\n",
    "            \n",
    "            # Cyclone risk cost\n",
    "            cyclone_risk_cost += calculate_cyclone_risk_cost(self.state, idx, self.bounds)\n",
    "    \n",
    "        # Distribution reward\n",
    "        distributed_grid_reward = calculate_distributed_grid_reward(self.state, idx, self.bounds)\n",
    "    \n",
    "        # Early choice reward\n",
    "        early_choice_reward = calculate_early_choice_reward(self.state, idx, self.bounds)\n",
    "    \n",
    "        costs_rewards = (solar_install_cost,\n",
    "                         wind_install_cost,\n",
    "                         solar_power_reward,\n",
    "                         wind_power_reward,\n",
    "                         transmission_loss_cost,\n",
    "                         transmission_build_cost,\n",
    "                         cyclone_risk_cost,\n",
    "                         distributed_grid_reward,\n",
    "                         early_choice_reward)\n",
    "        \n",
    "        total_reward = calculate_total_reward(costs_rewards, self.weights)\n",
    "    \n",
    "        return total_reward\n",
    "\n",
    "    def calculate_energy_output(self, cell_index, action_type):\n",
    "        # Calculate the energy output for the action\n",
    "        # Example: Different output for solar and wind\n",
    "        if action_type == 'solar':\n",
    "            return self.state.iloc[cell_index]['solar_output']\n",
    "        elif action_type == 'wind':\n",
    "            return self.state.iloc[cell_index]['wind_output']\n",
    "        return 0\n",
    "\n",
    "    def calculate_solar_install_cost(idx):\n",
    "        \"\"\"Cost of installing solar array on cell. Based on elevation and slope\"\"\"\n",
    "        state = self.state\n",
    "        bounds = self.bounds\n",
    "        return cost\n",
    "\n",
    "    def calculate_wind_install_cost(idx):\n",
    "        \"\"\"Cost of installing wind turbine on cell. Based on elevation and slope\"\"\"\n",
    "        state = self.state\n",
    "        bounds = self.bounds\n",
    "        return cost\n",
    "    \n",
    "    def calculate_solar_power_reward(idx):\n",
    "        \"\"\"Uses GHI and demand curve to determine the amount of demand satisfied by\n",
    "        the solar installation\"\"\"\n",
    "        state = self.state\n",
    "        bounds = self.bounds\n",
    "        return reward\n",
    "    \n",
    "    def calculate_wind_power_reward(idx):\n",
    "        \"\"\"Uses Wind Speed and demand curve to determine the amount of demand \n",
    "        satisfied by the solar installation\"\"\"\n",
    "        state = self.state\n",
    "        bounds = self.bounds\n",
    "        return reward\n",
    "    \n",
    "    def calculate_transmission_loss_cost(idx):\n",
    "        \"\"\"Energy lost due to transmission loss. Uses building density to determine \n",
    "        demand. Calculates average distance the power from the installation will \n",
    "        need to travel. Uses this distance to calculate transmission cost\"\"\"\n",
    "        state = self.state\n",
    "        bounds = self.bounds\n",
    "        return cost\n",
    "    \n",
    "    def calculate_transmission_build_cost(idx):\n",
    "        \"\"\"Cost of building new transmission infrastructure to serve all installations.\n",
    "        Based on min(distance to nearest transmission line, \n",
    "                     distance to nearest previous installation)\n",
    "        And cost per km of new transmission lines\"\"\"\n",
    "        state = self.state\n",
    "        bounds = self.bounds\n",
    "        return cost\n",
    "    \n",
    "    def calculate_cyclone_risk_cost(idx):\n",
    "        \"\"\"Increase cost proportional to chance of being destroyed. Uses cyclone risk\n",
    "        score at grid cell and type of installation.\"\"\"\n",
    "        state = self.state\n",
    "        bounds = self.bounds\n",
    "        return cost\n",
    "    \n",
    "    def calculate_distributed_grid_reward(idx):\n",
    "        \"\"\"Reward proportional to average distance between installations.\"\"\"\n",
    "        state = self.state\n",
    "        bounds = self.bounds\n",
    "        return reward\n",
    "    \n",
    "    def calculate_early_choice_reward(idx):\n",
    "        \"\"\"Rewards early installations -- this is a multiplying factor that \n",
    "        scales the total reward based on how early the action is taken\"\"\"\n",
    "        state = self.state\n",
    "        bounds = self.bounds\n",
    "        return reward\n",
    "    \n",
    "    def calculate_total_reward(idx):\n",
    "        \"\"\"Calculate total cost/reward based on\"\"\"\n",
    "        state = self.state\n",
    "        bounds = self.bounds\n",
    "        return total_cost_reward\n",
    "\n",
    "    def is_terminal_state(self):\n",
    "        # Define the terminal condition\n",
    "        # Example: Terminal state when a certain total energy output is reached\n",
    "        required_energy_output = 10000  # Example value\n",
    "        return self.total_energy_output >= required_energy_output\n",
    "\n",
    "    def render(self):\n",
    "        # Optional: Implement a method to visualize the current state of the environment\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cb54eb-f64b-4bd0-b00f-1dbe3d5c219a",
   "metadata": {},
   "source": [
    "# Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f382086d-cee9-46f6-8f48-4d6a81c8a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        self.input_shape = input_shape  # Store input_shape for feature size calculation\n",
    "        self.fc1 = nn.Linear(self._feature_size(), 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def _feature_size(self):\n",
    "        with torch.no_grad():  # No need to track gradients here\n",
    "            return self.conv3(self.conv2(self.conv1(torch.zeros(1, *self.input_shape)))).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ab3b4f-b218-4ce4-bf5d-482d2eae1923",
   "metadata": {},
   "source": [
    "# DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "507095e3-fc00-4972-9f51-7a04b0b59813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_space, action_space):\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.model = DQN(state_space.shape, action_space.n)\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            # Choose the best action (exploitation)\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor([state], dtype=torch.float32)\n",
    "                q_values = self.model(state_tensor)\n",
    "                action = q_values.max(1)[1].item()  # Select the action with the highest Q-value\n",
    "        else:\n",
    "            # Choose a random action (exploration)\n",
    "            action = random.randrange(self.action_space.n)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, batch):\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Compute Q values\n",
    "        current_q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.model(next_states).max(1)[0]\n",
    "        expected_q_values = rewards + (1 - dones) * next_q_values.detach()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = torch.nn.functional.mse_loss(current_q_values, expected_q_values)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254993b-b1b0-4822-82c9-ca9ba3294557",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4036b194-8954-45df-b0aa-aacef4da5c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, environment, episodes, epsilon_start, epsilon_end, epsilon_decay, replay_buffer, batch_size):\n",
    "    epsilon = epsilon_start\n",
    "    for episode in range(episodes):\n",
    "        state = environment.reset()\n",
    "        done = False\n",
    "        total_reward = 0  # To keep track of total reward per episode\n",
    "\n",
    "        while not done:\n",
    "            action = agent.select_action(state, epsilon)\n",
    "            next_state, reward, done, _ = environment.step(action)\n",
    "\n",
    "            # Store experience in replay buffer\n",
    "            replay_buffer.store(state, action, reward, next_state, done)\n",
    "\n",
    "            # Check if buffer is ready for sampling\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                # Sample a batch from replay buffer\n",
    "                batch = replay_buffer.sample(batch_size)\n",
    "                # Learn from the sampled experiences\n",
    "                agent.learn(batch)\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_end, epsilon_decay * epsilon)  # Ensure epsilon doesn't go below the minimum\n",
    "\n",
    "        # Optional: Log training progress\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97292df-63f3-4f2d-8c91-831df7a79826",
   "metadata": {},
   "source": [
    "# Calculating Final Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98271b3e-7f86-4a1a-9a19-c3089e3af3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_solar_install_cost(grid_gdf, idx, bounds):\n",
    "    \"\"\"Cost of installing solar array on cell. Based on elevation and slope\"\"\"\n",
    "    return cost\n",
    "\n",
    "def calculate_wind_install_cost(grid_gdf, idx, bounds):\n",
    "    \"\"\"Cost of installing wind turbine on cell. Based on elevation and slope\"\"\"\n",
    "    return cost\n",
    "\n",
    "def calculate_solar_power_reward(grid_gdf, idx, bounds):\n",
    "    \"\"\"Uses GHI and demand curve to determine the amount of demand satisfied by\n",
    "    the solar installation\"\"\"\n",
    "    return reward\n",
    "\n",
    "def calculate_wind_power_reward(grid_gdf, idx, bounds):\n",
    "    \"\"\"Uses Wind Speed and demand curve to determine the amount of demand \n",
    "    satisfied by the solar installation\"\"\"\n",
    "    return reward\n",
    "\n",
    "def calculate_transmission_loss_cost(grid_gdf, idx, bounds):\n",
    "    \"\"\"Energy lost due to transmission loss. Uses building density to determine \n",
    "    demand. Calculates average distance the power from the installation will \n",
    "    need to travel. Uses this distance to calculate transmission cost\"\"\"\n",
    "    return cost\n",
    "\n",
    "def calculate_transmission_build_cost(grid_gdf, idx, bounds):\n",
    "    \"\"\"Cost of building new transmission infrastructure to serve all installations.\n",
    "    Based on min(distance to nearest transmission line, \n",
    "                 distance to nearest previous installation)\n",
    "    And cost per km of new transmission lines\"\"\"\n",
    "    return cost\n",
    "\n",
    "def calculate_cyclone_risk_cost(grid_gdf, idx, bounds):\n",
    "    \"\"\"Increase cost proportional to chance of being destroyed. Uses cyclone risk\n",
    "    score at grid cell and type of installation.\"\"\"\n",
    "    return cost\n",
    "\n",
    "def calculate_distributed_grid_reward(grid_gdf, idx, bounds):\n",
    "    \"\"\"Reward proportional to average distance between installations.\"\"\"\n",
    "    return reward\n",
    "\n",
    "def calculate_early_choice_reward(grid_gdf, idx, bounds):\n",
    "    \"\"\"Rewards early installations -- this is a multiplying factor that \n",
    "    scales the total reward based on how early the action is taken\"\"\"\n",
    "    return reward\n",
    "\n",
    "def calculate_total_reward(costs_rewards, weights):\n",
    "    \"\"\"Calculate total cost/reward based on\"\"\"\n",
    "    return total_cost_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0d4d4-8d78-4e6d-a0e8-d2c98a8ab453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
